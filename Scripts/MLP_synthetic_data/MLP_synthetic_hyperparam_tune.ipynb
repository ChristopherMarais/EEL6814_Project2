{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe768f16-25b6-4f9f-a52a-02f9021de663",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file does not exist. Starting fresh.\n",
      "Testing hyperparameter combination 1/2880: {'learning_rate': 0.001, 'layer_sizes': [128, 64], 'dropout_rates': [0.1, 0.1], 'batch_norm': [True, True], 'weight_decay': 0.0, 'batch_size': 16, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 200, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0, 'early_stopping_mode': 'min', 'early_stopping_patience': 5, 'early_stopping_delta': 0.0001}\n",
      "Epoch [1/200], Training Loss: 0.5889, Training Accuracy: 81.80%, Validation Loss: 0.2828, Validation Accuracy: 91.68%\n",
      "Epoch [2/200], Training Loss: 0.3718, Training Accuracy: 88.40%, Validation Loss: 0.2296, Validation Accuracy: 93.42%\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import json\n",
    "import hashlib\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split, ConcatDataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "from MLP_model import MLPClassifier  # Import the MLPClassifier class\n",
    "from itertools import product\n",
    "\n",
    "# Function to set all random seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # If you are using multi-GPU.\n",
    "    \n",
    "# Set the base seed\n",
    "seed=42\n",
    "set_seed(seed)\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(m.weight, nonlinearity='leaky_relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),  # Adjust if necessary\n",
    "    transforms.Lambda(lambda x: x.view(-1))  # Flatten the image\n",
    "])\n",
    "\n",
    "# Load the KMNIST dataset\n",
    "# Load the full training dataset\n",
    "full_train_dataset = datasets.KMNIST(root='../data', train=True, transform=transform, download=True)\n",
    "\n",
    "# Define the sizes for the training and validation datasets\n",
    "val_size = 10000\n",
    "train_size = len(full_train_dataset) - val_size\n",
    "\n",
    "# Split the training data into train and validation sets\n",
    "# When using random_split, create a generator with the seed\n",
    "generator = torch.Generator().manual_seed(seed)\n",
    "train_dataset_partial, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "\n",
    "# Create a Dataset for the generated images\n",
    "class GeneratedImagesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the generated images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        for label in range(10):  # Assuming labels are from 0 to 9\n",
    "            for i in range(100):  # Assuming 100 images per class\n",
    "                filename = f'class_{label}_image_{i}.png'\n",
    "                filepath = os.path.join(root_dir, filename)\n",
    "                if os.path.exists(filepath):\n",
    "                    self.image_paths.append(filepath)\n",
    "                    self.labels.append(label)\n",
    "                else:\n",
    "                    print(f'File {filepath} does not exist')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_paths[idx]\n",
    "        image = Image.open(img_name).convert('L')  # 'L' for grayscale\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Instantiate the generated images dataset\n",
    "generated_images_dir = 'generated_images'  # Adjust the path if necessary\n",
    "generated_dataset = GeneratedImagesDataset(root_dir=generated_images_dir, transform=transform)\n",
    "\n",
    "# Combine the generated images dataset with the original training dataset\n",
    "train_dataset = ConcatDataset([train_dataset_partial, generated_dataset])\n",
    "\n",
    "# Input and output sizes\n",
    "input_size = 784  # For 28x28 images\n",
    "output_size = 10  # Number of classes\n",
    "\n",
    "# Hyperparameter grid for tuning\n",
    "hyperparameter_grid = {\n",
    "    'learning_rate': [1e-3, 1e-4, 1e-5],\n",
    "    'layer_sizes': [[128, 64], [256, 128], [512, 256]],\n",
    "    'dropout_rates': [[0.1, 0.1], [0.2, 0.2], [0.3, 0.3], [0.4, 0.4], [0.5, 0.5]],\n",
    "    'batch_norm': [[True, True], [False, False]],\n",
    "    'weight_decay': [0.0, 1e-5, 1e-4, 1e-3],\n",
    "    'batch_size': [16, 32, 64, 128],\n",
    "    'leaky_relu_negative_slope': [0.01, 0.1],  # Negative slope for LeakyReLU\n",
    "    'num_epochs': [200],  # Max number of training epochs\n",
    "    # ReduceLROnPlateau hyperparameters\n",
    "    'scheduler_mode': ['min'],  # Mode for the scheduler\n",
    "    'scheduler_factor': [0.1],  # Factor by which the learning rate will be reduced\n",
    "    'scheduler_patience': [3],  # Number of epochs with no improvement after which learning rate will be reduced\n",
    "    'scheduler_threshold': [1e-4],  # Threshold for measuring the new optimum\n",
    "    'scheduler_cooldown': [0],  # Number of epochs to wait before resuming normal operation after lr has been reduced\n",
    "    # Early Stopping hyperparameters\n",
    "    'early_stopping_mode': ['min'],  # Mode for early stopping\n",
    "    'early_stopping_patience': [5],  # Patience for early stopping\n",
    "    'early_stopping_delta': [1e-4],  # Minimum change to qualify as improvement\n",
    "}\n",
    "\n",
    "# Generate all combinations of hyperparameters\n",
    "keys, values = zip(*hyperparameter_grid.items())\n",
    "hyperparameter_combinations = [dict(zip(keys, v)) for v in product(*values)]\n",
    "\n",
    "csv_filename = 'MLP_synth_hyperparameter_tuning_results.csv'\n",
    "\n",
    "# Initialize an empty set to store hashes of existing results\n",
    "existing_hashes = set()\n",
    "\n",
    "# Initialize best_val_loss to infinity\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "if os.path.exists(csv_filename):\n",
    "    with open(csv_filename, 'r', newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            hparams_hash = row['hparams_hash']\n",
    "            existing_hashes.add(hparams_hash)\n",
    "            try:\n",
    "                current_val_loss = float(row['val_loss'])\n",
    "                if current_val_loss < best_val_loss:\n",
    "                    best_val_loss = current_val_loss\n",
    "            except ValueError:\n",
    "                # Handle cases where val_loss might not be a valid float\n",
    "                continue\n",
    "    print(f\"Existing best validation loss from CSV: {best_val_loss:.4f}\")\n",
    "else:\n",
    "    print(\"CSV file does not exist. Starting fresh.\")\n",
    "    # If the file does not exist, we'll create it later\n",
    "\n",
    "def train_and_evaluate(hparams):\n",
    "    # Unpack hyperparameters\n",
    "    learning_rate = hparams['learning_rate']\n",
    "    layer_sizes = hparams['layer_sizes']\n",
    "    dropout_rates = hparams['dropout_rates']\n",
    "    batch_norm = hparams['batch_norm']\n",
    "    weight_decay = hparams['weight_decay']\n",
    "    batch_size = hparams['batch_size']\n",
    "    leaky_relu_negative_slope = hparams['leaky_relu_negative_slope']\n",
    "    num_epochs = hparams.get('num_epochs', 50)\n",
    "    \n",
    "    # Scheduler hyperparameters\n",
    "    scheduler_mode = hparams['scheduler_mode']\n",
    "    scheduler_factor = hparams['scheduler_factor']\n",
    "    scheduler_patience = hparams['scheduler_patience']\n",
    "    scheduler_threshold = hparams['scheduler_threshold']\n",
    "    scheduler_cooldown = hparams['scheduler_cooldown']\n",
    "    \n",
    "    # Early Stopping hyperparameters\n",
    "    early_stopping_mode = hparams['early_stopping_mode']\n",
    "    early_stopping_patience = hparams['early_stopping_patience']\n",
    "    early_stopping_delta = hparams['early_stopping_delta']\n",
    "    \n",
    "    # Define activation functions: Always LeakyReLU with specified negative slope\n",
    "    activation_functions = [nn.LeakyReLU(negative_slope=leaky_relu_negative_slope) for _ in layer_sizes]\n",
    "\n",
    "    # Initialize the model\n",
    "    model = MLPClassifier(\n",
    "        input_size=input_size,\n",
    "        layer_sizes=layer_sizes,\n",
    "        output_size=output_size,\n",
    "        activation_functions=activation_functions,\n",
    "        dropout_rates=dropout_rates,\n",
    "        batch_norm=batch_norm,\n",
    "        weight_init=init_weights\n",
    "    ).to(device)\n",
    "\n",
    "    # Optimizer (always AdamW)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Scheduler: ReduceLROnPlateau\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode=scheduler_mode,\n",
    "        factor=scheduler_factor,\n",
    "        patience=scheduler_patience,\n",
    "        threshold=scheduler_threshold,\n",
    "        threshold_mode='rel',\n",
    "        cooldown=scheduler_cooldown,\n",
    "    )\n",
    "\n",
    "    # Data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Loss function\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Early Stopping variables\n",
    "    best_metric = None\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "\n",
    "    # Determine the comparison operator based on mode\n",
    "    if early_stopping_mode == 'min':\n",
    "        def is_improvement(current, best):\n",
    "            return current < best - early_stopping_delta\n",
    "        best_metric = float('inf')\n",
    "    elif early_stopping_mode == 'max':\n",
    "        def is_improvement(current, best):\n",
    "            return current > best + early_stopping_delta\n",
    "        best_metric = float('-inf')\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported early_stopping_mode: {early_stopping_mode}\")\n",
    "\n",
    "    # Initialize variables for logging\n",
    "    initial_lr = optimizer.param_groups[0]['lr']\n",
    "    current_lr = initial_lr\n",
    "    lr_reduction_epochs = []\n",
    "    epoch_logs = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        if early_stop:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for data in train_loader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Calculate training accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_accuracy = 100 * correct_train / total_train\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_function(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Calculate validation accuracy\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = 100 * correct_val / total_val\n",
    "\n",
    "        # Scheduler step: ReduceLROnPlateau expects a metric to monitor\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Check if learning rate has been reduced\n",
    "        new_lr = optimizer.param_groups[0]['lr']\n",
    "        if new_lr < current_lr:\n",
    "            lr_reduction_epochs.append(epoch+1)  # Epochs are 1-indexed\n",
    "            current_lr = new_lr\n",
    "\n",
    "        # Record per-epoch logs\n",
    "        epoch_logs.append({\n",
    "            'epoch': epoch+1,\n",
    "            'train_loss': train_loss,\n",
    "            'train_accuracy': train_accuracy,\n",
    "            'val_loss': val_loss,\n",
    "            'val_accuracy': val_accuracy,\n",
    "            'learning_rate': current_lr\n",
    "        })\n",
    "\n",
    "        # Early Stopping check\n",
    "        if is_improvement(val_loss, best_metric):\n",
    "            best_metric = val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= early_stopping_patience:\n",
    "            early_stop = True\n",
    "\n",
    "        # Optionally, print progress\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "              f\"Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%, \"\n",
    "              f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "    # Prepare training log\n",
    "    training_log = {\n",
    "        'final_epoch': epoch+1,\n",
    "        'lr_reduction_epochs': lr_reduction_epochs,\n",
    "        'epoch_logs': epoch_logs\n",
    "    }\n",
    "\n",
    "    # Return final validation loss and accuracy, training log, and the model\n",
    "    return val_loss, val_accuracy, training_log, model, activation_functions, optimizer, scheduler\n",
    "\n",
    "# Main loop for hyperparameter tuning\n",
    "results = []\n",
    "\n",
    "for idx, hparams in enumerate(hyperparameter_combinations):\n",
    "    # Set the base seed\n",
    "    set_seed(42)\n",
    "    # Generate a unique id for the hyperparameters\n",
    "    hparams_str = json.dumps(hparams, sort_keys=True)\n",
    "    hparams_hash = hashlib.md5(hparams_str.encode('utf-8')).hexdigest()\n",
    "    \n",
    "    # Check if this combination exists in existing_results\n",
    "    if hparams_hash in existing_hashes:\n",
    "        print(f\"Skipping hyperparameter combination {idx+1}/{len(hyperparameter_combinations)}: {hparams} (already tested)\")\n",
    "        continue\n",
    "    else:\n",
    "        print(f\"Testing hyperparameter combination {idx+1}/{len(hyperparameter_combinations)}: {hparams}\")\n",
    "        try:\n",
    "            # Call train_and_evaluate and record the computation time\n",
    "            start_time = time.time()\n",
    "            val_loss, val_accuracy, training_log, model, activation_functions, optimizer, scheduler = train_and_evaluate(hparams)\n",
    "            end_time = time.time()\n",
    "            computation_time = end_time - start_time\n",
    "\n",
    "            # Save the per-epoch logs to a file\n",
    "            log_filename = f\"misc/training_log_{hparams_hash}.json\"\n",
    "            with open(log_filename, 'w') as f:\n",
    "                json.dump(training_log, f)\n",
    "            \n",
    "            # Append the result to the CSV file\n",
    "            with open(csv_filename, 'a', newline='') as csvfile:\n",
    "                fieldnames = ['hparams_hash'] + list(hparams.keys()) + ['val_loss', 'val_accuracy', 'computation_time', 'final_num_epochs', 'lr_reduction_epochs', 'log_filename']\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                # If the file is new, write the header\n",
    "                if csvfile.tell() == 0:\n",
    "                    writer.writeheader()\n",
    "                result_row = {'hparams_hash': hparams_hash}\n",
    "                for key, value in hparams.items():\n",
    "                    # Convert value to string using json.dumps\n",
    "                    result_row[key] = json.dumps(value)\n",
    "                result_row.update({\n",
    "                    'val_loss': val_loss,\n",
    "                    'val_accuracy': val_accuracy,\n",
    "                    'computation_time': computation_time,\n",
    "                    'final_num_epochs': training_log['final_epoch'],\n",
    "                    'lr_reduction_epochs': json.dumps(training_log['lr_reduction_epochs']),\n",
    "                    'log_filename': log_filename\n",
    "                })\n",
    "                writer.writerow(result_row)\n",
    "\n",
    "            # Update the set of existing hashes\n",
    "            existing_hashes.add(hparams_hash)\n",
    "\n",
    "            # Check if current val_loss is better than the best_val_loss\n",
    "            if val_loss < best_val_loss: \n",
    "                best_val_loss = val_loss\n",
    "                # Save the model's state_dict\n",
    "                best_model_filename = 'MLP_synth_best_model.pth'\n",
    "                # Prepare all necessary information for saving\n",
    "                save_data = {\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'model_config': {\n",
    "                        'input_size': input_size,\n",
    "                        'output_size': output_size,\n",
    "                        'layer_sizes': hparams['layer_sizes'],\n",
    "                        'activation_functions': [str(type(act)) for act in activation_functions],\n",
    "                        'dropout_rates': hparams['dropout_rates'],\n",
    "                        'batch_norm': hparams['batch_norm'],\n",
    "                    },\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),  # Save optimizer state if needed for resuming training\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),  # Save scheduler state if used\n",
    "                    'hyperparameters': hparams,  # Save the hyperparameters for reference\n",
    "                    'training_log': training_log  # Include the training log if desired\n",
    "                }\n",
    "\n",
    "                # Save to file\n",
    "                torch.save(save_data, best_model_filename)\n",
    "                print(f\"New best model saved to {best_model_filename} with validation loss: {val_loss:.4f}\")\n",
    "            \n",
    "            # Append to results for final reporting\n",
    "            results.append({'hparams': hparams, 'val_loss': val_loss, 'val_accuracy': val_accuracy})\n",
    "            print(f\"Result - Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error with hyperparameters {hparams}: {e}\\n\")\n",
    "            continue\n",
    "\n",
    "# Find the best hyperparameters based on validation accuracy\n",
    "if results:\n",
    "    best_result = min(results, key=lambda x: x['val_loss'])  # Changed to min based on val_loss\n",
    "    print(\"Best hyperparameters based on validation loss:\")\n",
    "    print(best_result['hparams'])\n",
    "    print(f\"Validation Loss: {best_result['val_loss']:.4f}, Validation Accuracy: {best_result['val_accuracy']:.2f}%\")\n",
    "else:\n",
    "    print(\"No results to report.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EEL",
   "language": "EEL",
   "name": "eel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

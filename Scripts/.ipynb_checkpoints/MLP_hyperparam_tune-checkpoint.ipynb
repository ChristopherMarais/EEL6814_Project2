{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69f4c8bd-8652-406a-8e27-b6320d40de2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing hyperparameter combination 1/19200: {'learning_rate': 1e-05, 'layer_sizes': [256, 128, 64], 'dropout_rates': [0.1, 0.1, 0.1], 'batch_norm': [True, True, True], 'weight_decay': 0.0, 'batch_size': 16, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 100, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0, 'early_stopping_mode': 'min', 'early_stopping_patience': 5, 'early_stopping_delta': 0.0001}\n",
      "Epoch [1/100], Training Loss: 1.8495, Training Accuracy: 38.43%, Validation Loss: 1.5413, Validation Accuracy: 51.63%\n",
      "Epoch [2/100], Training Loss: 1.2095, Training Accuracy: 63.16%, Validation Loss: 1.2689, Validation Accuracy: 61.88%\n",
      "Epoch [3/100], Training Loss: 0.9854, Training Accuracy: 70.81%, Validation Loss: 1.0849, Validation Accuracy: 66.91%\n",
      "Epoch [4/100], Training Loss: 0.8571, Training Accuracy: 74.66%, Validation Loss: 0.9966, Validation Accuracy: 69.59%\n",
      "Epoch [5/100], Training Loss: 0.7763, Training Accuracy: 76.89%, Validation Loss: 0.9128, Validation Accuracy: 72.07%\n",
      "Epoch [6/100], Training Loss: 0.7195, Training Accuracy: 78.50%, Validation Loss: 0.8631, Validation Accuracy: 73.29%\n",
      "Epoch [7/100], Training Loss: 0.6727, Training Accuracy: 79.96%, Validation Loss: 0.8207, Validation Accuracy: 74.34%\n",
      "Epoch [8/100], Training Loss: 0.6382, Training Accuracy: 80.83%, Validation Loss: 0.7853, Validation Accuracy: 75.11%\n",
      "Epoch [9/100], Training Loss: 0.6050, Training Accuracy: 81.83%, Validation Loss: 0.7493, Validation Accuracy: 76.02%\n",
      "Epoch [10/100], Training Loss: 0.5786, Training Accuracy: 82.69%, Validation Loss: 0.7122, Validation Accuracy: 77.33%\n",
      "Epoch [11/100], Training Loss: 0.5523, Training Accuracy: 83.57%, Validation Loss: 0.6830, Validation Accuracy: 78.23%\n",
      "Epoch [12/100], Training Loss: 0.5383, Training Accuracy: 83.94%, Validation Loss: 0.6643, Validation Accuracy: 79.17%\n",
      "Epoch [13/100], Training Loss: 0.5172, Training Accuracy: 84.43%, Validation Loss: 0.6557, Validation Accuracy: 79.24%\n",
      "Epoch [14/100], Training Loss: 0.4976, Training Accuracy: 84.97%, Validation Loss: 0.6203, Validation Accuracy: 80.42%\n",
      "Epoch [15/100], Training Loss: 0.4834, Training Accuracy: 85.42%, Validation Loss: 0.6076, Validation Accuracy: 80.89%\n",
      "Epoch [16/100], Training Loss: 0.4722, Training Accuracy: 85.76%, Validation Loss: 0.6024, Validation Accuracy: 81.09%\n",
      "Epoch [17/100], Training Loss: 0.4551, Training Accuracy: 86.24%, Validation Loss: 0.5878, Validation Accuracy: 81.54%\n",
      "Epoch [18/100], Training Loss: 0.4415, Training Accuracy: 86.69%, Validation Loss: 0.5717, Validation Accuracy: 82.01%\n",
      "Epoch [19/100], Training Loss: 0.4375, Training Accuracy: 86.82%, Validation Loss: 0.5512, Validation Accuracy: 83.06%\n",
      "Epoch [20/100], Training Loss: 0.4254, Training Accuracy: 87.13%, Validation Loss: 0.5465, Validation Accuracy: 83.00%\n",
      "Epoch [21/100], Training Loss: 0.4132, Training Accuracy: 87.43%, Validation Loss: 0.5411, Validation Accuracy: 82.95%\n",
      "Epoch [22/100], Training Loss: 0.4038, Training Accuracy: 87.70%, Validation Loss: 0.5255, Validation Accuracy: 83.83%\n",
      "Epoch [23/100], Training Loss: 0.3915, Training Accuracy: 88.13%, Validation Loss: 0.5161, Validation Accuracy: 84.02%\n",
      "Epoch [24/100], Training Loss: 0.3867, Training Accuracy: 88.25%, Validation Loss: 0.5218, Validation Accuracy: 83.77%\n",
      "Epoch [25/100], Training Loss: 0.3805, Training Accuracy: 88.34%, Validation Loss: 0.4881, Validation Accuracy: 84.88%\n",
      "Epoch [26/100], Training Loss: 0.3701, Training Accuracy: 88.86%, Validation Loss: 0.4952, Validation Accuracy: 84.63%\n",
      "Epoch [27/100], Training Loss: 0.3637, Training Accuracy: 88.77%, Validation Loss: 0.4816, Validation Accuracy: 85.13%\n",
      "Epoch [28/100], Training Loss: 0.3620, Training Accuracy: 88.86%, Validation Loss: 0.4938, Validation Accuracy: 84.62%\n",
      "Epoch [29/100], Training Loss: 0.3542, Training Accuracy: 89.25%, Validation Loss: 0.4785, Validation Accuracy: 85.31%\n",
      "Epoch [30/100], Training Loss: 0.3466, Training Accuracy: 89.39%, Validation Loss: 0.4715, Validation Accuracy: 85.57%\n",
      "Epoch [31/100], Training Loss: 0.3410, Training Accuracy: 89.61%, Validation Loss: 0.4562, Validation Accuracy: 86.02%\n",
      "Epoch [32/100], Training Loss: 0.3356, Training Accuracy: 89.70%, Validation Loss: 0.4607, Validation Accuracy: 85.91%\n",
      "Epoch [33/100], Training Loss: 0.3324, Training Accuracy: 89.81%, Validation Loss: 0.4652, Validation Accuracy: 85.33%\n",
      "Epoch [34/100], Training Loss: 0.3276, Training Accuracy: 89.95%, Validation Loss: 0.4444, Validation Accuracy: 86.69%\n",
      "Epoch [35/100], Training Loss: 0.3235, Training Accuracy: 90.08%, Validation Loss: 0.4479, Validation Accuracy: 86.27%\n",
      "Epoch [36/100], Training Loss: 0.3148, Training Accuracy: 90.50%, Validation Loss: 0.4393, Validation Accuracy: 86.72%\n",
      "Epoch [37/100], Training Loss: 0.3130, Training Accuracy: 90.49%, Validation Loss: 0.4296, Validation Accuracy: 86.95%\n",
      "Epoch [38/100], Training Loss: 0.3082, Training Accuracy: 90.78%, Validation Loss: 0.4239, Validation Accuracy: 87.00%\n",
      "Epoch [39/100], Training Loss: 0.3060, Training Accuracy: 90.58%, Validation Loss: 0.4297, Validation Accuracy: 86.98%\n",
      "Epoch [40/100], Training Loss: 0.2986, Training Accuracy: 90.82%, Validation Loss: 0.4223, Validation Accuracy: 87.12%\n",
      "Epoch [41/100], Training Loss: 0.2968, Training Accuracy: 90.82%, Validation Loss: 0.4131, Validation Accuracy: 87.55%\n",
      "Epoch [42/100], Training Loss: 0.2933, Training Accuracy: 90.94%, Validation Loss: 0.4208, Validation Accuracy: 87.29%\n",
      "Epoch [43/100], Training Loss: 0.2885, Training Accuracy: 91.10%, Validation Loss: 0.4133, Validation Accuracy: 87.73%\n",
      "Epoch [44/100], Training Loss: 0.2825, Training Accuracy: 91.30%, Validation Loss: 0.4144, Validation Accuracy: 87.50%\n",
      "Epoch [45/100], Training Loss: 0.2818, Training Accuracy: 91.28%, Validation Loss: 0.4140, Validation Accuracy: 87.49%\n",
      "Epoch [46/100], Training Loss: 0.2757, Training Accuracy: 91.59%, Validation Loss: 0.4192, Validation Accuracy: 87.13%\n",
      "Early stopping at epoch 47\n",
      "Result - Validation Loss: 0.4192, Validation Accuracy: 87.13%\n",
      "\n",
      "Testing hyperparameter combination 2/19200: {'learning_rate': 1e-05, 'layer_sizes': [256, 128, 64], 'dropout_rates': [0.1, 0.1, 0.1], 'batch_norm': [True, True, True], 'weight_decay': 0.0, 'batch_size': 16, 'leaky_relu_negative_slope': 0.05, 'num_epochs': 100, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0, 'early_stopping_mode': 'min', 'early_stopping_patience': 5, 'early_stopping_delta': 0.0001}\n",
      "Epoch [1/100], Training Loss: 1.7783, Training Accuracy: 40.30%, Validation Loss: 1.4999, Validation Accuracy: 52.05%\n",
      "Epoch [2/100], Training Loss: 1.1399, Training Accuracy: 65.25%, Validation Loss: 1.2263, Validation Accuracy: 62.12%\n",
      "Epoch [3/100], Training Loss: 0.9405, Training Accuracy: 72.14%, Validation Loss: 1.0860, Validation Accuracy: 66.76%\n",
      "Epoch [4/100], Training Loss: 0.8312, Training Accuracy: 75.35%, Validation Loss: 0.9731, Validation Accuracy: 69.56%\n",
      "Epoch [5/100], Training Loss: 0.7542, Training Accuracy: 77.71%, Validation Loss: 0.9173, Validation Accuracy: 70.91%\n",
      "Epoch [6/100], Training Loss: 0.6964, Training Accuracy: 79.44%, Validation Loss: 0.8593, Validation Accuracy: 73.07%\n",
      "Epoch [7/100], Training Loss: 0.6586, Training Accuracy: 80.44%, Validation Loss: 0.8063, Validation Accuracy: 74.50%\n",
      "Epoch [8/100], Training Loss: 0.6287, Training Accuracy: 81.08%, Validation Loss: 0.7553, Validation Accuracy: 76.24%\n",
      "Epoch [9/100], Training Loss: 0.5920, Training Accuracy: 82.32%, Validation Loss: 0.7351, Validation Accuracy: 76.80%\n",
      "Epoch [10/100], Training Loss: 0.5684, Training Accuracy: 82.97%, Validation Loss: 0.6985, Validation Accuracy: 78.03%\n",
      "Epoch [11/100], Training Loss: 0.5449, Training Accuracy: 83.61%, Validation Loss: 0.6754, Validation Accuracy: 78.43%\n",
      "Epoch [12/100], Training Loss: 0.5260, Training Accuracy: 84.23%, Validation Loss: 0.6625, Validation Accuracy: 79.26%\n",
      "Epoch [13/100], Training Loss: 0.5120, Training Accuracy: 84.47%, Validation Loss: 0.6300, Validation Accuracy: 80.26%\n",
      "Epoch [14/100], Training Loss: 0.4924, Training Accuracy: 85.04%, Validation Loss: 0.6300, Validation Accuracy: 80.35%\n",
      "Epoch [15/100], Training Loss: 0.4814, Training Accuracy: 85.52%, Validation Loss: 0.6129, Validation Accuracy: 81.03%\n",
      "Epoch [16/100], Training Loss: 0.4660, Training Accuracy: 85.90%, Validation Loss: 0.5969, Validation Accuracy: 81.51%\n",
      "Epoch [17/100], Training Loss: 0.4540, Training Accuracy: 86.38%, Validation Loss: 0.5757, Validation Accuracy: 82.06%\n",
      "Epoch [18/100], Training Loss: 0.4439, Training Accuracy: 86.44%, Validation Loss: 0.5590, Validation Accuracy: 82.46%\n",
      "Epoch [19/100], Training Loss: 0.4271, Training Accuracy: 87.01%, Validation Loss: 0.5647, Validation Accuracy: 82.26%\n",
      "Epoch [20/100], Training Loss: 0.4199, Training Accuracy: 87.31%, Validation Loss: 0.5485, Validation Accuracy: 82.87%\n",
      "Epoch [21/100], Training Loss: 0.4136, Training Accuracy: 87.42%, Validation Loss: 0.5331, Validation Accuracy: 83.82%\n",
      "Epoch [22/100], Training Loss: 0.4032, Training Accuracy: 87.73%, Validation Loss: 0.5242, Validation Accuracy: 83.74%\n",
      "Epoch [23/100], Training Loss: 0.3972, Training Accuracy: 88.02%, Validation Loss: 0.5208, Validation Accuracy: 83.75%\n",
      "Epoch [24/100], Training Loss: 0.3885, Training Accuracy: 88.21%, Validation Loss: 0.5064, Validation Accuracy: 84.38%\n",
      "Epoch [25/100], Training Loss: 0.3823, Training Accuracy: 88.30%, Validation Loss: 0.4956, Validation Accuracy: 84.67%\n",
      "Epoch [26/100], Training Loss: 0.3743, Training Accuracy: 88.64%, Validation Loss: 0.4968, Validation Accuracy: 84.62%\n",
      "Epoch [27/100], Training Loss: 0.3683, Training Accuracy: 88.71%, Validation Loss: 0.4897, Validation Accuracy: 85.10%\n",
      "Epoch [28/100], Training Loss: 0.3594, Training Accuracy: 89.00%, Validation Loss: 0.4902, Validation Accuracy: 84.67%\n",
      "Epoch [29/100], Training Loss: 0.3598, Training Accuracy: 88.97%, Validation Loss: 0.4816, Validation Accuracy: 85.16%\n",
      "Epoch [30/100], Training Loss: 0.3499, Training Accuracy: 89.24%, Validation Loss: 0.4773, Validation Accuracy: 85.40%\n",
      "Epoch [31/100], Training Loss: 0.3415, Training Accuracy: 89.50%, Validation Loss: 0.4615, Validation Accuracy: 85.91%\n",
      "Epoch [32/100], Training Loss: 0.3416, Training Accuracy: 89.43%, Validation Loss: 0.4562, Validation Accuracy: 86.10%\n",
      "Epoch [33/100], Training Loss: 0.3363, Training Accuracy: 89.72%, Validation Loss: 0.4531, Validation Accuracy: 86.24%\n",
      "Epoch [34/100], Training Loss: 0.3315, Training Accuracy: 89.75%, Validation Loss: 0.4588, Validation Accuracy: 86.11%\n",
      "Epoch [35/100], Training Loss: 0.3250, Training Accuracy: 89.97%, Validation Loss: 0.4551, Validation Accuracy: 86.15%\n",
      "Epoch [36/100], Training Loss: 0.3215, Training Accuracy: 90.19%, Validation Loss: 0.4477, Validation Accuracy: 86.47%\n",
      "Epoch [37/100], Training Loss: 0.3093, Training Accuracy: 90.51%, Validation Loss: 0.4512, Validation Accuracy: 86.28%\n",
      "Epoch [38/100], Training Loss: 0.3084, Training Accuracy: 90.49%, Validation Loss: 0.4439, Validation Accuracy: 86.60%\n",
      "Epoch [39/100], Training Loss: 0.3073, Training Accuracy: 90.64%, Validation Loss: 0.4416, Validation Accuracy: 86.53%\n",
      "Epoch [40/100], Training Loss: 0.3036, Training Accuracy: 90.64%, Validation Loss: 0.4274, Validation Accuracy: 86.99%\n",
      "Epoch [41/100], Training Loss: 0.2955, Training Accuracy: 90.85%, Validation Loss: 0.4318, Validation Accuracy: 86.62%\n",
      "Epoch [42/100], Training Loss: 0.2958, Training Accuracy: 90.94%, Validation Loss: 0.4229, Validation Accuracy: 87.17%\n",
      "Epoch [43/100], Training Loss: 0.2905, Training Accuracy: 91.05%, Validation Loss: 0.4179, Validation Accuracy: 87.36%\n",
      "Epoch [44/100], Training Loss: 0.2896, Training Accuracy: 91.08%, Validation Loss: 0.4282, Validation Accuracy: 87.11%\n",
      "Epoch [45/100], Training Loss: 0.2887, Training Accuracy: 91.02%, Validation Loss: 0.4323, Validation Accuracy: 86.94%\n",
      "Epoch [46/100], Training Loss: 0.2874, Training Accuracy: 91.13%, Validation Loss: 0.4189, Validation Accuracy: 87.41%\n",
      "Epoch [47/100], Training Loss: 0.2811, Training Accuracy: 91.28%, Validation Loss: 0.4177, Validation Accuracy: 87.06%\n",
      "Epoch [48/100], Training Loss: 0.2747, Training Accuracy: 91.57%, Validation Loss: 0.4173, Validation Accuracy: 87.18%\n",
      "Epoch [49/100], Training Loss: 0.2733, Training Accuracy: 91.64%, Validation Loss: 0.4069, Validation Accuracy: 87.56%\n",
      "Epoch [50/100], Training Loss: 0.2685, Training Accuracy: 91.69%, Validation Loss: 0.4188, Validation Accuracy: 87.46%\n",
      "Epoch [51/100], Training Loss: 0.2675, Training Accuracy: 91.67%, Validation Loss: 0.4056, Validation Accuracy: 87.60%\n",
      "Epoch [52/100], Training Loss: 0.2629, Training Accuracy: 91.83%, Validation Loss: 0.4023, Validation Accuracy: 87.92%\n",
      "Epoch [53/100], Training Loss: 0.2619, Training Accuracy: 91.86%, Validation Loss: 0.4096, Validation Accuracy: 87.51%\n",
      "Epoch [54/100], Training Loss: 0.2617, Training Accuracy: 91.91%, Validation Loss: 0.4021, Validation Accuracy: 87.70%\n",
      "Epoch [55/100], Training Loss: 0.2545, Training Accuracy: 92.02%, Validation Loss: 0.4067, Validation Accuracy: 87.82%\n",
      "Epoch [56/100], Training Loss: 0.2547, Training Accuracy: 92.20%, Validation Loss: 0.3978, Validation Accuracy: 88.03%\n",
      "Epoch [57/100], Training Loss: 0.2504, Training Accuracy: 92.29%, Validation Loss: 0.4022, Validation Accuracy: 87.92%\n",
      "Epoch [58/100], Training Loss: 0.2520, Training Accuracy: 92.19%, Validation Loss: 0.4003, Validation Accuracy: 88.00%\n",
      "Epoch [59/100], Training Loss: 0.2488, Training Accuracy: 92.29%, Validation Loss: 0.3873, Validation Accuracy: 88.35%\n",
      "Epoch [60/100], Training Loss: 0.2485, Training Accuracy: 92.36%, Validation Loss: 0.3910, Validation Accuracy: 88.24%\n",
      "Epoch [61/100], Training Loss: 0.2419, Training Accuracy: 92.57%, Validation Loss: 0.3872, Validation Accuracy: 88.28%\n",
      "Epoch [62/100], Training Loss: 0.2415, Training Accuracy: 92.60%, Validation Loss: 0.3947, Validation Accuracy: 88.13%\n",
      "Epoch [63/100], Training Loss: 0.2332, Training Accuracy: 92.69%, Validation Loss: 0.3862, Validation Accuracy: 88.39%\n",
      "Epoch [64/100], Training Loss: 0.2381, Training Accuracy: 92.53%, Validation Loss: 0.3929, Validation Accuracy: 88.14%\n",
      "Epoch [65/100], Training Loss: 0.2376, Training Accuracy: 92.54%, Validation Loss: 0.3785, Validation Accuracy: 88.78%\n",
      "Epoch [66/100], Training Loss: 0.2298, Training Accuracy: 92.83%, Validation Loss: 0.3708, Validation Accuracy: 88.98%\n",
      "Epoch [67/100], Training Loss: 0.2352, Training Accuracy: 92.65%, Validation Loss: 0.3769, Validation Accuracy: 88.64%\n",
      "Epoch [68/100], Training Loss: 0.2255, Training Accuracy: 92.98%, Validation Loss: 0.3777, Validation Accuracy: 88.57%\n",
      "Epoch [69/100], Training Loss: 0.2263, Training Accuracy: 93.01%, Validation Loss: 0.3867, Validation Accuracy: 88.45%\n",
      "Epoch [70/100], Training Loss: 0.2274, Training Accuracy: 92.96%, Validation Loss: 0.3669, Validation Accuracy: 89.01%\n",
      "Epoch [71/100], Training Loss: 0.2257, Training Accuracy: 92.86%, Validation Loss: 0.3671, Validation Accuracy: 88.92%\n",
      "Epoch [72/100], Training Loss: 0.2235, Training Accuracy: 92.98%, Validation Loss: 0.3707, Validation Accuracy: 89.14%\n",
      "Epoch [73/100], Training Loss: 0.2209, Training Accuracy: 93.11%, Validation Loss: 0.3662, Validation Accuracy: 89.06%\n",
      "Epoch [74/100], Training Loss: 0.2172, Training Accuracy: 93.27%, Validation Loss: 0.3782, Validation Accuracy: 88.46%\n",
      "Epoch [75/100], Training Loss: 0.2161, Training Accuracy: 93.28%, Validation Loss: 0.3713, Validation Accuracy: 88.82%\n",
      "Epoch [76/100], Training Loss: 0.2145, Training Accuracy: 93.28%, Validation Loss: 0.3696, Validation Accuracy: 88.88%\n",
      "Epoch [77/100], Training Loss: 0.2152, Training Accuracy: 93.20%, Validation Loss: 0.3735, Validation Accuracy: 88.91%\n",
      "Epoch [78/100], Training Loss: 0.2121, Training Accuracy: 93.38%, Validation Loss: 0.3707, Validation Accuracy: 88.91%\n",
      "Early stopping at epoch 79\n",
      "Result - Validation Loss: 0.3707, Validation Accuracy: 88.91%\n",
      "\n",
      "Testing hyperparameter combination 3/19200: {'learning_rate': 1e-05, 'layer_sizes': [256, 128, 64], 'dropout_rates': [0.1, 0.1, 0.1], 'batch_norm': [True, True, True], 'weight_decay': 0.0, 'batch_size': 16, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 100, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0, 'early_stopping_mode': 'min', 'early_stopping_patience': 5, 'early_stopping_delta': 0.0001}\n",
      "Epoch [1/100], Training Loss: 1.8247, Training Accuracy: 40.30%, Validation Loss: 1.5090, Validation Accuracy: 53.68%\n",
      "Epoch [2/100], Training Loss: 1.1585, Training Accuracy: 65.67%, Validation Loss: 1.2298, Validation Accuracy: 62.95%\n",
      "Epoch [3/100], Training Loss: 0.9478, Training Accuracy: 72.36%, Validation Loss: 1.0776, Validation Accuracy: 67.83%\n",
      "Epoch [4/100], Training Loss: 0.8302, Training Accuracy: 75.86%, Validation Loss: 0.9877, Validation Accuracy: 69.86%\n",
      "Epoch [5/100], Training Loss: 0.7567, Training Accuracy: 77.91%, Validation Loss: 0.9320, Validation Accuracy: 71.15%\n",
      "Epoch [6/100], Training Loss: 0.7021, Training Accuracy: 79.19%, Validation Loss: 0.8740, Validation Accuracy: 73.22%\n",
      "Epoch [7/100], Training Loss: 0.6614, Training Accuracy: 80.43%, Validation Loss: 0.8202, Validation Accuracy: 74.62%\n",
      "Epoch [8/100], Training Loss: 0.6266, Training Accuracy: 81.25%, Validation Loss: 0.7761, Validation Accuracy: 75.94%\n",
      "Epoch [9/100], Training Loss: 0.5950, Training Accuracy: 82.23%, Validation Loss: 0.7596, Validation Accuracy: 76.50%\n",
      "Epoch [10/100], Training Loss: 0.5701, Training Accuracy: 82.92%, Validation Loss: 0.7436, Validation Accuracy: 76.70%\n",
      "Epoch [11/100], Training Loss: 0.5499, Training Accuracy: 83.48%, Validation Loss: 0.6979, Validation Accuracy: 78.15%\n",
      "Epoch [12/100], Training Loss: 0.5247, Training Accuracy: 84.21%, Validation Loss: 0.6810, Validation Accuracy: 78.61%\n",
      "Epoch [13/100], Training Loss: 0.5116, Training Accuracy: 84.77%, Validation Loss: 0.6614, Validation Accuracy: 79.19%\n",
      "Epoch [14/100], Training Loss: 0.4975, Training Accuracy: 85.04%, Validation Loss: 0.6496, Validation Accuracy: 79.57%\n",
      "Epoch [15/100], Training Loss: 0.4806, Training Accuracy: 85.48%, Validation Loss: 0.6204, Validation Accuracy: 80.65%\n",
      "Epoch [16/100], Training Loss: 0.4703, Training Accuracy: 85.75%, Validation Loss: 0.5953, Validation Accuracy: 81.39%\n",
      "Epoch [17/100], Training Loss: 0.4574, Training Accuracy: 86.19%, Validation Loss: 0.5970, Validation Accuracy: 81.30%\n",
      "Epoch [18/100], Training Loss: 0.4482, Training Accuracy: 86.40%, Validation Loss: 0.5930, Validation Accuracy: 81.42%\n",
      "Epoch [19/100], Training Loss: 0.4353, Training Accuracy: 86.55%, Validation Loss: 0.5818, Validation Accuracy: 81.82%\n",
      "Epoch [20/100], Training Loss: 0.4283, Training Accuracy: 87.02%, Validation Loss: 0.5615, Validation Accuracy: 82.52%\n",
      "Epoch [21/100], Training Loss: 0.4198, Training Accuracy: 87.21%, Validation Loss: 0.5672, Validation Accuracy: 82.30%\n",
      "Epoch [22/100], Training Loss: 0.4077, Training Accuracy: 87.53%, Validation Loss: 0.5504, Validation Accuracy: 82.67%\n",
      "Epoch [23/100], Training Loss: 0.4012, Training Accuracy: 87.71%, Validation Loss: 0.5306, Validation Accuracy: 83.72%\n",
      "Epoch [24/100], Training Loss: 0.3971, Training Accuracy: 87.87%, Validation Loss: 0.5417, Validation Accuracy: 83.05%\n",
      "Epoch [25/100], Training Loss: 0.3843, Training Accuracy: 88.35%, Validation Loss: 0.5383, Validation Accuracy: 82.91%\n",
      "Epoch [26/100], Training Loss: 0.3805, Training Accuracy: 88.28%, Validation Loss: 0.5226, Validation Accuracy: 83.83%\n",
      "Epoch [27/100], Training Loss: 0.3684, Training Accuracy: 88.76%, Validation Loss: 0.5013, Validation Accuracy: 84.26%\n",
      "Epoch [28/100], Training Loss: 0.3644, Training Accuracy: 88.96%, Validation Loss: 0.5081, Validation Accuracy: 84.00%\n",
      "Epoch [29/100], Training Loss: 0.3588, Training Accuracy: 89.15%, Validation Loss: 0.4987, Validation Accuracy: 84.64%\n",
      "Epoch [30/100], Training Loss: 0.3517, Training Accuracy: 89.30%, Validation Loss: 0.4875, Validation Accuracy: 84.88%\n",
      "Epoch [31/100], Training Loss: 0.3512, Training Accuracy: 89.22%, Validation Loss: 0.4713, Validation Accuracy: 85.42%\n",
      "Epoch [32/100], Training Loss: 0.3411, Training Accuracy: 89.53%, Validation Loss: 0.4800, Validation Accuracy: 85.00%\n",
      "Epoch [33/100], Training Loss: 0.3351, Training Accuracy: 89.80%, Validation Loss: 0.4636, Validation Accuracy: 85.71%\n",
      "Epoch [34/100], Training Loss: 0.3308, Training Accuracy: 89.82%, Validation Loss: 0.4755, Validation Accuracy: 85.50%\n",
      "Epoch [35/100], Training Loss: 0.3275, Training Accuracy: 89.84%, Validation Loss: 0.4638, Validation Accuracy: 85.89%\n",
      "Epoch [36/100], Training Loss: 0.3240, Training Accuracy: 90.02%, Validation Loss: 0.4596, Validation Accuracy: 85.72%\n",
      "Epoch [37/100], Training Loss: 0.3227, Training Accuracy: 89.99%, Validation Loss: 0.4581, Validation Accuracy: 86.09%\n",
      "Epoch [38/100], Training Loss: 0.3155, Training Accuracy: 90.25%, Validation Loss: 0.4517, Validation Accuracy: 86.37%\n",
      "Epoch [39/100], Training Loss: 0.3138, Training Accuracy: 90.42%, Validation Loss: 0.4452, Validation Accuracy: 86.53%\n",
      "Epoch [40/100], Training Loss: 0.3058, Training Accuracy: 90.42%, Validation Loss: 0.4422, Validation Accuracy: 86.54%\n",
      "Epoch [41/100], Training Loss: 0.2986, Training Accuracy: 90.80%, Validation Loss: 0.4343, Validation Accuracy: 86.62%\n",
      "Epoch [42/100], Training Loss: 0.2984, Training Accuracy: 90.74%, Validation Loss: 0.4299, Validation Accuracy: 86.93%\n",
      "Epoch [43/100], Training Loss: 0.2947, Training Accuracy: 90.92%, Validation Loss: 0.4242, Validation Accuracy: 87.00%\n",
      "Epoch [44/100], Training Loss: 0.2938, Training Accuracy: 90.95%, Validation Loss: 0.4368, Validation Accuracy: 86.77%\n",
      "Epoch [45/100], Training Loss: 0.2885, Training Accuracy: 91.03%, Validation Loss: 0.4175, Validation Accuracy: 87.43%\n",
      "Epoch [46/100], Training Loss: 0.2870, Training Accuracy: 91.13%, Validation Loss: 0.4260, Validation Accuracy: 86.84%\n",
      "Epoch [47/100], Training Loss: 0.2842, Training Accuracy: 91.23%, Validation Loss: 0.4200, Validation Accuracy: 87.15%\n",
      "Epoch [48/100], Training Loss: 0.2838, Training Accuracy: 91.16%, Validation Loss: 0.4086, Validation Accuracy: 87.73%\n",
      "Epoch [49/100], Training Loss: 0.2773, Training Accuracy: 91.39%, Validation Loss: 0.4079, Validation Accuracy: 87.46%\n",
      "Epoch [50/100], Training Loss: 0.2737, Training Accuracy: 91.41%, Validation Loss: 0.4138, Validation Accuracy: 87.58%\n",
      "Epoch [51/100], Training Loss: 0.2729, Training Accuracy: 91.54%, Validation Loss: 0.4202, Validation Accuracy: 86.90%\n",
      "Epoch [52/100], Training Loss: 0.2717, Training Accuracy: 91.46%, Validation Loss: 0.4059, Validation Accuracy: 87.77%\n",
      "Epoch [53/100], Training Loss: 0.2698, Training Accuracy: 91.70%, Validation Loss: 0.4280, Validation Accuracy: 86.79%\n",
      "Epoch [54/100], Training Loss: 0.2623, Training Accuracy: 91.88%, Validation Loss: 0.3959, Validation Accuracy: 88.02%\n",
      "Epoch [55/100], Training Loss: 0.2625, Training Accuracy: 91.87%, Validation Loss: 0.4026, Validation Accuracy: 87.75%\n",
      "Epoch [56/100], Training Loss: 0.2632, Training Accuracy: 91.85%, Validation Loss: 0.3993, Validation Accuracy: 87.89%\n",
      "Epoch [57/100], Training Loss: 0.2553, Training Accuracy: 92.02%, Validation Loss: 0.4016, Validation Accuracy: 87.64%\n",
      "Epoch [58/100], Training Loss: 0.2549, Training Accuracy: 92.10%, Validation Loss: 0.3900, Validation Accuracy: 88.06%\n",
      "Epoch [59/100], Training Loss: 0.2546, Training Accuracy: 92.07%, Validation Loss: 0.4021, Validation Accuracy: 87.90%\n",
      "Epoch [60/100], Training Loss: 0.2535, Training Accuracy: 92.24%, Validation Loss: 0.3901, Validation Accuracy: 88.18%\n",
      "Epoch [61/100], Training Loss: 0.2473, Training Accuracy: 92.32%, Validation Loss: 0.4033, Validation Accuracy: 87.38%\n",
      "Epoch [62/100], Training Loss: 0.2450, Training Accuracy: 92.42%, Validation Loss: 0.3824, Validation Accuracy: 88.53%\n",
      "Epoch [63/100], Training Loss: 0.2438, Training Accuracy: 92.43%, Validation Loss: 0.3874, Validation Accuracy: 88.16%\n",
      "Epoch [64/100], Training Loss: 0.2424, Training Accuracy: 92.41%, Validation Loss: 0.3830, Validation Accuracy: 88.38%\n",
      "Epoch [65/100], Training Loss: 0.2414, Training Accuracy: 92.53%, Validation Loss: 0.3925, Validation Accuracy: 88.04%\n",
      "Epoch [66/100], Training Loss: 0.2360, Training Accuracy: 92.63%, Validation Loss: 0.3966, Validation Accuracy: 87.81%\n",
      "Epoch [67/100], Training Loss: 0.2368, Training Accuracy: 92.74%, Validation Loss: 0.3834, Validation Accuracy: 88.31%\n",
      "Early stopping at epoch 68\n",
      "Result - Validation Loss: 0.3834, Validation Accuracy: 88.31%\n",
      "\n",
      "Testing hyperparameter combination 4/19200: {'learning_rate': 1e-05, 'layer_sizes': [256, 128, 64], 'dropout_rates': [0.1, 0.1, 0.1], 'batch_norm': [True, True, True], 'weight_decay': 0.0, 'batch_size': 16, 'leaky_relu_negative_slope': 0.2, 'num_epochs': 100, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0, 'early_stopping_mode': 'min', 'early_stopping_patience': 5, 'early_stopping_delta': 0.0001}\n",
      "Epoch [1/100], Training Loss: 1.6869, Training Accuracy: 44.76%, Validation Loss: 1.4448, Validation Accuracy: 54.87%\n",
      "Epoch [2/100], Training Loss: 1.0589, Training Accuracy: 67.97%, Validation Loss: 1.1961, Validation Accuracy: 63.06%\n",
      "Epoch [3/100], Training Loss: 0.8889, Training Accuracy: 73.26%, Validation Loss: 1.0824, Validation Accuracy: 66.02%\n",
      "Epoch [4/100], Training Loss: 0.7952, Training Accuracy: 76.12%, Validation Loss: 0.9692, Validation Accuracy: 69.58%\n",
      "Epoch [5/100], Training Loss: 0.7343, Training Accuracy: 77.91%, Validation Loss: 0.9147, Validation Accuracy: 71.37%\n",
      "Epoch [6/100], Training Loss: 0.6863, Training Accuracy: 79.21%, Validation Loss: 0.8618, Validation Accuracy: 72.50%\n",
      "Epoch [7/100], Training Loss: 0.6439, Training Accuracy: 80.64%, Validation Loss: 0.8176, Validation Accuracy: 74.19%\n",
      "Epoch [8/100], Training Loss: 0.6145, Training Accuracy: 81.25%, Validation Loss: 0.7820, Validation Accuracy: 75.53%\n",
      "Epoch [9/100], Training Loss: 0.5887, Training Accuracy: 82.18%, Validation Loss: 0.7495, Validation Accuracy: 76.41%\n",
      "Epoch [10/100], Training Loss: 0.5684, Training Accuracy: 82.80%, Validation Loss: 0.7337, Validation Accuracy: 76.57%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 281\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;66;03m# Call train_and_evaluate and record the computation time\u001b[39;00m\n\u001b[1;32m    280\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 281\u001b[0m     val_loss, val_accuracy, training_log \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    283\u001b[0m     computation_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "Cell \u001b[0;32mIn[1], line 172\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(hparams)\u001b[0m\n\u001b[1;32m    169\u001b[0m correct_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    170\u001b[0m total_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 172\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/blue/hulcr/gmarais/conda/envs/EEL/lib/python3.11/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m/blue/hulcr/gmarais/conda/envs/EEL/lib/python3.11/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/blue/hulcr/gmarais/conda/envs/EEL/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/blue/hulcr/gmarais/conda/envs/EEL/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/blue/hulcr/gmarais/conda/envs/EEL/lib/python3.11/site-packages/torchvision/datasets/mnist.py:146\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    143\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m/blue/hulcr/gmarais/conda/envs/EEL/lib/python3.11/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m/blue/hulcr/gmarais/conda/envs/EEL/lib/python3.11/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/blue/hulcr/gmarais/conda/envs/EEL/lib/python3.11/site-packages/torchvision/transforms/functional.py:168\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# handle PIL Image\u001b[39;00m\n\u001b[1;32m    167\u001b[0m mode_to_nptype \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint32, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI;16\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mbyteorder \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlittle\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI;16B\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint16, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mfloat32}\n\u001b[0;32m--> 168\u001b[0m img \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39marray(pic, mode_to_nptype\u001b[38;5;241m.\u001b[39mget(pic\u001b[38;5;241m.\u001b[39mmode, np\u001b[38;5;241m.\u001b[39muint8), copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    171\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m img\n",
      "File \u001b[0;32m/blue/hulcr/gmarais/conda/envs/EEL/lib/python3.11/site-packages/PIL/Image.py:756\u001b[0m, in \u001b[0;36mImage.__array_interface__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    754\u001b[0m     new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtobytes(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 756\u001b[0m     new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    757\u001b[0m new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m], new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtypestr\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _conv_type_shape(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    758\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new\n",
      "File \u001b[0;32m/blue/hulcr/gmarais/conda/envs/EEL/lib/python3.11/site-packages/PIL/Image.py:818\u001b[0m, in \u001b[0;36mImage.tobytes\u001b[0;34m(self, encoder_name, *args)\u001b[0m\n\u001b[1;32m    816\u001b[0m output \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 818\u001b[0m     bytes_consumed, errcode, data \u001b[38;5;241m=\u001b[39m \u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    819\u001b[0m     output\u001b[38;5;241m.\u001b[39mappend(data)\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errcode:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import csv\n",
    "import time\n",
    "import json\n",
    "import hashlib\n",
    "\n",
    "from MLP_model import MLPClassifier  # Import the MLPClassifier class\n",
    "from itertools import product\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(m.weight, nonlinearity='leaky_relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),  # Adjust if necessary\n",
    "    transforms.Lambda(lambda x: x.view(-1))  # Flatten the image\n",
    "])\n",
    "\n",
    "# Load the KMNIST dataset\n",
    "train_dataset = datasets.KMNIST(root='../data', train=True, transform=transform, download=True)\n",
    "val_dataset = datasets.KMNIST(root='../data', train=False, transform=transform, download=True)\n",
    "\n",
    "# Input and output sizes\n",
    "input_size = 784  # For 28x28 images\n",
    "output_size = 10  # Number of classes\n",
    "\n",
    "# Hyperparameter grid for tuning\n",
    "hyperparameter_grid = {\n",
    "    'learning_rate': [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3],\n",
    "    'layer_sizes': [[256, 128, 64], [512, 256, 128], [1024, 512, 256], [2048, 1024, 512]],\n",
    "    'dropout_rates': [[0.1, 0.1, 0.1], [0.2, 0.2, 0.2], [0.3, 0.3, 0.3], [0.4, 0.4, 0.4], [0.5, 0.5, 0.5]],\n",
    "    'batch_norm': [[True, True, True], [False, False, False]],\n",
    "    'weight_decay': [0.0, 1e-5, 1e-4, 1e-3],\n",
    "    'batch_size': [16, 32, 64, 128, 256],\n",
    "    'leaky_relu_negative_slope': [0.01, 0.05, 0.1, 0.2],  # Negative slope for LeakyReLU\n",
    "    'num_epochs': [100],  # Max number of training epochs\n",
    "    # ReduceLROnPlateau hyperparameters\n",
    "    'scheduler_mode': ['min'],  # Mode for the scheduler\n",
    "    'scheduler_factor': [0.1],  # Factor by which the learning rate will be reduced\n",
    "    'scheduler_patience': [3],  # Number of epochs with no improvement after which learning rate will be reduced\n",
    "    'scheduler_threshold': [1e-4],  # Threshold for measuring the new optimum\n",
    "    'scheduler_cooldown': [0],  # Number of epochs to wait before resuming normal operation after lr has been reduced\n",
    "    # Early Stopping hyperparameters\n",
    "    'early_stopping_mode': ['min'],  # Mode for early stopping\n",
    "    'early_stopping_patience': [5],  # Patience for early stopping\n",
    "    'early_stopping_delta': [1e-4],  # Minimum change to qualify as improvement\n",
    "}\n",
    "\n",
    "# Generate all combinations of hyperparameters\n",
    "keys, values = zip(*hyperparameter_grid.items())\n",
    "hyperparameter_combinations = [dict(zip(keys, v)) for v in product(*values)]\n",
    "\n",
    "csv_filename = 'MLP_hyperparameter_tuning_results.csv'\n",
    "\n",
    "# Initialize an empty set to store hashes of existing results\n",
    "existing_hashes = set()\n",
    "\n",
    "if os.path.exists(csv_filename):\n",
    "    with open(csv_filename, 'r', newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            hparams_hash = row['hparams_hash']\n",
    "            existing_hashes.add(hparams_hash)\n",
    "else:\n",
    "    # If the file does not exist, we'll create it later\n",
    "    pass\n",
    "\n",
    "def train_and_evaluate(hparams):\n",
    "    # Unpack hyperparameters\n",
    "    learning_rate = hparams['learning_rate']\n",
    "    layer_sizes = hparams['layer_sizes']\n",
    "    dropout_rates = hparams['dropout_rates']\n",
    "    batch_norm = hparams['batch_norm']\n",
    "    weight_decay = hparams['weight_decay']\n",
    "    batch_size = hparams['batch_size']\n",
    "    leaky_relu_negative_slope = hparams['leaky_relu_negative_slope']\n",
    "    num_epochs = hparams.get('num_epochs', 50)\n",
    "    \n",
    "    # Scheduler hyperparameters\n",
    "    scheduler_mode = hparams['scheduler_mode']\n",
    "    scheduler_factor = hparams['scheduler_factor']\n",
    "    scheduler_patience = hparams['scheduler_patience']\n",
    "    scheduler_threshold = hparams['scheduler_threshold']\n",
    "    scheduler_cooldown = hparams['scheduler_cooldown']\n",
    "    \n",
    "    # Early Stopping hyperparameters\n",
    "    early_stopping_mode = hparams['early_stopping_mode']\n",
    "    early_stopping_patience = hparams['early_stopping_patience']\n",
    "    early_stopping_delta = hparams['early_stopping_delta']\n",
    "    \n",
    "    # Define activation functions: Always LeakyReLU with specified negative slope\n",
    "    activation_functions = [nn.LeakyReLU(negative_slope=leaky_relu_negative_slope) for _ in layer_sizes]\n",
    "\n",
    "    # Initialize the model\n",
    "    model = MLPClassifier(\n",
    "        input_size=input_size,\n",
    "        layer_sizes=layer_sizes,\n",
    "        output_size=output_size,\n",
    "        activation_functions=activation_functions,\n",
    "        dropout_rates=dropout_rates,\n",
    "        batch_norm=batch_norm,\n",
    "        weight_init=init_weights\n",
    "    ).to(device)\n",
    "\n",
    "    # Optimizer (always AdamW)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Scheduler: ReduceLROnPlateau\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode=scheduler_mode,\n",
    "        factor=scheduler_factor,\n",
    "        patience=scheduler_patience,\n",
    "        threshold=scheduler_threshold,\n",
    "        threshold_mode='rel',\n",
    "        cooldown=scheduler_cooldown,\n",
    "    )\n",
    "\n",
    "    # Data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Loss function\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Early Stopping variables\n",
    "    best_metric = None\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "\n",
    "    # Determine the comparison operator based on mode\n",
    "    if early_stopping_mode == 'min':\n",
    "        def is_improvement(current, best):\n",
    "            return current < best - early_stopping_delta\n",
    "        best_metric = float('inf')\n",
    "    elif early_stopping_mode == 'max':\n",
    "        def is_improvement(current, best):\n",
    "            return current > best + early_stopping_delta\n",
    "        best_metric = float('-inf')\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported early_stopping_mode: {early_stopping_mode}\")\n",
    "\n",
    "    # Initialize variables for logging\n",
    "    initial_lr = optimizer.param_groups[0]['lr']\n",
    "    current_lr = initial_lr\n",
    "    lr_reduction_epochs = []\n",
    "    epoch_logs = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        if early_stop:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for data in train_loader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Calculate training accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_accuracy = 100 * correct_train / total_train\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_function(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Calculate validation accuracy\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = 100 * correct_val / total_val\n",
    "\n",
    "        # Scheduler step: ReduceLROnPlateau expects a metric to monitor\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Check if learning rate has been reduced\n",
    "        new_lr = optimizer.param_groups[0]['lr']\n",
    "        if new_lr < current_lr:\n",
    "            lr_reduction_epochs.append(epoch+1)  # Epochs are 1-indexed\n",
    "            current_lr = new_lr\n",
    "\n",
    "        # Record per-epoch logs\n",
    "        epoch_logs.append({\n",
    "            'epoch': epoch+1,\n",
    "            'train_loss': train_loss,\n",
    "            'train_accuracy': train_accuracy,\n",
    "            'val_loss': val_loss,\n",
    "            'val_accuracy': val_accuracy,\n",
    "            'learning_rate': current_lr\n",
    "        })\n",
    "\n",
    "        # Early Stopping check\n",
    "        if is_improvement(val_loss, best_metric):\n",
    "            best_metric = val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= early_stopping_patience:\n",
    "            early_stop = True\n",
    "\n",
    "        # Optionally, print progress\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "              f\"Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%, \"\n",
    "              f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "    # Prepare training log\n",
    "    training_log = {\n",
    "        'final_epoch': epoch+1,\n",
    "        'lr_reduction_epochs': lr_reduction_epochs,\n",
    "        'epoch_logs': epoch_logs\n",
    "    }\n",
    "\n",
    "    # Return final validation loss and accuracy, and training log\n",
    "    return val_loss, val_accuracy, training_log\n",
    "\n",
    "# Main loop for hyperparameter tuning\n",
    "results = []\n",
    "\n",
    "for idx, hparams in enumerate(hyperparameter_combinations):\n",
    "    # Generate a unique id for the hyperparameters\n",
    "    hparams_str = json.dumps(hparams, sort_keys=True)\n",
    "    hparams_hash = hashlib.md5(hparams_str.encode('utf-8')).hexdigest()\n",
    "    \n",
    "    # Check if this combination exists in existing_results\n",
    "    if hparams_hash in existing_hashes:\n",
    "        print(f\"Skipping hyperparameter combination {idx+1}/{len(hyperparameter_combinations)}: {hparams} (already tested)\")\n",
    "        continue\n",
    "    else:\n",
    "        print(f\"Testing hyperparameter combination {idx+1}/{len(hyperparameter_combinations)}: {hparams}\")\n",
    "        try:\n",
    "            # Call train_and_evaluate and record the computation time\n",
    "            start_time = time.time()\n",
    "            val_loss, val_accuracy, training_log = train_and_evaluate(hparams)\n",
    "            end_time = time.time()\n",
    "            computation_time = end_time - start_time\n",
    "\n",
    "            # Save the per-epoch logs to a file\n",
    "            log_filename = f\"training_log_{hparams_hash}.json\"\n",
    "            with open(log_filename, 'w') as f:\n",
    "                json.dump(training_log, f)\n",
    "            \n",
    "            # Append the result to the CSV file\n",
    "            with open(csv_filename, 'a', newline='') as csvfile:\n",
    "                fieldnames = ['hparams_hash'] + list(hparams.keys()) + ['val_loss', 'val_accuracy', 'computation_time', 'final_num_epochs', 'lr_reduction_epochs', 'log_filename']\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                # If the file is new, write the header\n",
    "                if csvfile.tell() == 0:\n",
    "                    writer.writeheader()\n",
    "                result_row = {'hparams_hash': hparams_hash}\n",
    "                for key, value in hparams.items():\n",
    "                    # Convert value to string using json.dumps\n",
    "                    result_row[key] = json.dumps(value)\n",
    "                result_row.update({\n",
    "                    'val_loss': val_loss,\n",
    "                    'val_accuracy': val_accuracy,\n",
    "                    'computation_time': computation_time,\n",
    "                    'final_num_epochs': training_log['final_epoch'],\n",
    "                    'lr_reduction_epochs': json.dumps(training_log['lr_reduction_epochs']),\n",
    "                    'log_filename': log_filename\n",
    "                })\n",
    "                writer.writerow(result_row)\n",
    "\n",
    "            # Update the set of existing hashes\n",
    "            existing_hashes.add(hparams_hash)\n",
    "\n",
    "            results.append({'hparams': hparams, 'val_loss': val_loss, 'val_accuracy': val_accuracy})\n",
    "            print(f\"Result - Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error with hyperparameters {hparams}: {e}\\n\")\n",
    "            continue\n",
    "\n",
    "# Find the best hyperparameters based on validation accuracy\n",
    "best_result = max(results, key=lambda x: x['val_accuracy'])\n",
    "print(\"Best hyperparameters based on validation accuracy:\")\n",
    "print(best_result['hparams'])\n",
    "print(f\"Validation Loss: {best_result['val_loss']:.4f}, Validation Accuracy: {best_result['val_accuracy']:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EEL",
   "language": "EEL",
   "name": "eel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

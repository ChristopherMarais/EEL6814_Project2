{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current global best validation loss from CSV: 0.1392\n",
      "Skipping hyperparameter combination 1/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.0, 'batch_size': 16, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 2/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.0, 'batch_size': 16, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 3/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.0, 'batch_size': 32, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 4/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.0, 'batch_size': 32, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 5/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.0, 'batch_size': 64, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 6/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.0, 'batch_size': 64, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 7/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.0, 'batch_size': 128, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 8/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.0, 'batch_size': 128, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 9/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 1e-05, 'batch_size': 16, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 10/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 1e-05, 'batch_size': 16, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 11/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 1e-05, 'batch_size': 32, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 12/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 1e-05, 'batch_size': 32, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 13/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 1e-05, 'batch_size': 64, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 14/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 1e-05, 'batch_size': 64, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 15/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 1e-05, 'batch_size': 128, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 16/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 1e-05, 'batch_size': 128, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 17/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.0001, 'batch_size': 16, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 18/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.0001, 'batch_size': 16, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 19/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.0001, 'batch_size': 32, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 20/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.0001, 'batch_size': 32, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 21/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.0001, 'batch_size': 64, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 22/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.0001, 'batch_size': 64, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 23/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.0001, 'batch_size': 128, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 24/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.0001, 'batch_size': 128, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 25/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.001, 'batch_size': 16, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 26/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.001, 'batch_size': 16, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 27/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.001, 'batch_size': 32, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 28/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.001, 'batch_size': 32, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 29/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.001, 'batch_size': 64, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 30/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.001, 'batch_size': 64, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 31/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.001, 'batch_size': 128, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 32/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.001, 'batch_size': 128, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 33/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.0, 'batch_size': 16, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 34/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.0, 'batch_size': 16, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 35/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.0, 'batch_size': 32, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 36/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.0, 'batch_size': 32, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 37/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.0, 'batch_size': 64, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 38/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.0, 'batch_size': 64, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 39/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.0, 'batch_size': 128, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 40/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.0, 'batch_size': 128, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 41/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 1e-05, 'batch_size': 16, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 42/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 1e-05, 'batch_size': 16, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 43/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 1e-05, 'batch_size': 32, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 44/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 1e-05, 'batch_size': 32, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Testing hyperparameter combination 45/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 1e-05, 'batch_size': 64, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0}\n",
      "Epoch [1/200], Training Loss: 0.4616, Validation Loss: 0.3542\n",
      "Epoch [2/200], Training Loss: 0.3950, Validation Loss: 0.3288\n",
      "Epoch [3/200], Training Loss: 0.3815, Validation Loss: 0.3194\n",
      "Epoch [4/200], Training Loss: 0.3735, Validation Loss: 0.3130\n",
      "Epoch [5/200], Training Loss: 0.3675, Validation Loss: 0.3075\n",
      "Epoch [6/200], Training Loss: 0.3633, Validation Loss: 0.3023\n",
      "Epoch [7/200], Training Loss: 0.3595, Validation Loss: 0.2999\n",
      "Epoch [8/200], Training Loss: 0.3564, Validation Loss: 0.2966\n",
      "Epoch [9/200], Training Loss: 0.3538, Validation Loss: 0.2929\n",
      "Epoch [10/200], Training Loss: 0.3515, Validation Loss: 0.2918\n",
      "Epoch [11/200], Training Loss: 0.3495, Validation Loss: 0.2884\n",
      "Epoch [12/200], Training Loss: 0.3477, Validation Loss: 0.2872\n",
      "Epoch [13/200], Training Loss: 0.3460, Validation Loss: 0.2847\n",
      "Epoch [14/200], Training Loss: 0.3444, Validation Loss: 0.2837\n",
      "Epoch [15/200], Training Loss: 0.3428, Validation Loss: 0.2808\n",
      "Epoch [16/200], Training Loss: 0.3413, Validation Loss: 0.2785\n",
      "Epoch [17/200], Training Loss: 0.3400, Validation Loss: 0.2790\n",
      "Epoch [18/200], Training Loss: 0.3388, Validation Loss: 0.2788\n",
      "Epoch [19/200], Training Loss: 0.3371, Validation Loss: 0.2768\n",
      "Epoch [20/200], Training Loss: 0.3361, Validation Loss: 0.2749\n",
      "Epoch [21/200], Training Loss: 0.3347, Validation Loss: 0.2740\n",
      "Epoch [22/200], Training Loss: 0.3332, Validation Loss: 0.2718\n",
      "Epoch [23/200], Training Loss: 0.3319, Validation Loss: 0.2696\n",
      "Epoch [24/200], Training Loss: 0.3309, Validation Loss: 0.2701\n",
      "Epoch [25/200], Training Loss: 0.3297, Validation Loss: 0.2691\n",
      "Epoch [26/200], Training Loss: 0.3285, Validation Loss: 0.2674\n",
      "Epoch [27/200], Training Loss: 0.3276, Validation Loss: 0.2677\n",
      "Epoch [28/200], Training Loss: 0.3267, Validation Loss: 0.2654\n",
      "Epoch [29/200], Training Loss: 0.3256, Validation Loss: 0.2645\n",
      "Epoch [30/200], Training Loss: 0.3249, Validation Loss: 0.2622\n",
      "Epoch [31/200], Training Loss: 0.3238, Validation Loss: 0.2624\n",
      "Epoch [32/200], Training Loss: 0.3229, Validation Loss: 0.2609\n",
      "Epoch [33/200], Training Loss: 0.3223, Validation Loss: 0.2598\n",
      "Epoch [34/200], Training Loss: 0.3213, Validation Loss: 0.2597\n",
      "Epoch [35/200], Training Loss: 0.3205, Validation Loss: 0.2580\n",
      "Epoch [36/200], Training Loss: 0.3196, Validation Loss: 0.2581\n",
      "Epoch [37/200], Training Loss: 0.3189, Validation Loss: 0.2563\n",
      "Epoch [38/200], Training Loss: 0.3182, Validation Loss: 0.2565\n",
      "Epoch [39/200], Training Loss: 0.3174, Validation Loss: 0.2553\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import csv\n",
    "import time\n",
    "import json\n",
    "import hashlib\n",
    "import random\n",
    "from SAE_model import StackedAutoencoder  # Import the SAE class\n",
    "from itertools import product\n",
    "\n",
    "# Function to set all random seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # If you are using multi-GPU.\n",
    "        \n",
    "# Set the base seed\n",
    "set_seed(42)\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(m.weight, nonlinearity='leaky_relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),  # Normalize to [-1, 1]\n",
    "    transforms.Lambda(lambda x: x.view(-1))  # Flatten the image to a vector\n",
    "])\n",
    "\n",
    "# Load the KMNIST dataset\n",
    "train_dataset = datasets.KMNIST(root='../data', train=True, transform=transform, download=True)\n",
    "val_dataset = datasets.KMNIST(root='../data', train=False, transform=transform, download=True)\n",
    "\n",
    "# Hyperparameter grid for tuning\n",
    "hyperparameter_grid = {\n",
    "    'learning_rate': [1e-3, 1e-4, 1e-5],\n",
    "    'layer_sizes': [\n",
    "        [800, 200, 25],\n",
    "        [800, 200, 50],\n",
    "        [800, 200, 100],\n",
    "        [800, 200, 150]\n",
    "    ],\n",
    "    'dropout_rates': [[0.0, 0.0, 0.0], [0.1, 0.1, 0.1], [0.25, 0.25, 0.25], [0.5, 0.5, 0.5]],\n",
    "    'weight_decay': [0.0, 1e-5, 1e-4, 1e-3],\n",
    "    'batch_size': [16, 32, 64, 128],\n",
    "    'leaky_relu_negative_slope': [0.01, 0.1],\n",
    "    'num_epochs': [200],  # Max number of training epochs\n",
    "    # Early Stopping hyperparameters\n",
    "    'early_stopping_patience': [5],\n",
    "    'early_stopping_min_delta': [1e-4],\n",
    "    # ReduceLROnPlateau hyperparameters\n",
    "    'scheduler_mode': ['min'],\n",
    "    'scheduler_factor': [0.1],\n",
    "    'scheduler_patience': [3],\n",
    "    'scheduler_threshold': [1e-4],\n",
    "    'scheduler_cooldown': [0]\n",
    "}\n",
    "\n",
    "# Generate all combinations of hyperparameters\n",
    "keys, values = zip(*hyperparameter_grid.items())\n",
    "hyperparameter_combinations = [dict(zip(keys, v)) for v in product(*values)]\n",
    "\n",
    "csv_filename = 'SAE_hyperparameter_tuning_results.csv'\n",
    "\n",
    "# Initialize an empty set to store hashes of existing results\n",
    "existing_hashes = set()\n",
    "\n",
    "# Initialize global_best_val_loss by reading existing CSV\n",
    "global_best_val_loss = float('inf')  # Initialize to infinity\n",
    "\n",
    "if os.path.exists(csv_filename):\n",
    "    with open(csv_filename, 'r', newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            hparams_hash = row['hparams_hash']\n",
    "            existing_hashes.add(hparams_hash)\n",
    "            try:\n",
    "                val_loss = float(row['val_loss'])\n",
    "                if val_loss < global_best_val_loss:\n",
    "                    global_best_val_loss = val_loss\n",
    "            except ValueError:\n",
    "                # If val_loss is not a float, skip\n",
    "                continue\n",
    "    print(f\"Current global best validation loss from CSV: {global_best_val_loss:.4f}\")\n",
    "else:\n",
    "    print(\"CSV file does not exist. Starting fresh.\")\n",
    "    # If the file does not exist, we'll create it later\n",
    "\n",
    "def train_and_evaluate(hparams):\n",
    "    # Unpack hyperparameters\n",
    "    learning_rate = hparams['learning_rate']\n",
    "    layer_sizes = hparams['layer_sizes']\n",
    "    dropout_rates = hparams['dropout_rates']\n",
    "    weight_decay = hparams['weight_decay']\n",
    "    batch_size = hparams['batch_size']\n",
    "    leaky_relu_negative_slope = hparams['leaky_relu_negative_slope']\n",
    "    num_epochs = hparams.get('num_epochs', 50)\n",
    "    \n",
    "    # Early Stopping hyperparameters\n",
    "    early_stopping_patience = hparams['early_stopping_patience']\n",
    "    early_stopping_min_delta = hparams['early_stopping_min_delta']\n",
    "    \n",
    "    # Scheduler hyperparameters\n",
    "    scheduler_mode = hparams['scheduler_mode']\n",
    "    scheduler_factor = hparams['scheduler_factor']\n",
    "    scheduler_patience = hparams['scheduler_patience']\n",
    "    scheduler_threshold = hparams['scheduler_threshold']\n",
    "    scheduler_cooldown = hparams['scheduler_cooldown']\n",
    "    \n",
    "    # Define activation functions\n",
    "    activation_functions = [nn.LeakyReLU(negative_slope=leaky_relu_negative_slope) for _ in layer_sizes]\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = StackedAutoencoder(\n",
    "        input_size=784,  # For 28x28 images\n",
    "        layer_sizes=layer_sizes,\n",
    "        activation_functions=activation_functions,\n",
    "        dropout_rates=dropout_rates,\n",
    "        weight_init=init_weights\n",
    "    ).to(device)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    # Scheduler: ReduceLROnPlateau\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode=scheduler_mode,\n",
    "        factor=scheduler_factor,\n",
    "        patience=scheduler_patience,\n",
    "        threshold=scheduler_threshold,\n",
    "        threshold_mode='rel',\n",
    "        cooldown=scheduler_cooldown,\n",
    "    )\n",
    "    \n",
    "    # Data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Loss function\n",
    "    loss_function = nn.MSELoss()\n",
    "    \n",
    "    # Early Stopping variables\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "    \n",
    "    # Initialize variables for logging\n",
    "    initial_lr = optimizer.param_groups[0]['lr']\n",
    "    current_lr = initial_lr\n",
    "    lr_reduction_epochs = []\n",
    "    epoch_logs = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        if early_stop:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for data in train_loader:\n",
    "            inputs, _ = data\n",
    "            inputs = inputs.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, inputs)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                inputs, _ = data\n",
    "                inputs = inputs.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_function(outputs, inputs)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        # Scheduler step\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Check if learning rate has been reduced\n",
    "        new_lr = optimizer.param_groups[0]['lr']\n",
    "        if new_lr < current_lr:\n",
    "            lr_reduction_epochs.append(epoch+1)  # Epochs are 1-indexed\n",
    "            current_lr = new_lr\n",
    "        \n",
    "        # Record per-epoch logs\n",
    "        epoch_logs.append({\n",
    "            'epoch': epoch+1,\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'learning_rate': current_lr\n",
    "        })\n",
    "        \n",
    "        # Early Stopping check\n",
    "        if val_loss < best_val_loss - early_stopping_min_delta:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        \n",
    "        if epochs_no_improve >= early_stopping_patience:\n",
    "            early_stop = True\n",
    "        \n",
    "        # Optionally, print progress\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "              f\"Training Loss: {train_loss:.4f}, \"\n",
    "              f\"Validation Loss: {val_loss:.4f}\")\n",
    "        \n",
    "    # Prepare training log\n",
    "    training_log = {\n",
    "        'final_epoch': epoch+1,\n",
    "        'lr_reduction_epochs': lr_reduction_epochs,\n",
    "        'epoch_logs': epoch_logs\n",
    "    }\n",
    "    \n",
    "    # Return final validation loss, training log, and the trained model\n",
    "    return val_loss, training_log, model, activation_functions, optimizer, scheduler\n",
    "\n",
    "# Main loop for hyperparameter tuning\n",
    "results = []\n",
    "\n",
    "for idx, hparams in enumerate(hyperparameter_combinations):\n",
    "    # Set the base seed\n",
    "    set_seed(42)\n",
    "    # Generate a unique id for the hyperparameters\n",
    "    hparams_str = json.dumps(hparams, sort_keys=True)\n",
    "    hparams_hash = hashlib.md5(hparams_str.encode('utf-8')).hexdigest()\n",
    "    \n",
    "    # Check if this combination exists in existing_results\n",
    "    if hparams_hash in existing_hashes:\n",
    "        print(f\"Skipping hyperparameter combination {idx+1}/{len(hyperparameter_combinations)}: {hparams} (already tested)\")\n",
    "        continue\n",
    "    else:\n",
    "        print(f\"Testing hyperparameter combination {idx+1}/{len(hyperparameter_combinations)}: {hparams}\")\n",
    "        try:\n",
    "            # Call train_and_evaluate and record the computation time\n",
    "            start_time = time.time()\n",
    "            val_loss, training_log, model, activation_functions, optimizer, scheduler = train_and_evaluate(hparams)\n",
    "            end_time = time.time()\n",
    "            computation_time = end_time - start_time\n",
    "\n",
    "            # Save the per-epoch logs to a file\n",
    "            log_filename = f\"misc/training_log_{hparams_hash}.json\"\n",
    "            with open(log_filename, 'w') as f:\n",
    "                json.dump(training_log, f)\n",
    "            \n",
    "            # Append the result to the CSV file\n",
    "            with open(csv_filename, 'a', newline='') as csvfile:\n",
    "                fieldnames = ['hparams_hash'] + list(hparams.keys()) + ['val_loss', 'computation_time', 'final_num_epochs', 'lr_reduction_epochs', 'log_filename']\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                # If the file is new, write the header\n",
    "                if csvfile.tell() == 0:\n",
    "                    writer.writeheader()\n",
    "                result_row = {'hparams_hash': hparams_hash}\n",
    "                for key, value in hparams.items():\n",
    "                    # Convert value to string using json.dumps\n",
    "                    result_row[key] = json.dumps(value)\n",
    "                result_row.update({\n",
    "                    'val_loss': val_loss,\n",
    "                    'computation_time': computation_time,\n",
    "                    'final_num_epochs': training_log['final_epoch'],\n",
    "                    'lr_reduction_epochs': json.dumps(training_log['lr_reduction_epochs']),\n",
    "                    'log_filename': log_filename\n",
    "                })\n",
    "                writer.writerow(result_row)\n",
    "            \n",
    "            # Update the set of existing hashes\n",
    "            existing_hashes.add(hparams_hash)\n",
    "\n",
    "            # Check and save the best model\n",
    "            if val_loss < global_best_val_loss: \n",
    "                torch.save({\n",
    "                    'state_dict': model.state_dict(),         # Model weights\n",
    "                    'config': {                               # Model configuration\n",
    "                        'input_size': 784,\n",
    "                        'layer_sizes': hparams['layer_sizes'],\n",
    "                        'activation_functions': [str(type(act)) for act in activation_functions],\n",
    "                        'dropout_rates': hparams['dropout_rates'],\n",
    "                        'weight_init': 'kaiming_uniform',\n",
    "                    },\n",
    "                    'hyperparameters': hparams,              # Hyperparameters\n",
    "                    'training_log': training_log,            # Training logs\n",
    "                    'best_val_loss': val_loss,               # Best validation loss\n",
    "                    'optimizer_state': optimizer.state_dict(), # Optimizer state\n",
    "                    'scheduler_state': scheduler.state_dict()  # Scheduler state\n",
    "                }, 'SAE_best_model.pth')\n",
    "                global_best_val_loss = val_loss\n",
    "                print(f\"New best model saved with validation loss: {val_loss:.4f}\\n\")\n",
    "            else:\n",
    "                print(f\"Validation loss {val_loss:.4f} did not improve over the best loss {global_best_val_loss:.4f}\\n\")\n",
    "            \n",
    "            results.append({'hparams': hparams, 'val_loss': val_loss})\n",
    "        except Exception as e:\n",
    "            print(f\"Error with hyperparameters {hparams}: {e}\\n\")\n",
    "            continue\n",
    "\n",
    "# Find the best hyperparameters based on validation loss\n",
    "if results:\n",
    "    best_result = min(results, key=lambda x: x['val_loss'])\n",
    "    print(\"Best hyperparameters based on validation loss:\")\n",
    "    print(best_result['hparams'])\n",
    "    print(f\"Validation Loss: {best_result['val_loss']:.4f}\")\n",
    "    print(\"The best model has been saved as 'best_model.pth'\")\n",
    "else:\n",
    "    print(\"No successful runs to report.\")\n"
   ]
  }
 ],
 "metadata": {
  "deepnote_notebook_id": "16d034cc308c411b9c5ec5cfddcc77a3",
  "kernelspec": {
   "display_name": "EEL",
   "language": "EEL",
   "name": "eel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

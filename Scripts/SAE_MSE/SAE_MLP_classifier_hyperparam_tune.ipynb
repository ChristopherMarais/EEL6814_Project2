{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1b1ae7f-dc8f-47bc-a95a-e704b853101c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/local/50745734/ipykernel_1964841/3879412673.py:50: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_checkpoint = torch.load('SAE_best_model.pth', map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current best validation accuracy from CSV: inf%\n",
      "Testing hyperparameter combination 1/24: {'learning_rate': 0.001, 'classifier_hidden_sizes': [100], 'batch_size': 32, 'weight_decay': 0.0001, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 100, 'batch_norm': True, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0}\n",
      "Epoch [1/100], Training Loss: 0.9309, Training Accuracy: 70.50%, Validation Loss: 0.6825, Validation Accuracy: 77.93%\n",
      "Epoch [2/100], Training Loss: 0.6940, Training Accuracy: 77.37%, Validation Loss: 0.6111, Validation Accuracy: 80.40%\n",
      "Epoch [3/100], Training Loss: 0.6544, Training Accuracy: 78.39%, Validation Loss: 0.5758, Validation Accuracy: 81.30%\n",
      "Epoch [4/100], Training Loss: 0.6342, Training Accuracy: 79.10%, Validation Loss: 0.5640, Validation Accuracy: 81.72%\n",
      "Epoch [5/100], Training Loss: 0.6215, Training Accuracy: 79.67%, Validation Loss: 0.5467, Validation Accuracy: 82.00%\n",
      "Epoch [6/100], Training Loss: 0.6116, Training Accuracy: 79.74%, Validation Loss: 0.5459, Validation Accuracy: 82.20%\n",
      "Epoch [7/100], Training Loss: 0.6069, Training Accuracy: 80.12%, Validation Loss: 0.5356, Validation Accuracy: 82.66%\n",
      "Epoch [8/100], Training Loss: 0.5950, Training Accuracy: 80.30%, Validation Loss: 0.5383, Validation Accuracy: 82.44%\n",
      "Epoch [9/100], Training Loss: 0.5918, Training Accuracy: 80.42%, Validation Loss: 0.5196, Validation Accuracy: 83.12%\n",
      "Epoch [10/100], Training Loss: 0.5839, Training Accuracy: 80.81%, Validation Loss: 0.5162, Validation Accuracy: 83.30%\n",
      "Epoch [11/100], Training Loss: 0.5822, Training Accuracy: 80.63%, Validation Loss: 0.5216, Validation Accuracy: 82.71%\n",
      "Epoch [12/100], Training Loss: 0.5777, Training Accuracy: 80.90%, Validation Loss: 0.5081, Validation Accuracy: 83.26%\n",
      "Epoch [13/100], Training Loss: 0.5730, Training Accuracy: 80.87%, Validation Loss: 0.5083, Validation Accuracy: 83.78%\n",
      "Epoch [14/100], Training Loss: 0.5670, Training Accuracy: 81.38%, Validation Loss: 0.5064, Validation Accuracy: 83.47%\n",
      "Epoch [15/100], Training Loss: 0.5632, Training Accuracy: 81.40%, Validation Loss: 0.5037, Validation Accuracy: 83.52%\n",
      "Epoch [16/100], Training Loss: 0.5578, Training Accuracy: 81.52%, Validation Loss: 0.5011, Validation Accuracy: 83.85%\n",
      "Epoch [17/100], Training Loss: 0.5600, Training Accuracy: 81.50%, Validation Loss: 0.4934, Validation Accuracy: 83.93%\n",
      "Epoch [18/100], Training Loss: 0.5526, Training Accuracy: 81.69%, Validation Loss: 0.4993, Validation Accuracy: 83.76%\n",
      "Epoch [19/100], Training Loss: 0.5512, Training Accuracy: 81.68%, Validation Loss: 0.4913, Validation Accuracy: 83.96%\n",
      "Epoch [20/100], Training Loss: 0.5482, Training Accuracy: 81.92%, Validation Loss: 0.4923, Validation Accuracy: 83.89%\n",
      "Epoch [21/100], Training Loss: 0.5442, Training Accuracy: 82.11%, Validation Loss: 0.4859, Validation Accuracy: 84.20%\n",
      "Epoch [22/100], Training Loss: 0.5461, Training Accuracy: 81.92%, Validation Loss: 0.4860, Validation Accuracy: 84.13%\n",
      "Epoch [23/100], Training Loss: 0.5471, Training Accuracy: 82.00%, Validation Loss: 0.5020, Validation Accuracy: 83.43%\n",
      "Epoch [24/100], Training Loss: 0.5461, Training Accuracy: 81.83%, Validation Loss: 0.4816, Validation Accuracy: 84.20%\n",
      "Epoch [25/100], Training Loss: 0.5381, Training Accuracy: 82.16%, Validation Loss: 0.4803, Validation Accuracy: 84.26%\n",
      "Epoch [26/100], Training Loss: 0.5395, Training Accuracy: 82.21%, Validation Loss: 0.4752, Validation Accuracy: 84.31%\n",
      "Epoch [27/100], Training Loss: 0.5384, Training Accuracy: 82.03%, Validation Loss: 0.4775, Validation Accuracy: 84.36%\n",
      "Epoch [28/100], Training Loss: 0.5330, Training Accuracy: 82.00%, Validation Loss: 0.4721, Validation Accuracy: 84.51%\n",
      "Epoch [29/100], Training Loss: 0.5344, Training Accuracy: 82.25%, Validation Loss: 0.4805, Validation Accuracy: 83.99%\n",
      "Epoch [30/100], Training Loss: 0.5341, Training Accuracy: 82.22%, Validation Loss: 0.4786, Validation Accuracy: 84.23%\n",
      "Epoch [31/100], Training Loss: 0.5344, Training Accuracy: 82.28%, Validation Loss: 0.4724, Validation Accuracy: 84.50%\n",
      "Epoch [32/100], Training Loss: 0.5269, Training Accuracy: 82.42%, Validation Loss: 0.4767, Validation Accuracy: 84.49%\n",
      "Epoch [33/100], Training Loss: 0.5196, Training Accuracy: 82.84%, Validation Loss: 0.4721, Validation Accuracy: 84.51%\n",
      "Early stopping at epoch 34\n",
      "New best model saved with validation accuracy: 84.51%\n",
      "\n",
      "Testing hyperparameter combination 2/24: {'learning_rate': 0.001, 'classifier_hidden_sizes': [100], 'batch_size': 32, 'weight_decay': 0.001, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 100, 'batch_norm': True, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 340\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;66;03m# Call train_and_evaluate and record the computation time\u001b[39;00m\n\u001b[1;32m    339\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 340\u001b[0m     val_loss, val_accuracy, training_log, model, optimizer, scheduler \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    342\u001b[0m     computation_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "Cell \u001b[0;32mIn[1], line 254\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(hparams)\u001b[0m\n\u001b[1;32m    252\u001b[0m     _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    253\u001b[0m     total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 254\u001b[0m     correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mpredicted\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m train_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[1;32m    256\u001b[0m train_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m correct \u001b[38;5;241m/\u001b[39m total\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "from torchvision import datasets, transforms\n",
    "import csv\n",
    "import time\n",
    "import json\n",
    "import hashlib\n",
    "import random\n",
    "from SAE_model import StackedAutoencoder  # Import the SAE class\n",
    "from itertools import product\n",
    "\n",
    "# Function to set all random seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # If you are using multi-GPU.\n",
    "\n",
    "# Set the base seed\n",
    "set_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),  # Normalize to [-1, 1]\n",
    "    transforms.Lambda(lambda x: x.view(-1))  # Flatten the image to a vector\n",
    "])\n",
    "\n",
    "# Load the KMNIST dataset\n",
    "# Load the full training dataset\n",
    "full_train_dataset = datasets.KMNIST(root='../data', train=True, transform=transform, download=True)\n",
    "\n",
    "# Define the sizes for the training and validation datasets\n",
    "val_size = 10000\n",
    "train_size = len(full_train_dataset) - val_size\n",
    "\n",
    "# Split the training data into train and validation sets\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "\n",
    "# Load the saved autoencoder model\n",
    "model_checkpoint = torch.load('SAE_best_model.pth', map_location=device)\n",
    "\n",
    "# Get the model configuration\n",
    "model_config = model_checkpoint['config']\n",
    "input_size = model_config['input_size']\n",
    "layer_sizes = model_config['layer_sizes']\n",
    "dropout_rates = model_config['dropout_rates']\n",
    "activation_functions = [nn.LeakyReLU(negative_slope=0.01) for _ in layer_sizes]  # Assuming LeakyReLU with negative_slope=0.01\n",
    "\n",
    "# Rebuild the autoencoder model\n",
    "autoencoder = StackedAutoencoder(\n",
    "    input_size=input_size,\n",
    "    layer_sizes=layer_sizes,\n",
    "    activation_functions=activation_functions,\n",
    "    dropout_rates=dropout_rates,\n",
    "    weight_init=None  # We don't need to initialize weights, as we'll load them\n",
    ").to(device)\n",
    "\n",
    "# Load the state_dict\n",
    "autoencoder.load_state_dict(model_checkpoint['state_dict'])\n",
    "\n",
    "# Set the autoencoder to evaluation mode\n",
    "autoencoder.eval()\n",
    "\n",
    "# Get the output size of the encoder\n",
    "encoder_output_size = layer_sizes[-1]\n",
    "\n",
    "# Define the classification model\n",
    "class SAEClassifier(nn.Module):\n",
    "    def __init__(self, encoder, encoder_output_size, classifier_hidden_sizes, num_classes, leaky_relu_negative_slope=0.01, batch_norm=True):\n",
    "        super(SAEClassifier, self).__init__()\n",
    "        # Encoder (pre-trained)\n",
    "        self.encoder = encoder  # We will freeze this\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Classifier head\n",
    "        layers = []\n",
    "        prev_size = encoder_output_size  # The output size of the encoder\n",
    "        for idx, hidden_size in enumerate(classifier_hidden_sizes):\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            if batch_norm:\n",
    "                layers.append(nn.BatchNorm1d(hidden_size))\n",
    "            layers.append(nn.LeakyReLU(negative_slope=leaky_relu_negative_slope))\n",
    "            prev_size = hidden_size\n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(prev_size, num_classes))\n",
    "        self.classifier = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Hyperparameter grid for tuning\n",
    "hyperparameter_grid = {\n",
    "    'learning_rate': [1e-3, 1e-4],\n",
    "    'classifier_hidden_sizes': [[100], [200], [100, 50]],\n",
    "    'batch_size': [32, 64],\n",
    "    'weight_decay': [1e-4, 1e-3],\n",
    "    'leaky_relu_negative_slope': [0.01],\n",
    "    'num_epochs': [100],  # Max number of training epochs\n",
    "    'batch_norm': [True],\n",
    "    # Early Stopping hyperparameters\n",
    "    'early_stopping_patience': [5],\n",
    "    'early_stopping_min_delta': [1e-4],\n",
    "    # Scheduler hyperparameters (ReduceLROnPlateau)\n",
    "    'scheduler_mode': ['min'],\n",
    "    'scheduler_factor': [0.1],\n",
    "    'scheduler_patience': [3],\n",
    "    'scheduler_threshold': [1e-4],\n",
    "    'scheduler_cooldown': [0]\n",
    "}\n",
    "\n",
    "# Generate all combinations of hyperparameters\n",
    "keys, values = zip(*hyperparameter_grid.items())\n",
    "hyperparameter_combinations = [dict(zip(keys, v)) for v in product(*values)]\n",
    "\n",
    "csv_filename = 'SAE_classifier_hyperparameter_tuning_results.csv'\n",
    "\n",
    "# Initialize an empty set to store hashes of existing results\n",
    "existing_hashes = set()\n",
    "\n",
    "# Initialize best_val_loss by reading existing CSV\n",
    "best_val_loss =  float('inf')\n",
    "\n",
    "if os.path.exists(csv_filename):\n",
    "    with open(csv_filename, 'r', newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            hparams_hash = row['hparams_hash']\n",
    "            existing_hashes.add(hparams_hash)\n",
    "            try:\n",
    "                val_loss = float(row['val_loss'])\n",
    "                if val_loss > best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "            except ValueError:\n",
    "                # If val_accuracy is not a float, skip\n",
    "                continue\n",
    "    print(f\"Current best validation accuracy from CSV: {best_val_loss:.3f}%\")\n",
    "else:\n",
    "    print(\"CSV file does not exist. Starting fresh.\")\n",
    "\n",
    "def train_and_evaluate(hparams):\n",
    "    # Unpack hyperparameters\n",
    "    learning_rate = hparams['learning_rate']\n",
    "    classifier_hidden_sizes = hparams['classifier_hidden_sizes']\n",
    "    batch_size = hparams['batch_size']\n",
    "    weight_decay = hparams['weight_decay']\n",
    "    leaky_relu_negative_slope = hparams['leaky_relu_negative_slope']\n",
    "    num_epochs = hparams.get('num_epochs', 20)\n",
    "    batch_norm = hparams['batch_norm']\n",
    "\n",
    "    # Early Stopping hyperparameters\n",
    "    early_stopping_patience = hparams['early_stopping_patience']\n",
    "    early_stopping_min_delta = hparams['early_stopping_min_delta']\n",
    "\n",
    "    # Scheduler hyperparameters\n",
    "    scheduler_mode = hparams['scheduler_mode']\n",
    "    scheduler_factor = hparams['scheduler_factor']\n",
    "    scheduler_patience = hparams['scheduler_patience']\n",
    "    scheduler_threshold = hparams['scheduler_threshold']\n",
    "    scheduler_cooldown = hparams['scheduler_cooldown']\n",
    "\n",
    "    # Define the model\n",
    "    model = SAEClassifier(\n",
    "        encoder=autoencoder.encoder,\n",
    "        encoder_output_size=encoder_output_size,\n",
    "        classifier_hidden_sizes=classifier_hidden_sizes,\n",
    "        num_classes=10,  # KMNIST has 10 classes\n",
    "        leaky_relu_negative_slope=leaky_relu_negative_slope,\n",
    "        batch_norm=batch_norm\n",
    "    ).to(device)\n",
    "\n",
    "    # Apply weight initialization to the classifier head\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_uniform_(m.weight, nonlinearity='leaky_relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.BatchNorm1d):\n",
    "            nn.init.ones_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "    model.classifier.apply(init_weights)\n",
    "\n",
    "    # Set up the optimizer (only trainable parameters)\n",
    "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Scheduler: ReduceLROnPlateau\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode=scheduler_mode,\n",
    "        factor=scheduler_factor,\n",
    "        patience=scheduler_patience,\n",
    "        threshold=scheduler_threshold,\n",
    "        threshold_mode='rel',\n",
    "        cooldown=scheduler_cooldown,\n",
    "    )\n",
    "\n",
    "    # Set up the loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Early Stopping variables\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "\n",
    "    # Initialize variables for logging\n",
    "    initial_lr = optimizer.param_groups[0]['lr']\n",
    "    current_lr = initial_lr\n",
    "    lr_reduction_epochs = []\n",
    "    epoch_logs = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        if early_stop:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data in train_loader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        train_loss /= len(train_loader)\n",
    "        train_accuracy = 100 * correct / total\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = 100 * correct / total\n",
    "\n",
    "        # Scheduler step\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Check if learning rate has been reduced\n",
    "        new_lr = optimizer.param_groups[0]['lr']\n",
    "        if new_lr < current_lr:\n",
    "            lr_reduction_epochs.append(epoch+1)  # Epochs are 1-indexed\n",
    "            current_lr = new_lr\n",
    "\n",
    "        # Record per-epoch logs\n",
    "        epoch_logs.append({\n",
    "            'epoch': epoch+1,\n",
    "            'train_loss': train_loss,\n",
    "            'train_accuracy': train_accuracy,\n",
    "            'val_loss': val_loss,\n",
    "            'val_accuracy': val_accuracy,\n",
    "            'learning_rate': current_lr\n",
    "        })\n",
    "\n",
    "        # Early Stopping check\n",
    "        if val_loss < best_val_loss - early_stopping_min_delta:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= early_stopping_patience:\n",
    "            early_stop = True\n",
    "\n",
    "        # Optionally, print progress\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "              f\"Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%, \"\n",
    "              f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "    # Prepare training log\n",
    "    training_log = {\n",
    "        'final_epoch': epoch+1,\n",
    "        'lr_reduction_epochs': lr_reduction_epochs,\n",
    "        'epoch_logs': epoch_logs\n",
    "    }\n",
    "\n",
    "    # Return final validation loss, accuracy, training log, and the trained model\n",
    "    return val_loss, val_accuracy, training_log, model, optimizer, scheduler\n",
    "\n",
    "# Main loop for hyperparameter tuning\n",
    "results = []\n",
    "\n",
    "for idx, hparams in enumerate(hyperparameter_combinations):\n",
    "    # Set the base seed\n",
    "    set_seed(42)\n",
    "    # Generate a unique id for the hyperparameters\n",
    "    hparams_str = json.dumps(hparams, sort_keys=True)\n",
    "    hparams_hash = hashlib.md5(hparams_str.encode('utf-8')).hexdigest()\n",
    "\n",
    "    # Check if this combination exists in existing_results\n",
    "    if hparams_hash in existing_hashes:\n",
    "        print(f\"Skipping hyperparameter combination {idx+1}/{len(hyperparameter_combinations)}: {hparams} (already tested)\")\n",
    "        continue\n",
    "    else:\n",
    "        print(f\"Testing hyperparameter combination {idx+1}/{len(hyperparameter_combinations)}: {hparams}\")\n",
    "        try:\n",
    "            # Call train_and_evaluate and record the computation time\n",
    "            start_time = time.time()\n",
    "            val_loss, val_accuracy, training_log, model, optimizer, scheduler = train_and_evaluate(hparams)\n",
    "            end_time = time.time()\n",
    "            computation_time = end_time - start_time\n",
    "\n",
    "            # Save the per-epoch logs to a file\n",
    "            log_filename = f\"misc/classifier_training_log_{hparams_hash}.json\"\n",
    "            os.makedirs(os.path.dirname(log_filename), exist_ok=True)\n",
    "            with open(log_filename, 'w') as f:\n",
    "                json.dump(training_log, f)\n",
    "\n",
    "            # Append the result to the CSV file\n",
    "            with open(csv_filename, 'a', newline='') as csvfile:\n",
    "                fieldnames = ['hparams_hash'] + list(hparams.keys()) + ['val_loss', 'val_accuracy', 'computation_time', 'final_num_epochs', 'lr_reduction_epochs', 'log_filename']\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                # If the file is new, write the header\n",
    "                if csvfile.tell() == 0:\n",
    "                    writer.writeheader()\n",
    "                result_row = {'hparams_hash': hparams_hash}\n",
    "                for key, value in hparams.items():\n",
    "                    # Convert value to string using json.dumps\n",
    "                    result_row[key] = json.dumps(value)\n",
    "                result_row.update({\n",
    "                    'val_loss': val_loss,\n",
    "                    'val_accuracy': val_accuracy,\n",
    "                    'computation_time': computation_time,\n",
    "                    'final_num_epochs': training_log['final_epoch'],\n",
    "                    'lr_reduction_epochs': json.dumps(training_log['lr_reduction_epochs']),\n",
    "                    'log_filename': log_filename\n",
    "                })\n",
    "                writer.writerow(result_row)\n",
    "\n",
    "            # Update the set of existing hashes\n",
    "            existing_hashes.add(hparams_hash)\n",
    "\n",
    "            # Check and save the best model\n",
    "            if val_loss < best_val_loss:\n",
    "                torch.save({\n",
    "                    'state_dict': model.state_dict(),         # Model weights\n",
    "                    'config': {                               # Model configuration\n",
    "                        'encoder_output_size': encoder_output_size,\n",
    "                        'classifier_hidden_sizes': hparams['classifier_hidden_sizes'],\n",
    "                        'leaky_relu_negative_slope': hparams['leaky_relu_negative_slope'],\n",
    "                        'batch_norm': hparams['batch_norm'],\n",
    "                    },\n",
    "                    'hyperparameters': hparams,              # Hyperparameters\n",
    "                    'training_log': training_log,            # Training logs\n",
    "                    'best_val_loss': val_loss,               # Best validation loss\n",
    "                    'best_val_accuracy': val_accuracy,       # Best validation accuracy\n",
    "                    'optimizer_state': optimizer.state_dict(), # Optimizer state\n",
    "                    'scheduler_state': scheduler.state_dict()  # Scheduler state\n",
    "                }, 'SAE_classifier_best_model.pth')\n",
    "                best_val_loss = val_loss\n",
    "                print(f\"New best model saved with validation accuracy: {val_accuracy:.2f}%\\n\")\n",
    "            else:\n",
    "                print(f\"Validation accuracy {val_loss:.3f}% did not improve over the best loss {best_val_loss:.3f}%\\n\")\n",
    "\n",
    "            results.append({'hparams': hparams, 'val_loss': val_loss, 'val_accuracy': val_accuracy})\n",
    "        except Exception as e:\n",
    "            print(f\"Error with hyperparameters {hparams}: {e}\\n\")\n",
    "            continue\n",
    "\n",
    "# Find the best hyperparameters based on validation accuracy\n",
    "if results:\n",
    "    best_result = max(results, key=lambda x: x['val_accuracy'])\n",
    "    print(\"Best hyperparameters based on validation accuracy:\")\n",
    "    print(best_result['hparams'])\n",
    "    print(f\"Validation Loss: {best_result['val_loss']:.4f}, Validation Accuracy: {best_result['val_accuracy']:.2f}%\")\n",
    "    print(\"The best model has been saved as 'SAE_classifier_best_model.pth'\")\n",
    "else:\n",
    "    print(\"No successful runs to report.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EEL",
   "language": "EEL",
   "name": "eel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1b1ae7f-dc8f-47bc-a95a-e704b853101c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/local/50745734/ipykernel_1437298/1693790732.py:42: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_checkpoint = torch.load('SAE_best_model.pth', map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file does not exist. Starting fresh.\n",
      "Testing hyperparameter combination 1/24: {'learning_rate': 0.001, 'classifier_hidden_sizes': [100], 'batch_size': 32, 'weight_decay': 0.0001, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 50, 'batch_norm': True, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0}\n",
      "Epoch [1/50], Training Loss: 0.8945, Training Accuracy: 71.41%, Validation Loss: 1.0636, Validation Accuracy: 63.48%\n",
      "Epoch [2/50], Training Loss: 0.6780, Training Accuracy: 77.73%, Validation Loss: 1.0093, Validation Accuracy: 65.92%\n",
      "Epoch [3/50], Training Loss: 0.6437, Training Accuracy: 78.75%, Validation Loss: 0.9928, Validation Accuracy: 66.13%\n",
      "Epoch [4/50], Training Loss: 0.6279, Training Accuracy: 79.23%, Validation Loss: 0.9448, Validation Accuracy: 68.23%\n",
      "Epoch [5/50], Training Loss: 0.6168, Training Accuracy: 79.75%, Validation Loss: 0.9354, Validation Accuracy: 69.08%\n",
      "Epoch [6/50], Training Loss: 0.6078, Training Accuracy: 79.97%, Validation Loss: 0.9257, Validation Accuracy: 68.79%\n",
      "Epoch [7/50], Training Loss: 0.5937, Training Accuracy: 80.50%, Validation Loss: 0.8939, Validation Accuracy: 70.05%\n",
      "Epoch [8/50], Training Loss: 0.5896, Training Accuracy: 80.47%, Validation Loss: 0.8846, Validation Accuracy: 70.57%\n",
      "Epoch [9/50], Training Loss: 0.5820, Training Accuracy: 80.74%, Validation Loss: 0.8762, Validation Accuracy: 70.76%\n",
      "Epoch [10/50], Training Loss: 0.5745, Training Accuracy: 81.02%, Validation Loss: 0.8711, Validation Accuracy: 70.73%\n",
      "Epoch [11/50], Training Loss: 0.5735, Training Accuracy: 81.09%, Validation Loss: 0.8704, Validation Accuracy: 71.35%\n",
      "Epoch [12/50], Training Loss: 0.5675, Training Accuracy: 81.08%, Validation Loss: 0.8482, Validation Accuracy: 71.41%\n",
      "Epoch [13/50], Training Loss: 0.5639, Training Accuracy: 81.22%, Validation Loss: 0.9118, Validation Accuracy: 69.27%\n",
      "Epoch [14/50], Training Loss: 0.5604, Training Accuracy: 81.48%, Validation Loss: 0.8605, Validation Accuracy: 70.87%\n",
      "Epoch [15/50], Training Loss: 0.5589, Training Accuracy: 81.45%, Validation Loss: 0.8411, Validation Accuracy: 71.76%\n",
      "Epoch [16/50], Training Loss: 0.5537, Training Accuracy: 81.64%, Validation Loss: 0.8397, Validation Accuracy: 72.23%\n",
      "Epoch [17/50], Training Loss: 0.5517, Training Accuracy: 81.64%, Validation Loss: 0.8311, Validation Accuracy: 72.09%\n",
      "Epoch [18/50], Training Loss: 0.5514, Training Accuracy: 81.69%, Validation Loss: 0.8649, Validation Accuracy: 70.68%\n",
      "Epoch [19/50], Training Loss: 0.5450, Training Accuracy: 81.85%, Validation Loss: 0.8551, Validation Accuracy: 71.56%\n",
      "Epoch [20/50], Training Loss: 0.5435, Training Accuracy: 81.78%, Validation Loss: 0.8171, Validation Accuracy: 72.77%\n",
      "Epoch [21/50], Training Loss: 0.5439, Training Accuracy: 81.88%, Validation Loss: 0.8311, Validation Accuracy: 72.27%\n",
      "Epoch [22/50], Training Loss: 0.5405, Training Accuracy: 82.08%, Validation Loss: 0.8072, Validation Accuracy: 73.27%\n",
      "Epoch [23/50], Training Loss: 0.5394, Training Accuracy: 82.11%, Validation Loss: 0.8411, Validation Accuracy: 71.82%\n",
      "Epoch [24/50], Training Loss: 0.5411, Training Accuracy: 82.06%, Validation Loss: 0.8269, Validation Accuracy: 71.97%\n",
      "Epoch [25/50], Training Loss: 0.5376, Training Accuracy: 82.17%, Validation Loss: 0.8253, Validation Accuracy: 72.47%\n",
      "Epoch [26/50], Training Loss: 0.5341, Training Accuracy: 82.22%, Validation Loss: 0.8149, Validation Accuracy: 72.41%\n",
      "Epoch [27/50], Training Loss: 0.5223, Training Accuracy: 82.53%, Validation Loss: 0.7911, Validation Accuracy: 73.30%\n",
      "Epoch [28/50], Training Loss: 0.5181, Training Accuracy: 82.69%, Validation Loss: 0.7909, Validation Accuracy: 73.36%\n",
      "Epoch [29/50], Training Loss: 0.5173, Training Accuracy: 82.78%, Validation Loss: 0.7872, Validation Accuracy: 73.56%\n",
      "Epoch [30/50], Training Loss: 0.5174, Training Accuracy: 82.78%, Validation Loss: 0.7905, Validation Accuracy: 73.28%\n",
      "Epoch [31/50], Training Loss: 0.5201, Training Accuracy: 82.77%, Validation Loss: 0.7949, Validation Accuracy: 73.21%\n",
      "Epoch [32/50], Training Loss: 0.5204, Training Accuracy: 82.67%, Validation Loss: 0.7973, Validation Accuracy: 72.97%\n",
      "Epoch [33/50], Training Loss: 0.5183, Training Accuracy: 82.77%, Validation Loss: 0.7946, Validation Accuracy: 73.55%\n",
      "Epoch [34/50], Training Loss: 0.5158, Training Accuracy: 82.99%, Validation Loss: 0.8019, Validation Accuracy: 72.84%\n",
      "Early stopping at epoch 35\n",
      "Error with hyperparameters {'learning_rate': 0.001, 'classifier_hidden_sizes': [100], 'batch_size': 32, 'weight_decay': 0.0001, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 50, 'batch_norm': True, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0}: '<' not supported between instances of 'float' and 'NoneType'\n",
      "\n",
      "Testing hyperparameter combination 2/24: {'learning_rate': 0.001, 'classifier_hidden_sizes': [100], 'batch_size': 32, 'weight_decay': 0.001, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 50, 'batch_norm': True, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0}\n",
      "Epoch [1/50], Training Loss: 0.8946, Training Accuracy: 71.42%, Validation Loss: 1.0641, Validation Accuracy: 63.44%\n",
      "Epoch [2/50], Training Loss: 0.6781, Training Accuracy: 77.72%, Validation Loss: 1.0092, Validation Accuracy: 65.96%\n",
      "Epoch [3/50], Training Loss: 0.6438, Training Accuracy: 78.74%, Validation Loss: 0.9925, Validation Accuracy: 66.25%\n",
      "Epoch [4/50], Training Loss: 0.6283, Training Accuracy: 79.24%, Validation Loss: 0.9461, Validation Accuracy: 68.13%\n",
      "Epoch [5/50], Training Loss: 0.6172, Training Accuracy: 79.71%, Validation Loss: 0.9345, Validation Accuracy: 68.93%\n",
      "Epoch [6/50], Training Loss: 0.6082, Training Accuracy: 80.00%, Validation Loss: 0.9258, Validation Accuracy: 68.90%\n",
      "Epoch [7/50], Training Loss: 0.5941, Training Accuracy: 80.48%, Validation Loss: 0.8960, Validation Accuracy: 70.10%\n",
      "Epoch [8/50], Training Loss: 0.5898, Training Accuracy: 80.45%, Validation Loss: 0.8854, Validation Accuracy: 70.59%\n",
      "Epoch [9/50], Training Loss: 0.5822, Training Accuracy: 80.73%, Validation Loss: 0.8768, Validation Accuracy: 70.66%\n",
      "Epoch [10/50], Training Loss: 0.5747, Training Accuracy: 81.01%, Validation Loss: 0.8751, Validation Accuracy: 70.56%\n",
      "Epoch [11/50], Training Loss: 0.5736, Training Accuracy: 81.08%, Validation Loss: 0.8708, Validation Accuracy: 71.22%\n",
      "Epoch [12/50], Training Loss: 0.5674, Training Accuracy: 81.09%, Validation Loss: 0.8493, Validation Accuracy: 71.38%\n",
      "Epoch [13/50], Training Loss: 0.5640, Training Accuracy: 81.25%, Validation Loss: 0.9130, Validation Accuracy: 69.22%\n",
      "Epoch [14/50], Training Loss: 0.5603, Training Accuracy: 81.42%, Validation Loss: 0.8625, Validation Accuracy: 70.81%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 332\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;66;03m# Call train_and_evaluate and record the computation time\u001b[39;00m\n\u001b[1;32m    331\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 332\u001b[0m     val_loss, val_accuracy, training_log, model, optimizer, scheduler \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    334\u001b[0m     computation_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "Cell \u001b[0;32mIn[1], line 235\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(hparams)\u001b[0m\n\u001b[1;32m    232\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m--> 235\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n",
      "File \u001b[0;32m/blue/hulcr/gmarais/conda/envs/EEL/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/blue/hulcr/gmarais/conda/envs/EEL/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[1], line 91\u001b[0m, in \u001b[0;36mSAEClassifier.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     90\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[0;32m---> 91\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/blue/hulcr/gmarais/conda/envs/EEL/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/blue/hulcr/gmarais/conda/envs/EEL/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/blue/hulcr/gmarais/conda/envs/EEL/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/blue/hulcr/gmarais/conda/envs/EEL/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/blue/hulcr/gmarais/conda/envs/EEL/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/blue/hulcr/gmarais/conda/envs/EEL/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import csv\n",
    "import time\n",
    "import json\n",
    "import hashlib\n",
    "import random\n",
    "from SAE_model import StackedAutoencoder  # Import the SAE class\n",
    "from itertools import product\n",
    "\n",
    "# Function to set all random seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # If you are using multi-GPU.\n",
    "\n",
    "# Set the base seed\n",
    "set_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),  # Normalize to [-1, 1]\n",
    "    transforms.Lambda(lambda x: x.view(-1))  # Flatten the image to a vector\n",
    "])\n",
    "\n",
    "# Load the KMNIST dataset\n",
    "train_dataset = datasets.KMNIST(root='../data', train=True, transform=transform, download=True)\n",
    "val_dataset = datasets.KMNIST(root='../data', train=False, transform=transform, download=True)\n",
    "\n",
    "# Load the saved autoencoder model\n",
    "model_checkpoint = torch.load('SAE_best_model.pth', map_location=device)\n",
    "\n",
    "# Get the model configuration\n",
    "model_config = model_checkpoint['config']\n",
    "input_size = model_config['input_size']\n",
    "layer_sizes = model_config['layer_sizes']\n",
    "dropout_rates = model_config['dropout_rates']\n",
    "activation_functions = [nn.LeakyReLU(negative_slope=0.01) for _ in layer_sizes]  # Assuming LeakyReLU with negative_slope=0.01\n",
    "\n",
    "# Rebuild the autoencoder model\n",
    "autoencoder = StackedAutoencoder(\n",
    "    input_size=input_size,\n",
    "    layer_sizes=layer_sizes,\n",
    "    activation_functions=activation_functions,\n",
    "    dropout_rates=dropout_rates,\n",
    "    weight_init=None  # We don't need to initialize weights, as we'll load them\n",
    ").to(device)\n",
    "\n",
    "# Load the state_dict\n",
    "autoencoder.load_state_dict(model_checkpoint['state_dict'])\n",
    "\n",
    "# Set the autoencoder to evaluation mode\n",
    "autoencoder.eval()\n",
    "\n",
    "# Get the output size of the encoder\n",
    "encoder_output_size = layer_sizes[-1]\n",
    "\n",
    "# Define the classification model\n",
    "class SAEClassifier(nn.Module):\n",
    "    def __init__(self, encoder, encoder_output_size, classifier_hidden_sizes, num_classes, leaky_relu_negative_slope=0.01, batch_norm=True):\n",
    "        super(SAEClassifier, self).__init__()\n",
    "        # Encoder (pre-trained)\n",
    "        self.encoder = encoder  # We will freeze this\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Classifier head\n",
    "        layers = []\n",
    "        prev_size = encoder_output_size  # The output size of the encoder\n",
    "        for idx, hidden_size in enumerate(classifier_hidden_sizes):\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            if batch_norm:\n",
    "                layers.append(nn.BatchNorm1d(hidden_size))\n",
    "            layers.append(nn.LeakyReLU(negative_slope=leaky_relu_negative_slope))\n",
    "            prev_size = hidden_size\n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(prev_size, num_classes))\n",
    "        self.classifier = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Hyperparameter grid for tuning\n",
    "hyperparameter_grid = {\n",
    "    'learning_rate': [1e-3, 1e-4],\n",
    "    'classifier_hidden_sizes': [[100], [200], [100, 50]],\n",
    "    'batch_size': [32, 64],\n",
    "    'weight_decay': [1e-4, 1e-3],\n",
    "    'leaky_relu_negative_slope': [0.01],\n",
    "    'num_epochs': [100],  # Max number of training epochs\n",
    "    'batch_norm': [True],\n",
    "    # Early Stopping hyperparameters\n",
    "    'early_stopping_patience': [5],\n",
    "    'early_stopping_min_delta': [1e-4],\n",
    "    # Scheduler hyperparameters (ReduceLROnPlateau)\n",
    "    'scheduler_mode': ['min'],\n",
    "    'scheduler_factor': [0.1],\n",
    "    'scheduler_patience': [3],\n",
    "    'scheduler_threshold': [1e-4],\n",
    "    'scheduler_cooldown': [0]\n",
    "}\n",
    "\n",
    "# Generate all combinations of hyperparameters\n",
    "keys, values = zip(*hyperparameter_grid.items())\n",
    "hyperparameter_combinations = [dict(zip(keys, v)) for v in product(*values)]\n",
    "\n",
    "csv_filename = 'SAE_classifier_hyperparameter_tuning_results.csv'\n",
    "\n",
    "# Initialize an empty set to store hashes of existing results\n",
    "existing_hashes = set()\n",
    "\n",
    "# Initialize best_val_loss by reading existing CSV\n",
    "best_val_loss =  None\n",
    "\n",
    "if os.path.exists(csv_filename):\n",
    "    with open(csv_filename, 'r', newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            hparams_hash = row['hparams_hash']\n",
    "            existing_hashes.add(hparams_hash)\n",
    "            try:\n",
    "                val_accuracy = float(row['val_accuracy'])\n",
    "                if val_loss > best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "            except ValueError:\n",
    "                # If val_accuracy is not a float, skip\n",
    "                continue\n",
    "    print(f\"Current best validation accuracy from CSV: {best_val_loss:.3f}%\")\n",
    "else:\n",
    "    print(\"CSV file does not exist. Starting fresh.\")\n",
    "\n",
    "def train_and_evaluate(hparams):\n",
    "    # Unpack hyperparameters\n",
    "    learning_rate = hparams['learning_rate']\n",
    "    classifier_hidden_sizes = hparams['classifier_hidden_sizes']\n",
    "    batch_size = hparams['batch_size']\n",
    "    weight_decay = hparams['weight_decay']\n",
    "    leaky_relu_negative_slope = hparams['leaky_relu_negative_slope']\n",
    "    num_epochs = hparams.get('num_epochs', 20)\n",
    "    batch_norm = hparams['batch_norm']\n",
    "\n",
    "    # Early Stopping hyperparameters\n",
    "    early_stopping_patience = hparams['early_stopping_patience']\n",
    "    early_stopping_min_delta = hparams['early_stopping_min_delta']\n",
    "\n",
    "    # Scheduler hyperparameters\n",
    "    scheduler_mode = hparams['scheduler_mode']\n",
    "    scheduler_factor = hparams['scheduler_factor']\n",
    "    scheduler_patience = hparams['scheduler_patience']\n",
    "    scheduler_threshold = hparams['scheduler_threshold']\n",
    "    scheduler_cooldown = hparams['scheduler_cooldown']\n",
    "\n",
    "    # Define the model\n",
    "    model = SAEClassifier(\n",
    "        encoder=autoencoder.encoder,\n",
    "        encoder_output_size=encoder_output_size,\n",
    "        classifier_hidden_sizes=classifier_hidden_sizes,\n",
    "        num_classes=10,  # KMNIST has 10 classes\n",
    "        leaky_relu_negative_slope=leaky_relu_negative_slope,\n",
    "        batch_norm=batch_norm\n",
    "    ).to(device)\n",
    "\n",
    "    # Apply weight initialization to the classifier head\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_uniform_(m.weight, nonlinearity='leaky_relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.BatchNorm1d):\n",
    "            nn.init.ones_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "    model.classifier.apply(init_weights)\n",
    "\n",
    "    # Set up the optimizer (only trainable parameters)\n",
    "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Scheduler: ReduceLROnPlateau\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode=scheduler_mode,\n",
    "        factor=scheduler_factor,\n",
    "        patience=scheduler_patience,\n",
    "        threshold=scheduler_threshold,\n",
    "        threshold_mode='rel',\n",
    "        cooldown=scheduler_cooldown,\n",
    "    )\n",
    "\n",
    "    # Set up the loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Early Stopping variables\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "\n",
    "    # Initialize variables for logging\n",
    "    initial_lr = optimizer.param_groups[0]['lr']\n",
    "    current_lr = initial_lr\n",
    "    lr_reduction_epochs = []\n",
    "    epoch_logs = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        if early_stop:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data in train_loader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        train_loss /= len(train_loader)\n",
    "        train_accuracy = 100 * correct / total\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = 100 * correct / total\n",
    "\n",
    "        # Scheduler step\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Check if learning rate has been reduced\n",
    "        new_lr = optimizer.param_groups[0]['lr']\n",
    "        if new_lr < current_lr:\n",
    "            lr_reduction_epochs.append(epoch+1)  # Epochs are 1-indexed\n",
    "            current_lr = new_lr\n",
    "\n",
    "        # Record per-epoch logs\n",
    "        epoch_logs.append({\n",
    "            'epoch': epoch+1,\n",
    "            'train_loss': train_loss,\n",
    "            'train_accuracy': train_accuracy,\n",
    "            'val_loss': val_loss,\n",
    "            'val_accuracy': val_accuracy,\n",
    "            'learning_rate': current_lr\n",
    "        })\n",
    "\n",
    "        # Early Stopping check\n",
    "        if val_loss < best_val_loss - early_stopping_min_delta:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= early_stopping_patience:\n",
    "            early_stop = True\n",
    "\n",
    "        # Optionally, print progress\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "              f\"Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%, \"\n",
    "              f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "    # Prepare training log\n",
    "    training_log = {\n",
    "        'final_epoch': epoch+1,\n",
    "        'lr_reduction_epochs': lr_reduction_epochs,\n",
    "        'epoch_logs': epoch_logs\n",
    "    }\n",
    "\n",
    "    # Return final validation loss, accuracy, training log, and the trained model\n",
    "    return val_loss, val_accuracy, training_log, model, optimizer, scheduler\n",
    "\n",
    "# Main loop for hyperparameter tuning\n",
    "results = []\n",
    "\n",
    "for idx, hparams in enumerate(hyperparameter_combinations):\n",
    "    # Set the base seed\n",
    "    set_seed(42)\n",
    "    # Generate a unique id for the hyperparameters\n",
    "    hparams_str = json.dumps(hparams, sort_keys=True)\n",
    "    hparams_hash = hashlib.md5(hparams_str.encode('utf-8')).hexdigest()\n",
    "\n",
    "    # Check if this combination exists in existing_results\n",
    "    if hparams_hash in existing_hashes:\n",
    "        print(f\"Skipping hyperparameter combination {idx+1}/{len(hyperparameter_combinations)}: {hparams} (already tested)\")\n",
    "        continue\n",
    "    else:\n",
    "        print(f\"Testing hyperparameter combination {idx+1}/{len(hyperparameter_combinations)}: {hparams}\")\n",
    "        try:\n",
    "            # Call train_and_evaluate and record the computation time\n",
    "            start_time = time.time()\n",
    "            val_loss, val_accuracy, training_log, model, optimizer, scheduler = train_and_evaluate(hparams)\n",
    "            end_time = time.time()\n",
    "            computation_time = end_time - start_time\n",
    "\n",
    "            # Save the per-epoch logs to a file\n",
    "            log_filename = f\"misc/classifier_training_log_{hparams_hash}.json\"\n",
    "            os.makedirs(os.path.dirname(log_filename), exist_ok=True)\n",
    "            with open(log_filename, 'w') as f:\n",
    "                json.dump(training_log, f)\n",
    "\n",
    "            # Append the result to the CSV file\n",
    "            with open(csv_filename, 'a', newline='') as csvfile:\n",
    "                fieldnames = ['hparams_hash'] + list(hparams.keys()) + ['val_loss', 'val_accuracy', 'computation_time', 'final_num_epochs', 'lr_reduction_epochs', 'log_filename']\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                # If the file is new, write the header\n",
    "                if csvfile.tell() == 0:\n",
    "                    writer.writeheader()\n",
    "                result_row = {'hparams_hash': hparams_hash}\n",
    "                for key, value in hparams.items():\n",
    "                    # Convert value to string using json.dumps\n",
    "                    result_row[key] = json.dumps(value)\n",
    "                result_row.update({\n",
    "                    'val_loss': val_loss,\n",
    "                    'val_accuracy': val_accuracy,\n",
    "                    'computation_time': computation_time,\n",
    "                    'final_num_epochs': training_log['final_epoch'],\n",
    "                    'lr_reduction_epochs': json.dumps(training_log['lr_reduction_epochs']),\n",
    "                    'log_filename': log_filename\n",
    "                })\n",
    "                writer.writerow(result_row)\n",
    "\n",
    "            # Update the set of existing hashes\n",
    "            existing_hashes.add(hparams_hash)\n",
    "\n",
    "            # Check and save the best model\n",
    "            if val_loss < best_val_loss:\n",
    "                torch.save({\n",
    "                    'state_dict': model.state_dict(),         # Model weights\n",
    "                    'config': {                               # Model configuration\n",
    "                        'encoder_output_size': encoder_output_size,\n",
    "                        'classifier_hidden_sizes': hparams['classifier_hidden_sizes'],\n",
    "                        'leaky_relu_negative_slope': hparams['leaky_relu_negative_slope'],\n",
    "                        'batch_norm': hparams['batch_norm'],\n",
    "                    },\n",
    "                    'hyperparameters': hparams,              # Hyperparameters\n",
    "                    'training_log': training_log,            # Training logs\n",
    "                    'best_val_loss': val_loss,               # Best validation loss\n",
    "                    'best_val_accuracy': val_accuracy,       # Best validation accuracy\n",
    "                    'optimizer_state': optimizer.state_dict(), # Optimizer state\n",
    "                    'scheduler_state': scheduler.state_dict()  # Scheduler state\n",
    "                }, 'SAE_classifier_best_model.pth')\n",
    "                best_val_loss = val_loss\n",
    "                print(f\"New best model saved with validation accuracy: {val_accuracy:.2f}%\\n\")\n",
    "            else:\n",
    "                print(f\"Validation accuracy {val_loss:.3f}% did not improve over the best loss {best_val_loss:.3f}%\\n\")\n",
    "\n",
    "            results.append({'hparams': hparams, 'val_loss': val_loss, 'val_accuracy': val_accuracy})\n",
    "        except Exception as e:\n",
    "            print(f\"Error with hyperparameters {hparams}: {e}\\n\")\n",
    "            continue\n",
    "\n",
    "# Find the best hyperparameters based on validation accuracy\n",
    "if results:\n",
    "    best_result = max(results, key=lambda x: x['val_accuracy'])\n",
    "    print(\"Best hyperparameters based on validation accuracy:\")\n",
    "    print(best_result['hparams'])\n",
    "    print(f\"Validation Loss: {best_result['val_loss']:.4f}, Validation Accuracy: {best_result['val_accuracy']:.2f}%\")\n",
    "    print(\"The best model has been saved as 'SAE_classifier_best_model.pth'\")\n",
    "else:\n",
    "    print(\"No successful runs to report.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EEL",
   "language": "EEL",
   "name": "eel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

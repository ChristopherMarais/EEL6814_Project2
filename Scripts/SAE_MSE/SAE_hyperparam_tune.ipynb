{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current global best validation loss from CSV: 0.1392\n",
      "Skipping hyperparameter combination 1/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.0, 'batch_size': 16, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 2/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.0, 'batch_size': 16, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 3/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.0, 'batch_size': 32, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 4/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.0, 'batch_size': 32, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 5/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.0, 'batch_size': 64, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 6/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.0, 'batch_size': 64, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 7/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.0, 'batch_size': 128, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 8/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.0, 'batch_size': 128, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 9/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 1e-05, 'batch_size': 16, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 10/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 1e-05, 'batch_size': 16, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 11/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 1e-05, 'batch_size': 32, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 12/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 1e-05, 'batch_size': 32, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 13/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 1e-05, 'batch_size': 64, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 14/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 1e-05, 'batch_size': 64, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 15/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 1e-05, 'batch_size': 128, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 16/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 1e-05, 'batch_size': 128, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 17/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.0001, 'batch_size': 16, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 18/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.0001, 'batch_size': 16, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 19/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.0001, 'batch_size': 32, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 20/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.0001, 'batch_size': 32, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 21/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.0001, 'batch_size': 64, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 22/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.0001, 'batch_size': 64, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 23/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.0001, 'batch_size': 128, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 24/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.0001, 'batch_size': 128, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 25/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.001, 'batch_size': 16, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 26/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.001, 'batch_size': 16, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 27/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.001, 'batch_size': 32, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 28/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.001, 'batch_size': 32, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 29/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.001, 'batch_size': 64, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 30/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.001, 'batch_size': 64, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 31/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.001, 'batch_size': 128, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 32/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.001, 'batch_size': 128, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 33/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.0, 'batch_size': 16, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 34/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.0, 'batch_size': 16, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 35/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.0, 'batch_size': 32, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 36/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.0, 'batch_size': 32, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 37/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.0, 'batch_size': 64, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 38/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.0, 'batch_size': 64, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 39/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.0, 'batch_size': 128, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 40/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.0, 'batch_size': 128, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 41/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 1e-05, 'batch_size': 16, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 42/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 1e-05, 'batch_size': 16, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 43/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 1e-05, 'batch_size': 32, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 44/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 1e-05, 'batch_size': 32, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 45/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 1e-05, 'batch_size': 64, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 46/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 1e-05, 'batch_size': 64, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 47/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 1e-05, 'batch_size': 128, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Testing hyperparameter combination 48/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 1e-05, 'batch_size': 128, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0}\n",
      "Epoch [1/200], Training Loss: 0.4266, Validation Loss: 0.3039\n",
      "Epoch [2/200], Training Loss: 0.3512, Validation Loss: 0.2732\n",
      "Epoch [3/200], Training Loss: 0.3330, Validation Loss: 0.2565\n",
      "Epoch [4/200], Training Loss: 0.3223, Validation Loss: 0.2454\n",
      "Epoch [5/200], Training Loss: 0.3141, Validation Loss: 0.2387\n",
      "Epoch [6/200], Training Loss: 0.3074, Validation Loss: 0.2309\n",
      "Epoch [7/200], Training Loss: 0.3018, Validation Loss: 0.2237\n",
      "Epoch [8/200], Training Loss: 0.2968, Validation Loss: 0.2179\n",
      "Epoch [9/200], Training Loss: 0.2927, Validation Loss: 0.2117\n",
      "Epoch [10/200], Training Loss: 0.2891, Validation Loss: 0.2085\n",
      "Epoch [11/200], Training Loss: 0.2861, Validation Loss: 0.2040\n",
      "Epoch [12/200], Training Loss: 0.2832, Validation Loss: 0.2014\n",
      "Epoch [13/200], Training Loss: 0.2810, Validation Loss: 0.1985\n",
      "Epoch [14/200], Training Loss: 0.2790, Validation Loss: 0.1968\n",
      "Epoch [15/200], Training Loss: 0.2773, Validation Loss: 0.1951\n",
      "Epoch [16/200], Training Loss: 0.2756, Validation Loss: 0.1920\n",
      "Epoch [17/200], Training Loss: 0.2744, Validation Loss: 0.1919\n",
      "Epoch [18/200], Training Loss: 0.2731, Validation Loss: 0.1895\n",
      "Epoch [19/200], Training Loss: 0.2717, Validation Loss: 0.1884\n",
      "Epoch [20/200], Training Loss: 0.2707, Validation Loss: 0.1865\n",
      "Epoch [21/200], Training Loss: 0.2698, Validation Loss: 0.1852\n",
      "Epoch [22/200], Training Loss: 0.2687, Validation Loss: 0.1841\n",
      "Epoch [23/200], Training Loss: 0.2677, Validation Loss: 0.1833\n",
      "Epoch [24/200], Training Loss: 0.2672, Validation Loss: 0.1821\n",
      "Epoch [25/200], Training Loss: 0.2662, Validation Loss: 0.1823\n",
      "Epoch [26/200], Training Loss: 0.2656, Validation Loss: 0.1812\n",
      "Epoch [27/200], Training Loss: 0.2651, Validation Loss: 0.1805\n",
      "Epoch [28/200], Training Loss: 0.2642, Validation Loss: 0.1795\n",
      "Epoch [29/200], Training Loss: 0.2636, Validation Loss: 0.1780\n",
      "Epoch [30/200], Training Loss: 0.2631, Validation Loss: 0.1784\n",
      "Epoch [31/200], Training Loss: 0.2626, Validation Loss: 0.1769\n",
      "Epoch [32/200], Training Loss: 0.2620, Validation Loss: 0.1756\n",
      "Epoch [33/200], Training Loss: 0.2616, Validation Loss: 0.1750\n",
      "Epoch [34/200], Training Loss: 0.2613, Validation Loss: 0.1744\n",
      "Epoch [35/200], Training Loss: 0.2608, Validation Loss: 0.1746\n",
      "Epoch [36/200], Training Loss: 0.2602, Validation Loss: 0.1730\n",
      "Epoch [37/200], Training Loss: 0.2596, Validation Loss: 0.1726\n",
      "Epoch [38/200], Training Loss: 0.2595, Validation Loss: 0.1729\n",
      "Epoch [39/200], Training Loss: 0.2589, Validation Loss: 0.1720\n",
      "Epoch [40/200], Training Loss: 0.2586, Validation Loss: 0.1717\n",
      "Epoch [41/200], Training Loss: 0.2582, Validation Loss: 0.1704\n",
      "Epoch [42/200], Training Loss: 0.2580, Validation Loss: 0.1704\n",
      "Epoch [43/200], Training Loss: 0.2576, Validation Loss: 0.1692\n",
      "Epoch [44/200], Training Loss: 0.2573, Validation Loss: 0.1694\n",
      "Epoch [45/200], Training Loss: 0.2571, Validation Loss: 0.1688\n",
      "Epoch [46/200], Training Loss: 0.2565, Validation Loss: 0.1688\n",
      "Epoch [47/200], Training Loss: 0.2563, Validation Loss: 0.1680\n",
      "Epoch [48/200], Training Loss: 0.2561, Validation Loss: 0.1681\n",
      "Epoch [49/200], Training Loss: 0.2559, Validation Loss: 0.1681\n",
      "Epoch [50/200], Training Loss: 0.2558, Validation Loss: 0.1676\n",
      "Epoch [51/200], Training Loss: 0.2553, Validation Loss: 0.1675\n",
      "Epoch [52/200], Training Loss: 0.2549, Validation Loss: 0.1665\n",
      "Epoch [53/200], Training Loss: 0.2547, Validation Loss: 0.1659\n",
      "Epoch [54/200], Training Loss: 0.2545, Validation Loss: 0.1659\n",
      "Epoch [55/200], Training Loss: 0.2546, Validation Loss: 0.1658\n",
      "Epoch [56/200], Training Loss: 0.2543, Validation Loss: 0.1656\n",
      "Epoch [57/200], Training Loss: 0.2539, Validation Loss: 0.1654\n",
      "Epoch [58/200], Training Loss: 0.2538, Validation Loss: 0.1651\n",
      "Epoch [59/200], Training Loss: 0.2538, Validation Loss: 0.1642\n",
      "Epoch [60/200], Training Loss: 0.2535, Validation Loss: 0.1645\n",
      "Epoch [61/200], Training Loss: 0.2532, Validation Loss: 0.1633\n",
      "Epoch [62/200], Training Loss: 0.2531, Validation Loss: 0.1642\n",
      "Epoch [63/200], Training Loss: 0.2528, Validation Loss: 0.1631\n",
      "Epoch [64/200], Training Loss: 0.2525, Validation Loss: 0.1639\n",
      "Epoch [65/200], Training Loss: 0.2524, Validation Loss: 0.1629\n",
      "Epoch [66/200], Training Loss: 0.2522, Validation Loss: 0.1627\n",
      "Epoch [67/200], Training Loss: 0.2522, Validation Loss: 0.1624\n",
      "Epoch [68/200], Training Loss: 0.2516, Validation Loss: 0.1628\n",
      "Epoch [69/200], Training Loss: 0.2517, Validation Loss: 0.1621\n",
      "Epoch [70/200], Training Loss: 0.2517, Validation Loss: 0.1621\n",
      "Epoch [71/200], Training Loss: 0.2514, Validation Loss: 0.1617\n",
      "Epoch [72/200], Training Loss: 0.2514, Validation Loss: 0.1619\n",
      "Epoch [73/200], Training Loss: 0.2510, Validation Loss: 0.1623\n",
      "Epoch [74/200], Training Loss: 0.2508, Validation Loss: 0.1613\n",
      "Epoch [75/200], Training Loss: 0.2510, Validation Loss: 0.1603\n",
      "Epoch [76/200], Training Loss: 0.2506, Validation Loss: 0.1610\n",
      "Epoch [77/200], Training Loss: 0.2506, Validation Loss: 0.1606\n",
      "Epoch [78/200], Training Loss: 0.2502, Validation Loss: 0.1600\n",
      "Epoch [79/200], Training Loss: 0.2503, Validation Loss: 0.1600\n",
      "Epoch [80/200], Training Loss: 0.2501, Validation Loss: 0.1600\n",
      "Epoch [81/200], Training Loss: 0.2499, Validation Loss: 0.1595\n",
      "Epoch [82/200], Training Loss: 0.2496, Validation Loss: 0.1599\n",
      "Epoch [83/200], Training Loss: 0.2496, Validation Loss: 0.1596\n",
      "Epoch [84/200], Training Loss: 0.2495, Validation Loss: 0.1596\n",
      "Epoch [85/200], Training Loss: 0.2494, Validation Loss: 0.1594\n",
      "Epoch [86/200], Training Loss: 0.2491, Validation Loss: 0.1588\n",
      "Epoch [87/200], Training Loss: 0.2493, Validation Loss: 0.1589\n",
      "Epoch [88/200], Training Loss: 0.2491, Validation Loss: 0.1584\n",
      "Epoch [89/200], Training Loss: 0.2489, Validation Loss: 0.1598\n",
      "Epoch [90/200], Training Loss: 0.2488, Validation Loss: 0.1579\n",
      "Epoch [91/200], Training Loss: 0.2488, Validation Loss: 0.1586\n",
      "Epoch [92/200], Training Loss: 0.2484, Validation Loss: 0.1588\n",
      "Epoch [93/200], Training Loss: 0.2485, Validation Loss: 0.1585\n",
      "Epoch [94/200], Training Loss: 0.2485, Validation Loss: 0.1568\n",
      "Epoch [95/200], Training Loss: 0.2484, Validation Loss: 0.1583\n",
      "Epoch [96/200], Training Loss: 0.2482, Validation Loss: 0.1580\n",
      "Epoch [97/200], Training Loss: 0.2482, Validation Loss: 0.1576\n",
      "Epoch [98/200], Training Loss: 0.2481, Validation Loss: 0.1579\n",
      "Epoch [99/200], Training Loss: 0.2437, Validation Loss: 0.1526\n",
      "Epoch [100/200], Training Loss: 0.2428, Validation Loss: 0.1520\n",
      "Epoch [101/200], Training Loss: 0.2427, Validation Loss: 0.1517\n",
      "Epoch [102/200], Training Loss: 0.2422, Validation Loss: 0.1515\n",
      "Epoch [103/200], Training Loss: 0.2420, Validation Loss: 0.1517\n",
      "Epoch [104/200], Training Loss: 0.2419, Validation Loss: 0.1513\n",
      "Epoch [105/200], Training Loss: 0.2417, Validation Loss: 0.1513\n",
      "Epoch [106/200], Training Loss: 0.2421, Validation Loss: 0.1516\n",
      "Epoch [107/200], Training Loss: 0.2418, Validation Loss: 0.1513\n",
      "Epoch [108/200], Training Loss: 0.2417, Validation Loss: 0.1511\n",
      "Epoch [109/200], Training Loss: 0.2416, Validation Loss: 0.1513\n",
      "Epoch [110/200], Training Loss: 0.2417, Validation Loss: 0.1512\n",
      "Epoch [111/200], Training Loss: 0.2417, Validation Loss: 0.1513\n",
      "Epoch [112/200], Training Loss: 0.2415, Validation Loss: 0.1510\n",
      "Epoch [113/200], Training Loss: 0.2419, Validation Loss: 0.1511\n",
      "Early stopping at epoch 114\n",
      "Validation loss 0.1511 did not improve over the best loss 0.1392\n",
      "\n",
      "Testing hyperparameter combination 49/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.0001, 'batch_size': 16, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0}\n",
      "Epoch [1/200], Training Loss: 0.4228, Validation Loss: 0.3401\n",
      "Epoch [2/200], Training Loss: 0.3844, Validation Loss: 0.3214\n",
      "Epoch [3/200], Training Loss: 0.3760, Validation Loss: 0.3172\n",
      "Epoch [4/200], Training Loss: 0.3708, Validation Loss: 0.3105\n",
      "Epoch [5/200], Training Loss: 0.3672, Validation Loss: 0.3076\n",
      "Epoch [6/200], Training Loss: 0.3653, Validation Loss: 0.3025\n",
      "Epoch [7/200], Training Loss: 0.3637, Validation Loss: 0.3039\n",
      "Epoch [8/200], Training Loss: 0.3622, Validation Loss: 0.3017\n",
      "Epoch [9/200], Training Loss: 0.3622, Validation Loss: 0.3005\n",
      "Epoch [10/200], Training Loss: 0.3614, Validation Loss: 0.3060\n",
      "Epoch [11/200], Training Loss: 0.3611, Validation Loss: 0.2999\n",
      "Epoch [12/200], Training Loss: 0.3606, Validation Loss: 0.3021\n",
      "Epoch [13/200], Training Loss: 0.3604, Validation Loss: 0.2997\n",
      "Epoch [14/200], Training Loss: 0.3607, Validation Loss: 0.3007\n",
      "Epoch [15/200], Training Loss: 0.3602, Validation Loss: 0.2995\n",
      "Epoch [16/200], Training Loss: 0.3601, Validation Loss: 0.3012\n",
      "Epoch [17/200], Training Loss: 0.3606, Validation Loss: 0.3013\n",
      "Epoch [18/200], Training Loss: 0.3605, Validation Loss: 0.2991\n",
      "Epoch [19/200], Training Loss: 0.3622, Validation Loss: 0.2972\n",
      "Epoch [20/200], Training Loss: 0.3606, Validation Loss: 0.3008\n",
      "Epoch [21/200], Training Loss: 0.3609, Validation Loss: 0.3017\n",
      "Epoch [22/200], Training Loss: 0.3629, Validation Loss: 0.3014\n",
      "Epoch [23/200], Training Loss: 0.3622, Validation Loss: 0.3027\n",
      "Epoch [24/200], Training Loss: 0.3572, Validation Loss: 0.2935\n",
      "Epoch [25/200], Training Loss: 0.3541, Validation Loss: 0.2916\n",
      "Epoch [26/200], Training Loss: 0.3529, Validation Loss: 0.2901\n",
      "Epoch [27/200], Training Loss: 0.3523, Validation Loss: 0.2897\n",
      "Epoch [28/200], Training Loss: 0.3511, Validation Loss: 0.2886\n",
      "Epoch [29/200], Training Loss: 0.3506, Validation Loss: 0.2887\n",
      "Epoch [30/200], Training Loss: 0.3501, Validation Loss: 0.2873\n",
      "Epoch [31/200], Training Loss: 0.3498, Validation Loss: 0.2879\n",
      "Epoch [32/200], Training Loss: 0.3498, Validation Loss: 0.2869\n",
      "Epoch [33/200], Training Loss: 0.3492, Validation Loss: 0.2868\n",
      "Epoch [34/200], Training Loss: 0.3490, Validation Loss: 0.2862\n",
      "Epoch [35/200], Training Loss: 0.3489, Validation Loss: 0.2859\n",
      "Epoch [36/200], Training Loss: 0.3485, Validation Loss: 0.2859\n",
      "Epoch [37/200], Training Loss: 0.3484, Validation Loss: 0.2853\n",
      "Epoch [38/200], Training Loss: 0.3482, Validation Loss: 0.2862\n",
      "Epoch [39/200], Training Loss: 0.3479, Validation Loss: 0.2859\n",
      "Epoch [40/200], Training Loss: 0.3478, Validation Loss: 0.2852\n",
      "Epoch [41/200], Training Loss: 0.3477, Validation Loss: 0.2844\n",
      "Epoch [42/200], Training Loss: 0.3473, Validation Loss: 0.2854\n",
      "Epoch [43/200], Training Loss: 0.3472, Validation Loss: 0.2852\n",
      "Epoch [44/200], Training Loss: 0.3471, Validation Loss: 0.2834\n",
      "Epoch [45/200], Training Loss: 0.3469, Validation Loss: 0.2841\n",
      "Epoch [46/200], Training Loss: 0.3469, Validation Loss: 0.2840\n",
      "Epoch [47/200], Training Loss: 0.3467, Validation Loss: 0.2832\n",
      "Epoch [48/200], Training Loss: 0.3464, Validation Loss: 0.2836\n",
      "Epoch [49/200], Training Loss: 0.3463, Validation Loss: 0.2834\n",
      "Epoch [50/200], Training Loss: 0.3460, Validation Loss: 0.2832\n",
      "Epoch [51/200], Training Loss: 0.3459, Validation Loss: 0.2830\n",
      "Epoch [52/200], Training Loss: 0.3457, Validation Loss: 0.2821\n",
      "Epoch [53/200], Training Loss: 0.3455, Validation Loss: 0.2833\n",
      "Epoch [54/200], Training Loss: 0.3455, Validation Loss: 0.2828\n",
      "Epoch [55/200], Training Loss: 0.3453, Validation Loss: 0.2820\n",
      "Epoch [56/200], Training Loss: 0.3454, Validation Loss: 0.2820\n",
      "Epoch [57/200], Training Loss: 0.3452, Validation Loss: 0.2821\n",
      "Epoch [58/200], Training Loss: 0.3451, Validation Loss: 0.2822\n",
      "Epoch [59/200], Training Loss: 0.3451, Validation Loss: 0.2815\n",
      "Epoch [60/200], Training Loss: 0.3448, Validation Loss: 0.2821\n",
      "Epoch [61/200], Training Loss: 0.3449, Validation Loss: 0.2811\n",
      "Epoch [62/200], Training Loss: 0.3446, Validation Loss: 0.2818\n",
      "Epoch [63/200], Training Loss: 0.3446, Validation Loss: 0.2806\n",
      "Epoch [64/200], Training Loss: 0.3442, Validation Loss: 0.2806\n",
      "Epoch [65/200], Training Loss: 0.3442, Validation Loss: 0.2804\n",
      "Epoch [66/200], Training Loss: 0.3441, Validation Loss: 0.2806\n",
      "Epoch [67/200], Training Loss: 0.3440, Validation Loss: 0.2807\n",
      "Epoch [68/200], Training Loss: 0.3439, Validation Loss: 0.2813\n",
      "Epoch [69/200], Training Loss: 0.3439, Validation Loss: 0.2802\n",
      "Epoch [70/200], Training Loss: 0.3437, Validation Loss: 0.2803\n",
      "Epoch [71/200], Training Loss: 0.3437, Validation Loss: 0.2809\n",
      "Epoch [72/200], Training Loss: 0.3438, Validation Loss: 0.2794\n",
      "Epoch [73/200], Training Loss: 0.3435, Validation Loss: 0.2798\n",
      "Epoch [74/200], Training Loss: 0.3434, Validation Loss: 0.2795\n",
      "Epoch [75/200], Training Loss: 0.3431, Validation Loss: 0.2803\n",
      "Epoch [76/200], Training Loss: 0.3433, Validation Loss: 0.2794\n",
      "Epoch [77/200], Training Loss: 0.3427, Validation Loss: 0.2788\n",
      "Epoch [78/200], Training Loss: 0.3423, Validation Loss: 0.2784\n",
      "Epoch [79/200], Training Loss: 0.3424, Validation Loss: 0.2789\n",
      "Epoch [80/200], Training Loss: 0.3422, Validation Loss: 0.2784\n",
      "Epoch [81/200], Training Loss: 0.3423, Validation Loss: 0.2786\n",
      "Epoch [82/200], Training Loss: 0.3422, Validation Loss: 0.2784\n",
      "Epoch [83/200], Training Loss: 0.3422, Validation Loss: 0.2785\n",
      "Early stopping at epoch 84\n",
      "Validation loss 0.2785 did not improve over the best loss 0.1392\n",
      "\n",
      "Testing hyperparameter combination 50/1536: {'learning_rate': 0.001, 'layer_sizes': [800, 200, 25], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.0001, 'batch_size': 16, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 200, 'early_stopping_patience': 5, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0}\n",
      "Epoch [1/200], Training Loss: 0.3629, Validation Loss: 0.2656\n",
      "Epoch [2/200], Training Loss: 0.3249, Validation Loss: 0.2484\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "from torchvision import datasets, transforms\n",
    "import csv\n",
    "import time\n",
    "import json\n",
    "import hashlib\n",
    "import random\n",
    "from SAE_model import StackedAutoencoder  # Import the SAE class\n",
    "from itertools import product\n",
    "\n",
    "# Function to set all random seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # If you are using multi-GPU.\n",
    "        \n",
    "# Set the base seed\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(m.weight, nonlinearity='leaky_relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),  # Normalize to [-1, 1]\n",
    "    transforms.Lambda(lambda x: x.view(-1))  # Flatten the image to a vector\n",
    "])\n",
    "\n",
    "# Load the KMNIST dataset\n",
    "# Load the full training dataset\n",
    "full_train_dataset = datasets.KMNIST(root='../data', train=True, transform=transform, download=True)\n",
    "\n",
    "# Define the sizes for the training and validation datasets\n",
    "val_size = 10000\n",
    "train_size = len(full_train_dataset) - val_size\n",
    "\n",
    "# Split the training data into train and validation sets\n",
    "# When using random_split, create a generator with the seed\n",
    "generator = torch.Generator().manual_seed(seed)\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "\n",
    "# Hyperparameter grid for tuning\n",
    "hyperparameter_grid = {\n",
    "    'learning_rate': [1e-3, 1e-4, 1e-5],\n",
    "    'layer_sizes': [\n",
    "        [800, 200, 25],\n",
    "        [800, 200, 50],\n",
    "        [800, 200, 100],\n",
    "        [800, 200, 150]\n",
    "    ],\n",
    "    'dropout_rates': [[0.0, 0.0, 0.0], [0.1, 0.1, 0.1], [0.25, 0.25, 0.25], [0.5, 0.5, 0.5]],\n",
    "    'weight_decay': [0.0, 1e-5, 1e-4, 1e-3],\n",
    "    'batch_size': [16, 32, 64, 128],\n",
    "    'leaky_relu_negative_slope': [0.01, 0.1],\n",
    "    'num_epochs': [200],  # Max number of training epochs\n",
    "    # Early Stopping hyperparameters\n",
    "    'early_stopping_patience': [5],\n",
    "    'early_stopping_min_delta': [1e-4],\n",
    "    # ReduceLROnPlateau hyperparameters\n",
    "    'scheduler_mode': ['min'],\n",
    "    'scheduler_factor': [0.1],\n",
    "    'scheduler_patience': [3],\n",
    "    'scheduler_threshold': [1e-4],\n",
    "    'scheduler_cooldown': [0]\n",
    "}\n",
    "\n",
    "# Generate all combinations of hyperparameters\n",
    "keys, values = zip(*hyperparameter_grid.items())\n",
    "hyperparameter_combinations = [dict(zip(keys, v)) for v in product(*values)]\n",
    "\n",
    "csv_filename = 'SAE_hyperparameter_tuning_results.csv'\n",
    "\n",
    "# Initialize an empty set to store hashes of existing results\n",
    "existing_hashes = set()\n",
    "\n",
    "# Initialize global_best_val_loss by reading existing CSV\n",
    "global_best_val_loss = float('inf')  # Initialize to infinity\n",
    "\n",
    "if os.path.exists(csv_filename):\n",
    "    with open(csv_filename, 'r', newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            hparams_hash = row['hparams_hash']\n",
    "            existing_hashes.add(hparams_hash)\n",
    "            try:\n",
    "                val_loss = float(row['val_loss'])\n",
    "                if val_loss < global_best_val_loss:\n",
    "                    global_best_val_loss = val_loss\n",
    "            except ValueError:\n",
    "                # If val_loss is not a float, skip\n",
    "                continue\n",
    "    print(f\"Current global best validation loss from CSV: {global_best_val_loss:.4f}\")\n",
    "else:\n",
    "    print(\"CSV file does not exist. Starting fresh.\")\n",
    "    # If the file does not exist, we'll create it later\n",
    "\n",
    "def train_and_evaluate(hparams):\n",
    "    # Unpack hyperparameters\n",
    "    learning_rate = hparams['learning_rate']\n",
    "    layer_sizes = hparams['layer_sizes']\n",
    "    dropout_rates = hparams['dropout_rates']\n",
    "    weight_decay = hparams['weight_decay']\n",
    "    batch_size = hparams['batch_size']\n",
    "    leaky_relu_negative_slope = hparams['leaky_relu_negative_slope']\n",
    "    num_epochs = hparams.get('num_epochs', 50)\n",
    "    \n",
    "    # Early Stopping hyperparameters\n",
    "    early_stopping_patience = hparams['early_stopping_patience']\n",
    "    early_stopping_min_delta = hparams['early_stopping_min_delta']\n",
    "    \n",
    "    # Scheduler hyperparameters\n",
    "    scheduler_mode = hparams['scheduler_mode']\n",
    "    scheduler_factor = hparams['scheduler_factor']\n",
    "    scheduler_patience = hparams['scheduler_patience']\n",
    "    scheduler_threshold = hparams['scheduler_threshold']\n",
    "    scheduler_cooldown = hparams['scheduler_cooldown']\n",
    "    \n",
    "    # Define activation functions\n",
    "    activation_functions = [nn.LeakyReLU(negative_slope=leaky_relu_negative_slope) for _ in layer_sizes]\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = StackedAutoencoder(\n",
    "        input_size=784,  # For 28x28 images\n",
    "        layer_sizes=layer_sizes,\n",
    "        activation_functions=activation_functions,\n",
    "        dropout_rates=dropout_rates,\n",
    "        weight_init=init_weights\n",
    "    ).to(device)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    # Scheduler: ReduceLROnPlateau\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode=scheduler_mode,\n",
    "        factor=scheduler_factor,\n",
    "        patience=scheduler_patience,\n",
    "        threshold=scheduler_threshold,\n",
    "        threshold_mode='rel',\n",
    "        cooldown=scheduler_cooldown,\n",
    "    )\n",
    "    \n",
    "    # Data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Loss function\n",
    "    loss_function = nn.MSELoss()\n",
    "    \n",
    "    # Early Stopping variables\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "    \n",
    "    # Initialize variables for logging\n",
    "    initial_lr = optimizer.param_groups[0]['lr']\n",
    "    current_lr = initial_lr\n",
    "    lr_reduction_epochs = []\n",
    "    epoch_logs = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        if early_stop:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for data in train_loader:\n",
    "            inputs, _ = data\n",
    "            inputs = inputs.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, inputs)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                inputs, _ = data\n",
    "                inputs = inputs.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_function(outputs, inputs)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        # Scheduler step\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Check if learning rate has been reduced\n",
    "        new_lr = optimizer.param_groups[0]['lr']\n",
    "        if new_lr < current_lr:\n",
    "            lr_reduction_epochs.append(epoch+1)  # Epochs are 1-indexed\n",
    "            current_lr = new_lr\n",
    "        \n",
    "        # Record per-epoch logs\n",
    "        epoch_logs.append({\n",
    "            'epoch': epoch+1,\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'learning_rate': current_lr\n",
    "        })\n",
    "        \n",
    "        # Early Stopping check\n",
    "        if val_loss < best_val_loss - early_stopping_min_delta:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        \n",
    "        if epochs_no_improve >= early_stopping_patience:\n",
    "            early_stop = True\n",
    "        \n",
    "        # Optionally, print progress\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "              f\"Training Loss: {train_loss:.4f}, \"\n",
    "              f\"Validation Loss: {val_loss:.4f}\")\n",
    "        \n",
    "    # Prepare training log\n",
    "    training_log = {\n",
    "        'final_epoch': epoch+1,\n",
    "        'lr_reduction_epochs': lr_reduction_epochs,\n",
    "        'epoch_logs': epoch_logs\n",
    "    }\n",
    "    \n",
    "    # Return final validation loss, training log, and the trained model\n",
    "    return val_loss, training_log, model, activation_functions, optimizer, scheduler\n",
    "\n",
    "# Main loop for hyperparameter tuning\n",
    "results = []\n",
    "\n",
    "for idx, hparams in enumerate(hyperparameter_combinations):\n",
    "    # Set the base seed\n",
    "    set_seed(42)\n",
    "    # Generate a unique id for the hyperparameters\n",
    "    hparams_str = json.dumps(hparams, sort_keys=True)\n",
    "    hparams_hash = hashlib.md5(hparams_str.encode('utf-8')).hexdigest()\n",
    "    \n",
    "    # Check if this combination exists in existing_results\n",
    "    if hparams_hash in existing_hashes:\n",
    "        print(f\"Skipping hyperparameter combination {idx+1}/{len(hyperparameter_combinations)}: {hparams} (already tested)\")\n",
    "        continue\n",
    "    else:\n",
    "        print(f\"Testing hyperparameter combination {idx+1}/{len(hyperparameter_combinations)}: {hparams}\")\n",
    "        try:\n",
    "            # Call train_and_evaluate and record the computation time\n",
    "            start_time = time.time()\n",
    "            val_loss, training_log, model, activation_functions, optimizer, scheduler = train_and_evaluate(hparams)\n",
    "            end_time = time.time()\n",
    "            computation_time = end_time - start_time\n",
    "\n",
    "            # Save the per-epoch logs to a file\n",
    "            log_filename = f\"misc/training_log_{hparams_hash}.json\"\n",
    "            with open(log_filename, 'w') as f:\n",
    "                json.dump(training_log, f)\n",
    "            \n",
    "            # Append the result to the CSV file\n",
    "            with open(csv_filename, 'a', newline='') as csvfile:\n",
    "                fieldnames = ['hparams_hash'] + list(hparams.keys()) + ['val_loss', 'computation_time', 'final_num_epochs', 'lr_reduction_epochs', 'log_filename']\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                # If the file is new, write the header\n",
    "                if csvfile.tell() == 0:\n",
    "                    writer.writeheader()\n",
    "                result_row = {'hparams_hash': hparams_hash}\n",
    "                for key, value in hparams.items():\n",
    "                    # Convert value to string using json.dumps\n",
    "                    result_row[key] = json.dumps(value)\n",
    "                result_row.update({\n",
    "                    'val_loss': val_loss,\n",
    "                    'computation_time': computation_time,\n",
    "                    'final_num_epochs': training_log['final_epoch'],\n",
    "                    'lr_reduction_epochs': json.dumps(training_log['lr_reduction_epochs']),\n",
    "                    'log_filename': log_filename\n",
    "                })\n",
    "                writer.writerow(result_row)\n",
    "            \n",
    "            # Update the set of existing hashes\n",
    "            existing_hashes.add(hparams_hash)\n",
    "\n",
    "            # Check and save the best model\n",
    "            if val_loss < global_best_val_loss: \n",
    "                torch.save({\n",
    "                    'state_dict': model.state_dict(),         # Model weights\n",
    "                    'config': {                               # Model configuration\n",
    "                        'input_size': 784,\n",
    "                        'layer_sizes': hparams['layer_sizes'],\n",
    "                        'activation_functions': [str(type(act)) for act in activation_functions],\n",
    "                        'dropout_rates': hparams['dropout_rates'],\n",
    "                        'weight_init': 'kaiming_uniform',\n",
    "                    },\n",
    "                    'hyperparameters': hparams,              # Hyperparameters\n",
    "                    'training_log': training_log,            # Training logs\n",
    "                    'best_val_loss': val_loss,               # Best validation loss\n",
    "                    'optimizer_state': optimizer.state_dict(), # Optimizer state\n",
    "                    'scheduler_state': scheduler.state_dict()  # Scheduler state\n",
    "                }, 'SAE_best_model.pth')\n",
    "                global_best_val_loss = val_loss\n",
    "                print(f\"New best model saved with validation loss: {val_loss:.4f}\\n\")\n",
    "            else:\n",
    "                print(f\"Validation loss {val_loss:.4f} did not improve over the best loss {global_best_val_loss:.4f}\\n\")\n",
    "            \n",
    "            results.append({'hparams': hparams, 'val_loss': val_loss})\n",
    "        except Exception as e:\n",
    "            print(f\"Error with hyperparameters {hparams}: {e}\\n\")\n",
    "            continue\n",
    "\n",
    "# Find the best hyperparameters based on validation loss\n",
    "if results:\n",
    "    best_result = min(results, key=lambda x: x['val_loss'])\n",
    "    print(\"Best hyperparameters based on validation loss:\")\n",
    "    print(best_result['hparams'])\n",
    "    print(f\"Validation Loss: {best_result['val_loss']:.4f}\")\n",
    "    print(\"The best model has been saved as 'best_model.pth'\")\n",
    "else:\n",
    "    print(\"No successful runs to report.\")\n"
   ]
  }
 ],
 "metadata": {
  "deepnote_notebook_id": "16d034cc308c411b9c5ec5cfddcc77a3",
  "kernelspec": {
   "display_name": "EEL",
   "language": "EEL",
   "name": "eel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

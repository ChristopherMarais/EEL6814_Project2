{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current global best validation loss from CSV: 0.0361\n",
      "Skipping hyperparameter combination 1/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.001, 'batch_size': 64, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 2/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.001, 'batch_size': 64, 'leaky_relu_negative_slope': 0.2, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 3/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.001, 'batch_size': 64, 'leaky_relu_negative_slope': 0.3, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 4/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.001, 'batch_size': 64, 'leaky_relu_negative_slope': 0.4, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 5/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.001, 'batch_size': 64, 'leaky_relu_negative_slope': 0.5, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 6/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.001, 'batch_size': 128, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 7/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.001, 'batch_size': 128, 'leaky_relu_negative_slope': 0.2, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 8/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.001, 'batch_size': 128, 'leaky_relu_negative_slope': 0.3, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 9/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.001, 'batch_size': 128, 'leaky_relu_negative_slope': 0.4, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 10/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.001, 'batch_size': 128, 'leaky_relu_negative_slope': 0.5, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 11/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.001, 'batch_size': 256, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 12/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.001, 'batch_size': 256, 'leaky_relu_negative_slope': 0.2, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 13/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.001, 'batch_size': 256, 'leaky_relu_negative_slope': 0.3, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 14/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.001, 'batch_size': 256, 'leaky_relu_negative_slope': 0.4, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 15/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.001, 'batch_size': 256, 'leaky_relu_negative_slope': 0.5, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 16/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.1, 'batch_size': 64, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 17/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.1, 'batch_size': 64, 'leaky_relu_negative_slope': 0.2, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 18/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.1, 'batch_size': 64, 'leaky_relu_negative_slope': 0.3, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 19/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.1, 'batch_size': 64, 'leaky_relu_negative_slope': 0.4, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 20/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.1, 'batch_size': 64, 'leaky_relu_negative_slope': 0.5, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 21/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.1, 'batch_size': 128, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 22/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.1, 'batch_size': 128, 'leaky_relu_negative_slope': 0.2, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 23/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.1, 'batch_size': 128, 'leaky_relu_negative_slope': 0.3, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 24/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.1, 'batch_size': 128, 'leaky_relu_negative_slope': 0.4, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 25/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.1, 'batch_size': 128, 'leaky_relu_negative_slope': 0.5, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 26/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.1, 'batch_size': 256, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 27/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.1, 'batch_size': 256, 'leaky_relu_negative_slope': 0.2, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 28/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.1, 'batch_size': 256, 'leaky_relu_negative_slope': 0.3, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 29/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.1, 'batch_size': 256, 'leaky_relu_negative_slope': 0.4, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 30/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.1, 'batch_size': 256, 'leaky_relu_negative_slope': 0.5, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 31/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.2, 'batch_size': 64, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 32/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.2, 'batch_size': 64, 'leaky_relu_negative_slope': 0.2, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 33/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.2, 'batch_size': 64, 'leaky_relu_negative_slope': 0.3, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 34/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.2, 'batch_size': 64, 'leaky_relu_negative_slope': 0.4, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 35/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.2, 'batch_size': 64, 'leaky_relu_negative_slope': 0.5, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 36/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.2, 'batch_size': 128, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 37/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.2, 'batch_size': 128, 'leaky_relu_negative_slope': 0.2, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 38/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.2, 'batch_size': 128, 'leaky_relu_negative_slope': 0.3, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 39/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.2, 'batch_size': 128, 'leaky_relu_negative_slope': 0.4, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 40/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.2, 'batch_size': 128, 'leaky_relu_negative_slope': 0.5, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 41/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.2, 'batch_size': 256, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 42/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.2, 'batch_size': 256, 'leaky_relu_negative_slope': 0.2, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 43/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.2, 'batch_size': 256, 'leaky_relu_negative_slope': 0.3, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 44/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.2, 'batch_size': 256, 'leaky_relu_negative_slope': 0.4, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 45/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.2, 'batch_size': 256, 'leaky_relu_negative_slope': 0.5, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 46/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.001, 'batch_size': 64, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 47/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.001, 'batch_size': 64, 'leaky_relu_negative_slope': 0.2, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 48/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.001, 'batch_size': 64, 'leaky_relu_negative_slope': 0.3, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 49/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.001, 'batch_size': 64, 'leaky_relu_negative_slope': 0.4, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 50/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.001, 'batch_size': 64, 'leaky_relu_negative_slope': 0.5, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 51/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.001, 'batch_size': 128, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 52/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.001, 'batch_size': 128, 'leaky_relu_negative_slope': 0.2, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 53/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.001, 'batch_size': 128, 'leaky_relu_negative_slope': 0.3, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 54/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.001, 'batch_size': 128, 'leaky_relu_negative_slope': 0.4, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 55/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.001, 'batch_size': 128, 'leaky_relu_negative_slope': 0.5, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 56/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.001, 'batch_size': 256, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 57/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.001, 'batch_size': 256, 'leaky_relu_negative_slope': 0.2, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 58/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.001, 'batch_size': 256, 'leaky_relu_negative_slope': 0.3, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 59/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.001, 'batch_size': 256, 'leaky_relu_negative_slope': 0.4, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 60/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.001, 'batch_size': 256, 'leaky_relu_negative_slope': 0.5, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 61/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.1, 'batch_size': 64, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 62/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.1, 'batch_size': 64, 'leaky_relu_negative_slope': 0.2, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 63/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.1, 'batch_size': 64, 'leaky_relu_negative_slope': 0.3, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 64/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.1, 'batch_size': 64, 'leaky_relu_negative_slope': 0.4, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 65/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.1, 'batch_size': 64, 'leaky_relu_negative_slope': 0.5, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 66/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.1, 'batch_size': 128, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 67/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.1, 'batch_size': 128, 'leaky_relu_negative_slope': 0.2, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 68/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.1, 'batch_size': 128, 'leaky_relu_negative_slope': 0.3, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 69/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.1, 'batch_size': 128, 'leaky_relu_negative_slope': 0.4, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 70/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.1, 'batch_size': 128, 'leaky_relu_negative_slope': 0.5, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 71/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.1, 'batch_size': 256, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 72/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.1, 'batch_size': 256, 'leaky_relu_negative_slope': 0.2, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 73/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.1, 'batch_size': 256, 'leaky_relu_negative_slope': 0.3, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 74/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.1, 'batch_size': 256, 'leaky_relu_negative_slope': 0.4, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 75/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.1, 'batch_size': 256, 'leaky_relu_negative_slope': 0.5, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 76/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.2, 'batch_size': 64, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 77/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.2, 'batch_size': 64, 'leaky_relu_negative_slope': 0.2, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 78/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.2, 'batch_size': 64, 'leaky_relu_negative_slope': 0.3, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 79/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.2, 'batch_size': 64, 'leaky_relu_negative_slope': 0.4, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 80/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.2, 'batch_size': 64, 'leaky_relu_negative_slope': 0.5, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 81/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.2, 'batch_size': 128, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 82/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.2, 'batch_size': 128, 'leaky_relu_negative_slope': 0.2, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 83/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.2, 'batch_size': 128, 'leaky_relu_negative_slope': 0.3, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 84/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.2, 'batch_size': 128, 'leaky_relu_negative_slope': 0.4, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 85/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.2, 'batch_size': 128, 'leaky_relu_negative_slope': 0.5, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 86/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.2, 'batch_size': 256, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 87/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.2, 'batch_size': 256, 'leaky_relu_negative_slope': 0.2, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 88/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.2, 'batch_size': 256, 'leaky_relu_negative_slope': 0.3, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 89/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.2, 'batch_size': 256, 'leaky_relu_negative_slope': 0.4, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 90/360: {'learning_rate': 0.1, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.2, 'batch_size': 256, 'leaky_relu_negative_slope': 0.5, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 91/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.001, 'batch_size': 64, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 92/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.001, 'batch_size': 64, 'leaky_relu_negative_slope': 0.2, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 93/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.001, 'batch_size': 64, 'leaky_relu_negative_slope': 0.3, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 94/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.001, 'batch_size': 64, 'leaky_relu_negative_slope': 0.4, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 95/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.001, 'batch_size': 64, 'leaky_relu_negative_slope': 0.5, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 96/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.001, 'batch_size': 128, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 97/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.001, 'batch_size': 128, 'leaky_relu_negative_slope': 0.2, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 98/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.001, 'batch_size': 128, 'leaky_relu_negative_slope': 0.3, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 99/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.001, 'batch_size': 128, 'leaky_relu_negative_slope': 0.4, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 100/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.001, 'batch_size': 128, 'leaky_relu_negative_slope': 0.5, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 101/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.001, 'batch_size': 256, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 102/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.001, 'batch_size': 256, 'leaky_relu_negative_slope': 0.2, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 103/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.001, 'batch_size': 256, 'leaky_relu_negative_slope': 0.3, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 104/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.001, 'batch_size': 256, 'leaky_relu_negative_slope': 0.4, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 105/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.001, 'batch_size': 256, 'leaky_relu_negative_slope': 0.5, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 106/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.1, 'batch_size': 64, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 107/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.1, 'batch_size': 64, 'leaky_relu_negative_slope': 0.2, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 108/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.1, 'batch_size': 64, 'leaky_relu_negative_slope': 0.3, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 109/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.1, 'batch_size': 64, 'leaky_relu_negative_slope': 0.4, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 110/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.1, 'batch_size': 64, 'leaky_relu_negative_slope': 0.5, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 111/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.1, 'batch_size': 128, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 112/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.1, 'batch_size': 128, 'leaky_relu_negative_slope': 0.2, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 113/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.1, 'batch_size': 128, 'leaky_relu_negative_slope': 0.3, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 114/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.1, 'batch_size': 128, 'leaky_relu_negative_slope': 0.4, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 115/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.1, 'batch_size': 128, 'leaky_relu_negative_slope': 0.5, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 116/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.1, 'batch_size': 256, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 117/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.1, 'batch_size': 256, 'leaky_relu_negative_slope': 0.2, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 118/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.1, 'batch_size': 256, 'leaky_relu_negative_slope': 0.3, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 119/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.1, 'batch_size': 256, 'leaky_relu_negative_slope': 0.4, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 120/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.1, 'batch_size': 256, 'leaky_relu_negative_slope': 0.5, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 121/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.2, 'batch_size': 64, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 122/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.2, 'batch_size': 64, 'leaky_relu_negative_slope': 0.2, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 123/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.2, 'batch_size': 64, 'leaky_relu_negative_slope': 0.3, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 124/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.2, 'batch_size': 64, 'leaky_relu_negative_slope': 0.4, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 125/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.2, 'batch_size': 64, 'leaky_relu_negative_slope': 0.5, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 126/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.2, 'batch_size': 128, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 127/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.2, 'batch_size': 128, 'leaky_relu_negative_slope': 0.2, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 128/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.2, 'batch_size': 128, 'leaky_relu_negative_slope': 0.3, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 129/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.2, 'batch_size': 128, 'leaky_relu_negative_slope': 0.4, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 130/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.2, 'batch_size': 128, 'leaky_relu_negative_slope': 0.5, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Skipping hyperparameter combination 131/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.2, 'batch_size': 256, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0} (already tested)\n",
      "Testing hyperparameter combination 132/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.2, 'batch_size': 256, 'leaky_relu_negative_slope': 0.2, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0}\n",
      "Epoch [1/500], Training Loss: 0.4580, Validation Loss: 0.4380\n",
      "Epoch [2/500], Training Loss: 0.4372, Validation Loss: 0.4371\n",
      "Epoch [3/500], Training Loss: 0.4373, Validation Loss: 0.4375\n",
      "Epoch [4/500], Training Loss: 0.4371, Validation Loss: 0.4377\n",
      "Epoch [5/500], Training Loss: 0.4372, Validation Loss: 0.4377\n",
      "Epoch [6/500], Training Loss: 0.4137, Validation Loss: 0.3664\n",
      "Epoch [7/500], Training Loss: 0.3511, Validation Loss: 0.3215\n",
      "Epoch [8/500], Training Loss: 0.2942, Validation Loss: 0.2743\n",
      "Epoch [9/500], Training Loss: 0.2588, Validation Loss: 0.2456\n",
      "Epoch [10/500], Training Loss: 0.2322, Validation Loss: 0.2213\n",
      "Epoch [11/500], Training Loss: 0.2112, Validation Loss: 0.2029\n",
      "Epoch [12/500], Training Loss: 0.1942, Validation Loss: 0.1875\n",
      "Epoch [13/500], Training Loss: 0.1799, Validation Loss: 0.1757\n",
      "Epoch [14/500], Training Loss: 0.1686, Validation Loss: 0.1658\n",
      "Epoch [15/500], Training Loss: 0.1587, Validation Loss: 0.1571\n",
      "Epoch [16/500], Training Loss: 0.1502, Validation Loss: 0.1482\n",
      "Epoch [17/500], Training Loss: 0.1430, Validation Loss: 0.1421\n",
      "Epoch [18/500], Training Loss: 0.1368, Validation Loss: 0.1360\n",
      "Epoch [19/500], Training Loss: 0.1308, Validation Loss: 0.1320\n",
      "Epoch [20/500], Training Loss: 0.1264, Validation Loss: 0.1270\n",
      "Epoch [21/500], Training Loss: 0.1218, Validation Loss: 0.1225\n",
      "Epoch [22/500], Training Loss: 0.1181, Validation Loss: 0.1184\n",
      "Epoch [23/500], Training Loss: 0.1145, Validation Loss: 0.1173\n",
      "Epoch [24/500], Training Loss: 0.1116, Validation Loss: 0.1127\n",
      "Epoch [25/500], Training Loss: 0.1087, Validation Loss: 0.1097\n",
      "Epoch [26/500], Training Loss: 0.1064, Validation Loss: 0.1077\n",
      "Epoch [27/500], Training Loss: 0.1039, Validation Loss: 0.1071\n",
      "Epoch [28/500], Training Loss: 0.1018, Validation Loss: 0.1032\n",
      "Epoch [29/500], Training Loss: 0.1000, Validation Loss: 0.1024\n",
      "Epoch [30/500], Training Loss: 0.0983, Validation Loss: 0.1022\n",
      "Epoch [31/500], Training Loss: 0.0967, Validation Loss: 0.0994\n",
      "Epoch [32/500], Training Loss: 0.0954, Validation Loss: 0.0972\n",
      "Epoch [33/500], Training Loss: 0.0936, Validation Loss: 0.0966\n",
      "Epoch [34/500], Training Loss: 0.0925, Validation Loss: 0.0945\n",
      "Epoch [35/500], Training Loss: 0.0910, Validation Loss: 0.0931\n",
      "Epoch [36/500], Training Loss: 0.0898, Validation Loss: 0.0922\n",
      "Epoch [37/500], Training Loss: 0.0889, Validation Loss: 0.0907\n",
      "Epoch [38/500], Training Loss: 0.0878, Validation Loss: 0.0912\n",
      "Epoch [39/500], Training Loss: 0.0868, Validation Loss: 0.0895\n",
      "Epoch [40/500], Training Loss: 0.0858, Validation Loss: 0.0889\n",
      "Epoch [41/500], Training Loss: 0.0849, Validation Loss: 0.0880\n",
      "Epoch [42/500], Training Loss: 0.0839, Validation Loss: 0.0859\n",
      "Epoch [43/500], Training Loss: 0.0833, Validation Loss: 0.0872\n",
      "Epoch [44/500], Training Loss: 0.0825, Validation Loss: 0.0856\n",
      "Epoch [45/500], Training Loss: 0.0818, Validation Loss: 0.0845\n",
      "Epoch [46/500], Training Loss: 0.0807, Validation Loss: 0.0839\n",
      "Epoch [47/500], Training Loss: 0.0805, Validation Loss: 0.0835\n",
      "Epoch [48/500], Training Loss: 0.0799, Validation Loss: 0.0827\n",
      "Epoch [49/500], Training Loss: 0.0790, Validation Loss: 0.0821\n",
      "Epoch [50/500], Training Loss: 0.0786, Validation Loss: 0.0814\n",
      "Epoch [51/500], Training Loss: 0.0779, Validation Loss: 0.0801\n",
      "Epoch [52/500], Training Loss: 0.0774, Validation Loss: 0.0810\n",
      "Epoch [53/500], Training Loss: 0.0771, Validation Loss: 0.0804\n",
      "Epoch [54/500], Training Loss: 0.0764, Validation Loss: 0.0790\n",
      "Epoch [55/500], Training Loss: 0.0759, Validation Loss: 0.0805\n",
      "Epoch [56/500], Training Loss: 0.0755, Validation Loss: 0.0785\n",
      "Epoch [57/500], Training Loss: 0.0751, Validation Loss: 0.0779\n",
      "Epoch [58/500], Training Loss: 0.0746, Validation Loss: 0.0774\n",
      "Epoch [59/500], Training Loss: 0.0742, Validation Loss: 0.0770\n",
      "Epoch [60/500], Training Loss: 0.0738, Validation Loss: 0.0774\n",
      "Epoch [61/500], Training Loss: 0.0736, Validation Loss: 0.0777\n",
      "Epoch [62/500], Training Loss: 0.0732, Validation Loss: 0.0764\n",
      "Epoch [63/500], Training Loss: 0.0728, Validation Loss: 0.0766\n",
      "Epoch [64/500], Training Loss: 0.0726, Validation Loss: 0.0755\n",
      "Epoch [65/500], Training Loss: 0.0720, Validation Loss: 0.0753\n",
      "Epoch [66/500], Training Loss: 0.0717, Validation Loss: 0.0744\n",
      "Epoch [67/500], Training Loss: 0.0715, Validation Loss: 0.0748\n",
      "Epoch [68/500], Training Loss: 0.0712, Validation Loss: 0.0740\n",
      "Epoch [69/500], Training Loss: 0.0708, Validation Loss: 0.0734\n",
      "Epoch [70/500], Training Loss: 0.0705, Validation Loss: 0.0740\n",
      "Epoch [71/500], Training Loss: 0.0704, Validation Loss: 0.0737\n",
      "Epoch [72/500], Training Loss: 0.0700, Validation Loss: 0.0728\n",
      "Epoch [73/500], Training Loss: 0.0698, Validation Loss: 0.0729\n",
      "Epoch [74/500], Training Loss: 0.0695, Validation Loss: 0.0727\n",
      "Epoch [75/500], Training Loss: 0.0691, Validation Loss: 0.0720\n",
      "Epoch [76/500], Training Loss: 0.0690, Validation Loss: 0.0721\n",
      "Epoch [77/500], Training Loss: 0.0687, Validation Loss: 0.0721\n",
      "Epoch [78/500], Training Loss: 0.0687, Validation Loss: 0.0715\n",
      "Epoch [79/500], Training Loss: 0.0683, Validation Loss: 0.0720\n",
      "Epoch [80/500], Training Loss: 0.0680, Validation Loss: 0.0707\n",
      "Epoch [81/500], Training Loss: 0.0678, Validation Loss: 0.0718\n",
      "Epoch [82/500], Training Loss: 0.0677, Validation Loss: 0.0715\n",
      "Epoch [83/500], Training Loss: 0.0674, Validation Loss: 0.0707\n",
      "Epoch [84/500], Training Loss: 0.0673, Validation Loss: 0.0704\n",
      "Epoch [85/500], Training Loss: 0.0671, Validation Loss: 0.0702\n",
      "Epoch [86/500], Training Loss: 0.0667, Validation Loss: 0.0705\n",
      "Epoch [87/500], Training Loss: 0.0665, Validation Loss: 0.0692\n",
      "Epoch [88/500], Training Loss: 0.0665, Validation Loss: 0.0709\n",
      "Epoch [89/500], Training Loss: 0.0663, Validation Loss: 0.0693\n",
      "Epoch [90/500], Training Loss: 0.0661, Validation Loss: 0.0695\n",
      "Epoch [91/500], Training Loss: 0.0613, Validation Loss: 0.0638\n",
      "Epoch [92/500], Training Loss: 0.0609, Validation Loss: 0.0636\n",
      "Epoch [93/500], Training Loss: 0.0608, Validation Loss: 0.0636\n",
      "Epoch [94/500], Training Loss: 0.0607, Validation Loss: 0.0635\n",
      "Epoch [95/500], Training Loss: 0.0606, Validation Loss: 0.0634\n",
      "Epoch [96/500], Training Loss: 0.0605, Validation Loss: 0.0633\n",
      "Epoch [97/500], Training Loss: 0.0604, Validation Loss: 0.0633\n",
      "Epoch [98/500], Training Loss: 0.0602, Validation Loss: 0.0632\n",
      "Epoch [99/500], Training Loss: 0.0601, Validation Loss: 0.0631\n",
      "Epoch [100/500], Training Loss: 0.0600, Validation Loss: 0.0629\n",
      "Epoch [101/500], Training Loss: 0.0599, Validation Loss: 0.0628\n",
      "Epoch [102/500], Training Loss: 0.0598, Validation Loss: 0.0627\n",
      "Epoch [103/500], Training Loss: 0.0596, Validation Loss: 0.0626\n",
      "Epoch [104/500], Training Loss: 0.0595, Validation Loss: 0.0624\n",
      "Epoch [105/500], Training Loss: 0.0594, Validation Loss: 0.0623\n",
      "Epoch [106/500], Training Loss: 0.0592, Validation Loss: 0.0621\n",
      "Epoch [107/500], Training Loss: 0.0591, Validation Loss: 0.0622\n",
      "Epoch [108/500], Training Loss: 0.0590, Validation Loss: 0.0620\n",
      "Epoch [109/500], Training Loss: 0.0588, Validation Loss: 0.0620\n",
      "Epoch [110/500], Training Loss: 0.0587, Validation Loss: 0.0617\n",
      "Epoch [111/500], Training Loss: 0.0586, Validation Loss: 0.0616\n",
      "Epoch [112/500], Training Loss: 0.0585, Validation Loss: 0.0616\n",
      "Epoch [113/500], Training Loss: 0.0584, Validation Loss: 0.0615\n",
      "Epoch [114/500], Training Loss: 0.0583, Validation Loss: 0.0614\n",
      "Epoch [115/500], Training Loss: 0.0581, Validation Loss: 0.0611\n",
      "Epoch [116/500], Training Loss: 0.0580, Validation Loss: 0.0611\n",
      "Epoch [117/500], Training Loss: 0.0579, Validation Loss: 0.0609\n",
      "Epoch [118/500], Training Loss: 0.0579, Validation Loss: 0.0609\n",
      "Epoch [119/500], Training Loss: 0.0577, Validation Loss: 0.0609\n",
      "Epoch [120/500], Training Loss: 0.0577, Validation Loss: 0.0608\n",
      "Epoch [121/500], Training Loss: 0.0575, Validation Loss: 0.0606\n",
      "Epoch [122/500], Training Loss: 0.0575, Validation Loss: 0.0606\n",
      "Epoch [123/500], Training Loss: 0.0574, Validation Loss: 0.0605\n",
      "Epoch [124/500], Training Loss: 0.0573, Validation Loss: 0.0604\n",
      "Epoch [125/500], Training Loss: 0.0572, Validation Loss: 0.0603\n",
      "Epoch [126/500], Training Loss: 0.0571, Validation Loss: 0.0603\n",
      "Epoch [127/500], Training Loss: 0.0570, Validation Loss: 0.0603\n",
      "Epoch [128/500], Training Loss: 0.0569, Validation Loss: 0.0601\n",
      "Epoch [129/500], Training Loss: 0.0569, Validation Loss: 0.0600\n",
      "Epoch [130/500], Training Loss: 0.0568, Validation Loss: 0.0600\n",
      "Epoch [131/500], Training Loss: 0.0567, Validation Loss: 0.0600\n",
      "Epoch [132/500], Training Loss: 0.0566, Validation Loss: 0.0599\n",
      "Epoch [133/500], Training Loss: 0.0565, Validation Loss: 0.0597\n",
      "Epoch [134/500], Training Loss: 0.0565, Validation Loss: 0.0597\n",
      "Epoch [135/500], Training Loss: 0.0564, Validation Loss: 0.0596\n",
      "Epoch [136/500], Training Loss: 0.0563, Validation Loss: 0.0595\n",
      "Epoch [137/500], Training Loss: 0.0563, Validation Loss: 0.0596\n",
      "Epoch [138/500], Training Loss: 0.0562, Validation Loss: 0.0596\n",
      "Epoch [139/500], Training Loss: 0.0561, Validation Loss: 0.0594\n",
      "Epoch [140/500], Training Loss: 0.0561, Validation Loss: 0.0593\n",
      "Epoch [141/500], Training Loss: 0.0560, Validation Loss: 0.0593\n",
      "Epoch [142/500], Training Loss: 0.0559, Validation Loss: 0.0592\n",
      "Epoch [143/500], Training Loss: 0.0559, Validation Loss: 0.0590\n",
      "Epoch [144/500], Training Loss: 0.0558, Validation Loss: 0.0590\n",
      "Epoch [145/500], Training Loss: 0.0557, Validation Loss: 0.0591\n",
      "Epoch [146/500], Training Loss: 0.0557, Validation Loss: 0.0589\n",
      "Epoch [147/500], Training Loss: 0.0556, Validation Loss: 0.0588\n",
      "Epoch [148/500], Training Loss: 0.0556, Validation Loss: 0.0588\n",
      "Epoch [149/500], Training Loss: 0.0555, Validation Loss: 0.0587\n",
      "Epoch [150/500], Training Loss: 0.0554, Validation Loss: 0.0587\n",
      "Epoch [151/500], Training Loss: 0.0553, Validation Loss: 0.0585\n",
      "Epoch [152/500], Training Loss: 0.0553, Validation Loss: 0.0587\n",
      "Epoch [153/500], Training Loss: 0.0553, Validation Loss: 0.0585\n",
      "Epoch [154/500], Training Loss: 0.0552, Validation Loss: 0.0586\n",
      "Epoch [155/500], Training Loss: 0.0543, Validation Loss: 0.0576\n",
      "Epoch [156/500], Training Loss: 0.0542, Validation Loss: 0.0576\n",
      "Epoch [157/500], Training Loss: 0.0542, Validation Loss: 0.0576\n",
      "Epoch [158/500], Training Loss: 0.0542, Validation Loss: 0.0576\n",
      "Epoch [159/500], Training Loss: 0.0542, Validation Loss: 0.0576\n",
      "Epoch [160/500], Training Loss: 0.0541, Validation Loss: 0.0576\n",
      "Epoch [161/500], Training Loss: 0.0541, Validation Loss: 0.0575\n",
      "Epoch [162/500], Training Loss: 0.0541, Validation Loss: 0.0575\n",
      "Early stopping at epoch 163\n",
      "Validation loss 0.0575 did not improve over the best loss 0.0361\n",
      "\n",
      "Testing hyperparameter combination 133/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.2, 'batch_size': 256, 'leaky_relu_negative_slope': 0.3, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0}\n",
      "Epoch [1/500], Training Loss: 0.4699, Validation Loss: 0.4377\n",
      "Epoch [2/500], Training Loss: 0.4375, Validation Loss: 0.4381\n",
      "Epoch [3/500], Training Loss: 0.4375, Validation Loss: 0.4381\n",
      "Epoch [4/500], Training Loss: 0.4377, Validation Loss: 0.4376\n",
      "Epoch [5/500], Training Loss: 0.4281, Validation Loss: 0.3756\n",
      "Epoch [6/500], Training Loss: 0.3317, Validation Loss: 0.2965\n",
      "Epoch [7/500], Training Loss: 0.2612, Validation Loss: 0.2302\n",
      "Epoch [8/500], Training Loss: 0.2080, Validation Loss: 0.1941\n",
      "Epoch [9/500], Training Loss: 0.1766, Validation Loss: 0.1683\n",
      "Epoch [10/500], Training Loss: 0.1567, Validation Loss: 0.1577\n",
      "Epoch [11/500], Training Loss: 0.1437, Validation Loss: 0.1401\n",
      "Epoch [12/500], Training Loss: 0.1340, Validation Loss: 0.1315\n",
      "Epoch [13/500], Training Loss: 0.1258, Validation Loss: 0.1271\n",
      "Epoch [14/500], Training Loss: 0.1220, Validation Loss: 0.1263\n",
      "Epoch [15/500], Training Loss: 0.1182, Validation Loss: 0.1178\n",
      "Epoch [16/500], Training Loss: 0.1150, Validation Loss: 0.1152\n",
      "Epoch [17/500], Training Loss: 0.1123, Validation Loss: 0.1200\n",
      "Epoch [18/500], Training Loss: 0.1117, Validation Loss: 0.1110\n",
      "Epoch [19/500], Training Loss: 0.1085, Validation Loss: 0.1110\n",
      "Epoch [20/500], Training Loss: 0.1084, Validation Loss: 0.1353\n",
      "Epoch [21/500], Training Loss: 18.9517, Validation Loss: 0.5793\n",
      "Epoch [22/500], Training Loss: 0.7659, Validation Loss: 0.4830\n",
      "Epoch [23/500], Training Loss: 0.5545, Validation Loss: 0.4681\n",
      "Epoch [24/500], Training Loss: 0.5103, Validation Loss: 0.4592\n",
      "Epoch [25/500], Training Loss: 0.4897, Validation Loss: 0.4580\n",
      "Early stopping at epoch 26\n",
      "Validation loss 0.4580 did not improve over the best loss 0.0361\n",
      "\n",
      "Testing hyperparameter combination 134/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.0, 0.0, 0.0], 'weight_decay': 0.2, 'batch_size': 256, 'leaky_relu_negative_slope': 0.4, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0}\n",
      "Epoch [1/500], Training Loss: 0.4815, Validation Loss: 0.4388\n",
      "Epoch [2/500], Training Loss: 0.4374, Validation Loss: 0.4379\n",
      "Epoch [3/500], Training Loss: 0.4376, Validation Loss: 0.4382\n",
      "Epoch [4/500], Training Loss: 0.4374, Validation Loss: 0.4374\n",
      "Epoch [5/500], Training Loss: 0.4376, Validation Loss: 0.4209\n",
      "Epoch [6/500], Training Loss: 0.3473, Validation Loss: 0.2724\n",
      "Epoch [7/500], Training Loss: 0.2353, Validation Loss: 0.2081\n",
      "Epoch [8/500], Training Loss: 0.1860, Validation Loss: 0.1735\n",
      "Epoch [9/500], Training Loss: 0.1592, Validation Loss: 0.1542\n",
      "Epoch [10/500], Training Loss: 0.1416, Validation Loss: 0.1397\n",
      "Epoch [11/500], Training Loss: 0.1305, Validation Loss: 0.1283\n",
      "Epoch [12/500], Training Loss: 0.1223, Validation Loss: 0.1259\n",
      "Epoch [13/500], Training Loss: 0.1188, Validation Loss: 0.1158\n",
      "Epoch [14/500], Training Loss: 0.1127, Validation Loss: 0.1144\n",
      "Epoch [15/500], Training Loss: 0.1093, Validation Loss: 0.1136\n",
      "Epoch [16/500], Training Loss: 0.1112, Validation Loss: 0.1151\n",
      "Epoch [17/500], Training Loss: 0.1061, Validation Loss: 0.1049\n",
      "Epoch [18/500], Training Loss: 0.1046, Validation Loss: 0.1255\n",
      "Epoch [19/500], Training Loss: 0.1112, Validation Loss: 0.1091\n",
      "Epoch [20/500], Training Loss: 0.1043, Validation Loss: 0.1082\n",
      "Epoch [21/500], Training Loss: 0.0772, Validation Loss: 0.0751\n",
      "Epoch [22/500], Training Loss: 0.0724, Validation Loss: 0.0732\n",
      "Epoch [23/500], Training Loss: 0.0708, Validation Loss: 0.0721\n",
      "Epoch [24/500], Training Loss: 0.0696, Validation Loss: 0.0711\n",
      "Epoch [25/500], Training Loss: 0.0685, Validation Loss: 0.0699\n",
      "Epoch [26/500], Training Loss: 0.0675, Validation Loss: 0.0692\n",
      "Epoch [27/500], Training Loss: 0.0666, Validation Loss: 0.0685\n",
      "Epoch [28/500], Training Loss: 0.0658, Validation Loss: 0.0678\n",
      "Epoch [29/500], Training Loss: 0.0649, Validation Loss: 0.0669\n",
      "Epoch [30/500], Training Loss: 0.0641, Validation Loss: 0.0663\n",
      "Epoch [31/500], Training Loss: 0.0632, Validation Loss: 0.0655\n",
      "Epoch [32/500], Training Loss: 0.0624, Validation Loss: 0.0645\n",
      "Epoch [33/500], Training Loss: 0.0617, Validation Loss: 0.0640\n",
      "Epoch [34/500], Training Loss: 0.0609, Validation Loss: 0.0632\n",
      "Epoch [35/500], Training Loss: 0.0602, Validation Loss: 0.0625\n",
      "Epoch [36/500], Training Loss: 0.0594, Validation Loss: 0.0613\n",
      "Epoch [37/500], Training Loss: 0.0589, Validation Loss: 0.0607\n",
      "Epoch [38/500], Training Loss: 0.0582, Validation Loss: 0.0603\n",
      "Epoch [39/500], Training Loss: 0.0576, Validation Loss: 0.0602\n",
      "Epoch [40/500], Training Loss: 0.0571, Validation Loss: 0.0593\n",
      "Epoch [41/500], Training Loss: 0.0565, Validation Loss: 0.0584\n",
      "Epoch [42/500], Training Loss: 0.0560, Validation Loss: 0.0585\n",
      "Epoch [43/500], Training Loss: 0.0554, Validation Loss: 0.0577\n",
      "Epoch [44/500], Training Loss: 0.0549, Validation Loss: 0.0575\n",
      "Epoch [45/500], Training Loss: 0.0545, Validation Loss: 0.0565\n",
      "Epoch [46/500], Training Loss: 0.0539, Validation Loss: 0.0563\n",
      "Epoch [47/500], Training Loss: 0.0535, Validation Loss: 0.0564\n",
      "Epoch [48/500], Training Loss: 0.0532, Validation Loss: 0.0559\n",
      "Epoch [49/500], Training Loss: 0.0527, Validation Loss: 0.0548\n",
      "Epoch [50/500], Training Loss: 0.0524, Validation Loss: 0.0550\n",
      "Epoch [51/500], Training Loss: 0.0520, Validation Loss: 0.0547\n",
      "Epoch [52/500], Training Loss: 0.0517, Validation Loss: 0.0546\n",
      "Epoch [53/500], Training Loss: 0.0515, Validation Loss: 0.0534\n",
      "Epoch [54/500], Training Loss: 0.0511, Validation Loss: 0.0549\n",
      "Epoch [55/500], Training Loss: 0.0508, Validation Loss: 0.0533\n",
      "Epoch [56/500], Training Loss: 0.0505, Validation Loss: 0.0524\n",
      "Epoch [57/500], Training Loss: 0.0501, Validation Loss: 0.0522\n",
      "Epoch [58/500], Training Loss: 0.0499, Validation Loss: 0.0523\n",
      "Epoch [59/500], Training Loss: 0.0496, Validation Loss: 0.0521\n",
      "Epoch [60/500], Training Loss: 0.0493, Validation Loss: 0.0516\n",
      "Epoch [61/500], Training Loss: 0.0491, Validation Loss: 0.0513\n",
      "Epoch [62/500], Training Loss: 0.0489, Validation Loss: 0.0516\n",
      "Epoch [63/500], Training Loss: 0.0487, Validation Loss: 0.0509\n",
      "Epoch [64/500], Training Loss: 0.0485, Validation Loss: 0.0508\n",
      "Epoch [65/500], Training Loss: 0.0482, Validation Loss: 0.0505\n",
      "Epoch [66/500], Training Loss: 0.0480, Validation Loss: 0.0505\n",
      "Epoch [67/500], Training Loss: 0.0477, Validation Loss: 0.0501\n",
      "Epoch [68/500], Training Loss: 0.0475, Validation Loss: 0.0507\n",
      "Epoch [69/500], Training Loss: 0.0474, Validation Loss: 0.0502\n",
      "Epoch [70/500], Training Loss: 0.0472, Validation Loss: 0.0495\n",
      "Epoch [71/500], Training Loss: 0.0470, Validation Loss: 0.0498\n",
      "Epoch [72/500], Training Loss: 0.0469, Validation Loss: 0.0487\n",
      "Epoch [73/500], Training Loss: 0.0467, Validation Loss: 0.0490\n",
      "Epoch [74/500], Training Loss: 0.0466, Validation Loss: 0.0490\n",
      "Epoch [75/500], Training Loss: 0.0464, Validation Loss: 0.0490\n",
      "Epoch [76/500], Training Loss: 0.0432, Validation Loss: 0.0450\n",
      "Epoch [77/500], Training Loss: 0.0428, Validation Loss: 0.0449\n",
      "Epoch [78/500], Training Loss: 0.0427, Validation Loss: 0.0449\n",
      "Epoch [79/500], Training Loss: 0.0426, Validation Loss: 0.0448\n",
      "Epoch [80/500], Training Loss: 0.0426, Validation Loss: 0.0447\n",
      "Epoch [81/500], Training Loss: 0.0425, Validation Loss: 0.0447\n",
      "Epoch [82/500], Training Loss: 0.0424, Validation Loss: 0.0446\n",
      "Epoch [83/500], Training Loss: 0.0424, Validation Loss: 0.0445\n",
      "Epoch [84/500], Training Loss: 0.0423, Validation Loss: 0.0444\n",
      "Epoch [85/500], Training Loss: 0.0422, Validation Loss: 0.0443\n",
      "Epoch [86/500], Training Loss: 0.0421, Validation Loss: 0.0443\n",
      "Epoch [87/500], Training Loss: 0.0420, Validation Loss: 0.0442\n",
      "Epoch [88/500], Training Loss: 0.0419, Validation Loss: 0.0441\n",
      "Epoch [89/500], Training Loss: 0.0418, Validation Loss: 0.0440\n",
      "Epoch [90/500], Training Loss: 0.0417, Validation Loss: 0.0439\n",
      "Epoch [91/500], Training Loss: 0.0417, Validation Loss: 0.0439\n",
      "Epoch [92/500], Training Loss: 0.0416, Validation Loss: 0.0438\n",
      "Epoch [93/500], Training Loss: 0.0415, Validation Loss: 0.0438\n",
      "Epoch [94/500], Training Loss: 0.0414, Validation Loss: 0.0436\n",
      "Epoch [95/500], Training Loss: 0.0413, Validation Loss: 0.0435\n",
      "Epoch [96/500], Training Loss: 0.0413, Validation Loss: 0.0434\n",
      "Epoch [97/500], Training Loss: 0.0412, Validation Loss: 0.0434\n",
      "Epoch [98/500], Training Loss: 0.0411, Validation Loss: 0.0433\n",
      "Epoch [99/500], Training Loss: 0.0410, Validation Loss: 0.0433\n",
      "Epoch [100/500], Training Loss: 0.0410, Validation Loss: 0.0432\n",
      "Epoch [101/500], Training Loss: 0.0409, Validation Loss: 0.0431\n",
      "Epoch [102/500], Training Loss: 0.0408, Validation Loss: 0.0431\n",
      "Epoch [103/500], Training Loss: 0.0407, Validation Loss: 0.0430\n",
      "Epoch [104/500], Training Loss: 0.0407, Validation Loss: 0.0429\n",
      "Epoch [105/500], Training Loss: 0.0406, Validation Loss: 0.0428\n",
      "Epoch [106/500], Training Loss: 0.0406, Validation Loss: 0.0428\n",
      "Epoch [107/500], Training Loss: 0.0405, Validation Loss: 0.0427\n",
      "Epoch [108/500], Training Loss: 0.0404, Validation Loss: 0.0428\n",
      "Epoch [109/500], Training Loss: 0.0404, Validation Loss: 0.0426\n",
      "Epoch [110/500], Training Loss: 0.0403, Validation Loss: 0.0426\n",
      "Epoch [111/500], Training Loss: 0.0403, Validation Loss: 0.0425\n",
      "Epoch [112/500], Training Loss: 0.0402, Validation Loss: 0.0425\n",
      "Epoch [113/500], Training Loss: 0.0402, Validation Loss: 0.0424\n",
      "Epoch [114/500], Training Loss: 0.0401, Validation Loss: 0.0424\n",
      "Epoch [115/500], Training Loss: 0.0400, Validation Loss: 0.0423\n",
      "Epoch [116/500], Training Loss: 0.0400, Validation Loss: 0.0423\n",
      "Epoch [117/500], Training Loss: 0.0399, Validation Loss: 0.0422\n",
      "Epoch [118/500], Training Loss: 0.0399, Validation Loss: 0.0421\n",
      "Epoch [119/500], Training Loss: 0.0398, Validation Loss: 0.0422\n",
      "Epoch [120/500], Training Loss: 0.0398, Validation Loss: 0.0421\n",
      "Epoch [121/500], Training Loss: 0.0397, Validation Loss: 0.0420\n",
      "Epoch [122/500], Training Loss: 0.0397, Validation Loss: 0.0419\n",
      "Epoch [123/500], Training Loss: 0.0396, Validation Loss: 0.0419\n",
      "Epoch [124/500], Training Loss: 0.0396, Validation Loss: 0.0418\n",
      "Epoch [125/500], Training Loss: 0.0396, Validation Loss: 0.0418\n",
      "Epoch [126/500], Training Loss: 0.0395, Validation Loss: 0.0418\n",
      "Epoch [127/500], Training Loss: 0.0395, Validation Loss: 0.0417\n",
      "Epoch [128/500], Training Loss: 0.0394, Validation Loss: 0.0417\n",
      "Epoch [129/500], Training Loss: 0.0394, Validation Loss: 0.0416\n",
      "Epoch [130/500], Training Loss: 0.0393, Validation Loss: 0.0416\n",
      "Epoch [131/500], Training Loss: 0.0393, Validation Loss: 0.0416\n",
      "Epoch [132/500], Training Loss: 0.0393, Validation Loss: 0.0415\n",
      "Epoch [133/500], Training Loss: 0.0392, Validation Loss: 0.0414\n",
      "Epoch [134/500], Training Loss: 0.0392, Validation Loss: 0.0414\n",
      "Epoch [135/500], Training Loss: 0.0391, Validation Loss: 0.0414\n",
      "Epoch [136/500], Training Loss: 0.0391, Validation Loss: 0.0414\n",
      "Epoch [137/500], Training Loss: 0.0390, Validation Loss: 0.0414\n",
      "Epoch [138/500], Training Loss: 0.0390, Validation Loss: 0.0413\n",
      "Epoch [139/500], Training Loss: 0.0390, Validation Loss: 0.0412\n",
      "Epoch [140/500], Training Loss: 0.0389, Validation Loss: 0.0412\n",
      "Epoch [141/500], Training Loss: 0.0389, Validation Loss: 0.0411\n",
      "Epoch [142/500], Training Loss: 0.0388, Validation Loss: 0.0411\n",
      "Epoch [143/500], Training Loss: 0.0388, Validation Loss: 0.0410\n",
      "Epoch [144/500], Training Loss: 0.0388, Validation Loss: 0.0410\n",
      "Epoch [145/500], Training Loss: 0.0387, Validation Loss: 0.0410\n",
      "Epoch [146/500], Training Loss: 0.0387, Validation Loss: 0.0410\n",
      "Epoch [147/500], Training Loss: 0.0387, Validation Loss: 0.0409\n",
      "Epoch [148/500], Training Loss: 0.0386, Validation Loss: 0.0408\n",
      "Epoch [149/500], Training Loss: 0.0386, Validation Loss: 0.0408\n",
      "Epoch [150/500], Training Loss: 0.0385, Validation Loss: 0.0408\n",
      "Epoch [164/500], Training Loss: 0.0381, Validation Loss: 0.0404\n",
      "Epoch [165/500], Training Loss: 0.0380, Validation Loss: 0.0403\n",
      "Epoch [166/500], Training Loss: 0.0380, Validation Loss: 0.0403\n",
      "Epoch [167/500], Training Loss: 0.0380, Validation Loss: 0.0402\n",
      "Epoch [168/500], Training Loss: 0.0380, Validation Loss: 0.0403\n",
      "Epoch [169/500], Training Loss: 0.0379, Validation Loss: 0.0402\n",
      "Epoch [170/500], Training Loss: 0.0379, Validation Loss: 0.0402\n",
      "Epoch [171/500], Training Loss: 0.0373, Validation Loss: 0.0396\n",
      "Epoch [172/500], Training Loss: 0.0372, Validation Loss: 0.0396\n",
      "Epoch [173/500], Training Loss: 0.0372, Validation Loss: 0.0396\n",
      "Epoch [10/500], Training Loss: 0.1369, Validation Loss: 0.1391\n",
      "Epoch [11/500], Training Loss: 0.1246, Validation Loss: 0.1219\n",
      "Epoch [12/500], Training Loss: 0.1197, Validation Loss: 0.1350\n",
      "Epoch [13/500], Training Loss: 0.1157, Validation Loss: 0.1286\n",
      "Epoch [14/500], Training Loss: 0.1101, Validation Loss: 0.1070\n",
      "Epoch [15/500], Training Loss: 0.1086, Validation Loss: 0.1089\n",
      "Epoch [16/500], Training Loss: 0.1052, Validation Loss: 0.1130\n",
      "Epoch [17/500], Training Loss: 0.1161, Validation Loss: 0.1127\n",
      "Epoch [18/500], Training Loss: 0.0782, Validation Loss: 0.0755\n",
      "Epoch [19/500], Training Loss: 0.0730, Validation Loss: 0.0735\n",
      "Epoch [20/500], Training Loss: 0.0711, Validation Loss: 0.0717\n",
      "Epoch [21/500], Training Loss: 0.0696, Validation Loss: 0.0703\n",
      "Epoch [22/500], Training Loss: 0.0682, Validation Loss: 0.0691\n",
      "Epoch [23/500], Training Loss: 0.0669, Validation Loss: 0.0681\n",
      "Epoch [24/500], Training Loss: 0.0657, Validation Loss: 0.0666\n",
      "Epoch [25/500], Training Loss: 0.0646, Validation Loss: 0.0658\n",
      "Epoch [26/500], Training Loss: 0.0635, Validation Loss: 0.0650\n",
      "Epoch [27/500], Training Loss: 0.0624, Validation Loss: 0.0640\n",
      "Epoch [28/500], Training Loss: 0.0614, Validation Loss: 0.0627\n",
      "Epoch [29/500], Training Loss: 0.0603, Validation Loss: 0.0616\n",
      "Epoch [30/500], Training Loss: 0.0594, Validation Loss: 0.0611\n",
      "Epoch [31/500], Training Loss: 0.0586, Validation Loss: 0.0602\n",
      "Epoch [32/500], Training Loss: 0.0578, Validation Loss: 0.0593\n",
      "Epoch [33/500], Training Loss: 0.0571, Validation Loss: 0.0588\n",
      "Epoch [34/500], Training Loss: 0.0563, Validation Loss: 0.0583\n",
      "Epoch [35/500], Training Loss: 0.0556, Validation Loss: 0.0576\n",
      "Epoch [36/500], Training Loss: 0.0550, Validation Loss: 0.0567\n",
      "Epoch [37/500], Training Loss: 0.0544, Validation Loss: 0.0562\n",
      "Epoch [38/500], Training Loss: 0.0538, Validation Loss: 0.0559\n",
      "Epoch [39/500], Training Loss: 0.0533, Validation Loss: 0.0549\n",
      "Epoch [40/500], Training Loss: 0.0528, Validation Loss: 0.0546\n",
      "Epoch [41/500], Training Loss: 0.0523, Validation Loss: 0.0546\n",
      "Epoch [42/500], Training Loss: 0.0519, Validation Loss: 0.0537\n",
      "Epoch [43/500], Training Loss: 0.0515, Validation Loss: 0.0532\n",
      "Epoch [44/500], Training Loss: 0.0510, Validation Loss: 0.0527\n",
      "Epoch [45/500], Training Loss: 0.0506, Validation Loss: 0.0528\n",
      "Epoch [46/500], Training Loss: 0.0503, Validation Loss: 0.0523\n",
      "Epoch [47/500], Training Loss: 0.0500, Validation Loss: 0.0516\n",
      "Epoch [48/500], Training Loss: 0.0495, Validation Loss: 0.0510\n",
      "Epoch [49/500], Training Loss: 0.0492, Validation Loss: 0.0507\n",
      "Epoch [50/500], Training Loss: 0.0490, Validation Loss: 0.0510\n",
      "Epoch [51/500], Training Loss: 0.0485, Validation Loss: 0.0508\n",
      "Epoch [52/500], Training Loss: 0.0484, Validation Loss: 0.0514\n",
      "Epoch [53/500], Training Loss: 0.0450, Validation Loss: 0.0465\n",
      "Epoch [54/500], Training Loss: 0.0446, Validation Loss: 0.0463\n",
      "Epoch [55/500], Training Loss: 0.0445, Validation Loss: 0.0462\n",
      "Epoch [56/500], Training Loss: 0.0444, Validation Loss: 0.0461\n",
      "Epoch [57/500], Training Loss: 0.0443, Validation Loss: 0.0461\n",
      "Epoch [58/500], Training Loss: 0.0443, Validation Loss: 0.0460\n",
      "Epoch [59/500], Training Loss: 0.0442, Validation Loss: 0.0459\n",
      "Epoch [60/500], Training Loss: 0.0441, Validation Loss: 0.0458\n",
      "Epoch [61/500], Training Loss: 0.0440, Validation Loss: 0.0457\n",
      "Epoch [62/500], Training Loss: 0.0439, Validation Loss: 0.0456\n",
      "Epoch [63/500], Training Loss: 0.0438, Validation Loss: 0.0456\n",
      "Epoch [64/500], Training Loss: 0.0437, Validation Loss: 0.0455\n",
      "Epoch [65/500], Training Loss: 0.0436, Validation Loss: 0.0454\n",
      "Epoch [66/500], Training Loss: 0.0435, Validation Loss: 0.0453\n",
      "Epoch [67/500], Training Loss: 0.0434, Validation Loss: 0.0452\n",
      "Epoch [68/500], Training Loss: 0.0433, Validation Loss: 0.0451\n",
      "Epoch [69/500], Training Loss: 0.0432, Validation Loss: 0.0451\n",
      "Epoch [70/500], Training Loss: 0.0431, Validation Loss: 0.0449\n",
      "Epoch [71/500], Training Loss: 0.0430, Validation Loss: 0.0449\n",
      "Epoch [72/500], Training Loss: 0.0429, Validation Loss: 0.0448\n",
      "Epoch [73/500], Training Loss: 0.0429, Validation Loss: 0.0446\n",
      "Epoch [74/500], Training Loss: 0.0428, Validation Loss: 0.0446\n",
      "Epoch [75/500], Training Loss: 0.0427, Validation Loss: 0.0446\n",
      "Epoch [76/500], Training Loss: 0.0426, Validation Loss: 0.0444\n",
      "Epoch [77/500], Training Loss: 0.0425, Validation Loss: 0.0444\n",
      "Epoch [78/500], Training Loss: 0.0425, Validation Loss: 0.0442\n",
      "Epoch [79/500], Training Loss: 0.0424, Validation Loss: 0.0442\n",
      "Epoch [80/500], Training Loss: 0.0423, Validation Loss: 0.0442\n",
      "Epoch [81/500], Training Loss: 0.0422, Validation Loss: 0.0441\n",
      "Epoch [82/500], Training Loss: 0.0422, Validation Loss: 0.0439\n",
      "Epoch [83/500], Training Loss: 0.0421, Validation Loss: 0.0439\n",
      "Epoch [84/500], Training Loss: 0.0420, Validation Loss: 0.0439\n",
      "Epoch [85/500], Training Loss: 0.0420, Validation Loss: 0.0437\n",
      "Epoch [86/500], Training Loss: 0.0419, Validation Loss: 0.0437\n",
      "Epoch [87/500], Training Loss: 0.0418, Validation Loss: 0.0437\n",
      "Epoch [88/500], Training Loss: 0.0418, Validation Loss: 0.0435\n",
      "Epoch [89/500], Training Loss: 0.0417, Validation Loss: 0.0435\n",
      "Epoch [90/500], Training Loss: 0.0416, Validation Loss: 0.0434\n",
      "Epoch [91/500], Training Loss: 0.0416, Validation Loss: 0.0434\n",
      "Epoch [92/500], Training Loss: 0.0415, Validation Loss: 0.0433\n",
      "Epoch [93/500], Training Loss: 0.0415, Validation Loss: 0.0433\n",
      "Epoch [94/500], Training Loss: 0.0414, Validation Loss: 0.0432\n",
      "Epoch [95/500], Training Loss: 0.0413, Validation Loss: 0.0431\n",
      "Epoch [96/500], Training Loss: 0.0413, Validation Loss: 0.0431\n",
      "Epoch [97/500], Training Loss: 0.0412, Validation Loss: 0.0430\n",
      "Epoch [98/500], Training Loss: 0.0411, Validation Loss: 0.0430\n",
      "Epoch [99/500], Training Loss: 0.0411, Validation Loss: 0.0430\n",
      "Epoch [100/500], Training Loss: 0.0411, Validation Loss: 0.0428\n",
      "Epoch [101/500], Training Loss: 0.0410, Validation Loss: 0.0428\n",
      "Epoch [102/500], Training Loss: 0.0409, Validation Loss: 0.0428\n",
      "Epoch [103/500], Training Loss: 0.0409, Validation Loss: 0.0427\n",
      "Epoch [104/500], Training Loss: 0.0408, Validation Loss: 0.0427\n",
      "Epoch [105/500], Training Loss: 0.0408, Validation Loss: 0.0427\n",
      "Epoch [106/500], Training Loss: 0.0407, Validation Loss: 0.0426\n",
      "Epoch [107/500], Training Loss: 0.0407, Validation Loss: 0.0425\n",
      "Epoch [108/500], Training Loss: 0.0406, Validation Loss: 0.0424\n",
      "Epoch [109/500], Training Loss: 0.0406, Validation Loss: 0.0424\n",
      "Epoch [110/500], Training Loss: 0.0405, Validation Loss: 0.0423\n",
      "Epoch [111/500], Training Loss: 0.0405, Validation Loss: 0.0423\n",
      "Epoch [112/500], Training Loss: 0.0404, Validation Loss: 0.0423\n",
      "Epoch [113/500], Training Loss: 0.0404, Validation Loss: 0.0423\n",
      "Epoch [114/500], Training Loss: 0.0403, Validation Loss: 0.0422\n",
      "Epoch [115/500], Training Loss: 0.0403, Validation Loss: 0.0421\n",
      "Epoch [116/500], Training Loss: 0.0403, Validation Loss: 0.0421\n",
      "Epoch [117/500], Training Loss: 0.0402, Validation Loss: 0.0420\n",
      "Epoch [118/500], Training Loss: 0.0402, Validation Loss: 0.0420\n",
      "Epoch [119/500], Training Loss: 0.0401, Validation Loss: 0.0419\n",
      "Epoch [120/500], Training Loss: 0.0401, Validation Loss: 0.0419\n",
      "Epoch [121/500], Training Loss: 0.0400, Validation Loss: 0.0419\n",
      "Epoch [122/500], Training Loss: 0.0400, Validation Loss: 0.0418\n",
      "Epoch [123/500], Training Loss: 0.0399, Validation Loss: 0.0417\n",
      "Epoch [124/500], Training Loss: 0.0399, Validation Loss: 0.0417\n",
      "Epoch [125/500], Training Loss: 0.0399, Validation Loss: 0.0417\n",
      "Epoch [126/500], Training Loss: 0.0398, Validation Loss: 0.0416\n",
      "Epoch [127/500], Training Loss: 0.0398, Validation Loss: 0.0416\n",
      "Epoch [128/500], Training Loss: 0.0397, Validation Loss: 0.0415\n",
      "Epoch [129/500], Training Loss: 0.0397, Validation Loss: 0.0415\n",
      "Epoch [130/500], Training Loss: 0.0396, Validation Loss: 0.0415\n",
      "Epoch [131/500], Training Loss: 0.0396, Validation Loss: 0.0415\n",
      "Epoch [132/500], Training Loss: 0.0396, Validation Loss: 0.0414\n",
      "Epoch [133/500], Training Loss: 0.0395, Validation Loss: 0.0413\n",
      "Epoch [134/500], Training Loss: 0.0395, Validation Loss: 0.0413\n",
      "Epoch [135/500], Training Loss: 0.0394, Validation Loss: 0.0412\n",
      "Epoch [136/500], Training Loss: 0.0394, Validation Loss: 0.0412\n",
      "Epoch [137/500], Training Loss: 0.0394, Validation Loss: 0.0412\n",
      "Epoch [138/500], Training Loss: 0.0393, Validation Loss: 0.0411\n",
      "Epoch [139/500], Training Loss: 0.0393, Validation Loss: 0.0411\n",
      "Epoch [140/500], Training Loss: 0.0393, Validation Loss: 0.0411\n",
      "Epoch [141/500], Training Loss: 0.0392, Validation Loss: 0.0410\n",
      "Epoch [142/500], Training Loss: 0.0392, Validation Loss: 0.0410\n",
      "Epoch [143/500], Training Loss: 0.0391, Validation Loss: 0.0410\n",
      "Epoch [144/500], Training Loss: 0.0391, Validation Loss: 0.0409\n",
      "Epoch [145/500], Training Loss: 0.0391, Validation Loss: 0.0409\n",
      "Epoch [146/500], Training Loss: 0.0390, Validation Loss: 0.0409\n",
      "Epoch [147/500], Training Loss: 0.0390, Validation Loss: 0.0408\n",
      "Epoch [148/500], Training Loss: 0.0390, Validation Loss: 0.0408\n",
      "Epoch [149/500], Training Loss: 0.0389, Validation Loss: 0.0408\n",
      "Epoch [150/500], Training Loss: 0.0389, Validation Loss: 0.0407\n",
      "Epoch [151/500], Training Loss: 0.0389, Validation Loss: 0.0408\n",
      "Epoch [152/500], Training Loss: 0.0388, Validation Loss: 0.0407\n",
      "Epoch [153/500], Training Loss: 0.0388, Validation Loss: 0.0406\n",
      "Epoch [154/500], Training Loss: 0.0388, Validation Loss: 0.0406\n",
      "Epoch [155/500], Training Loss: 0.0387, Validation Loss: 0.0405\n",
      "Epoch [156/500], Training Loss: 0.0387, Validation Loss: 0.0406\n",
      "Epoch [157/500], Training Loss: 0.0387, Validation Loss: 0.0405\n",
      "Epoch [158/500], Training Loss: 0.0386, Validation Loss: 0.0405\n",
      "Epoch [159/500], Training Loss: 0.0386, Validation Loss: 0.0404\n",
      "Epoch [160/500], Training Loss: 0.0386, Validation Loss: 0.0404\n",
      "Epoch [161/500], Training Loss: 0.0385, Validation Loss: 0.0404\n",
      "Epoch [162/500], Training Loss: 0.0385, Validation Loss: 0.0403\n",
      "Epoch [163/500], Training Loss: 0.0385, Validation Loss: 0.0403\n",
      "Epoch [164/500], Training Loss: 0.0385, Validation Loss: 0.0403\n",
      "Epoch [165/500], Training Loss: 0.0384, Validation Loss: 0.0403\n",
      "Epoch [166/500], Training Loss: 0.0384, Validation Loss: 0.0402\n",
      "Epoch [167/500], Training Loss: 0.0384, Validation Loss: 0.0401\n",
      "Epoch [168/500], Training Loss: 0.0384, Validation Loss: 0.0402\n",
      "Epoch [169/500], Training Loss: 0.0383, Validation Loss: 0.0401\n",
      "Epoch [170/500], Training Loss: 0.0383, Validation Loss: 0.0401\n",
      "Epoch [171/500], Training Loss: 0.0383, Validation Loss: 0.0402\n",
      "Epoch [172/500], Training Loss: 0.0383, Validation Loss: 0.0401\n",
      "Epoch [173/500], Training Loss: 0.0382, Validation Loss: 0.0400\n",
      "Epoch [174/500], Training Loss: 0.0382, Validation Loss: 0.0400\n",
      "Epoch [175/500], Training Loss: 0.0382, Validation Loss: 0.0400\n",
      "Epoch [176/500], Training Loss: 0.0381, Validation Loss: 0.0400\n",
      "Epoch [177/500], Training Loss: 0.0381, Validation Loss: 0.0399\n",
      "Epoch [178/500], Training Loss: 0.0381, Validation Loss: 0.0399\n",
      "Epoch [179/500], Training Loss: 0.0381, Validation Loss: 0.0399\n",
      "Epoch [180/500], Training Loss: 0.0380, Validation Loss: 0.0399\n",
      "Epoch [181/500], Training Loss: 0.0380, Validation Loss: 0.0398\n",
      "Epoch [182/500], Training Loss: 0.0380, Validation Loss: 0.0398\n",
      "Epoch [183/500], Training Loss: 0.0380, Validation Loss: 0.0398\n",
      "Epoch [184/500], Training Loss: 0.0379, Validation Loss: 0.0397\n",
      "Epoch [185/500], Training Loss: 0.0379, Validation Loss: 0.0397\n",
      "Epoch [186/500], Training Loss: 0.0379, Validation Loss: 0.0397\n",
      "Epoch [187/500], Training Loss: 0.0379, Validation Loss: 0.0397\n",
      "Epoch [188/500], Training Loss: 0.0379, Validation Loss: 0.0396\n",
      "Epoch [189/500], Training Loss: 0.0378, Validation Loss: 0.0397\n",
      "Epoch [190/500], Training Loss: 0.0378, Validation Loss: 0.0396\n",
      "Epoch [191/500], Training Loss: 0.0378, Validation Loss: 0.0396\n",
      "Epoch [192/500], Training Loss: 0.0378, Validation Loss: 0.0396\n",
      "Epoch [193/500], Training Loss: 0.0377, Validation Loss: 0.0396\n",
      "Epoch [194/500], Training Loss: 0.0377, Validation Loss: 0.0395\n",
      "Epoch [195/500], Training Loss: 0.0377, Validation Loss: 0.0395\n",
      "Epoch [196/500], Training Loss: 0.0377, Validation Loss: 0.0395\n",
      "Epoch [197/500], Training Loss: 0.0377, Validation Loss: 0.0395\n",
      "Epoch [198/500], Training Loss: 0.0376, Validation Loss: 0.0394\n",
      "Epoch [199/500], Training Loss: 0.0376, Validation Loss: 0.0394\n",
      "Epoch [200/500], Training Loss: 0.0376, Validation Loss: 0.0394\n",
      "Epoch [201/500], Training Loss: 0.0376, Validation Loss: 0.0393\n",
      "Epoch [202/500], Training Loss: 0.0375, Validation Loss: 0.0394\n",
      "Epoch [203/500], Training Loss: 0.0375, Validation Loss: 0.0393\n",
      "Epoch [204/500], Training Loss: 0.0375, Validation Loss: 0.0393\n",
      "Epoch [205/500], Training Loss: 0.0369, Validation Loss: 0.0387\n",
      "Epoch [206/500], Training Loss: 0.0368, Validation Loss: 0.0387\n",
      "Epoch [207/500], Training Loss: 0.0368, Validation Loss: 0.0387\n",
      "Epoch [208/500], Training Loss: 0.0368, Validation Loss: 0.0387\n",
      "Epoch [209/500], Training Loss: 0.0368, Validation Loss: 0.0387\n",
      "Epoch [210/500], Training Loss: 0.0368, Validation Loss: 0.0387\n",
      "Epoch [211/500], Training Loss: 0.0368, Validation Loss: 0.0387\n",
      "Epoch [212/500], Training Loss: 0.0368, Validation Loss: 0.0387\n",
      "Early stopping at epoch 213\n",
      "Validation loss 0.0387 did not improve over the best loss 0.0361\n",
      "\n",
      "Testing hyperparameter combination 136/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.001, 'batch_size': 64, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0}\n",
      "Epoch [1/500], Training Loss: 0.4864, Validation Loss: 0.4598\n",
      "Epoch [2/500], Training Loss: 0.7587, Validation Loss: 0.4495\n",
      "Epoch [3/500], Training Loss: 0.4869, Validation Loss: 0.4648\n",
      "Epoch [4/500], Training Loss: 1.4401, Validation Loss: 0.4483\n",
      "Epoch [5/500], Training Loss: 0.4977, Validation Loss: 0.4456\n",
      "Epoch [6/500], Training Loss: 0.4906, Validation Loss: 0.4399\n",
      "Epoch [7/500], Training Loss: 0.4832, Validation Loss: 0.4451\n",
      "Epoch [8/500], Training Loss: 0.4839, Validation Loss: 0.4421\n",
      "Epoch [9/500], Training Loss: 0.4865, Validation Loss: 0.4487\n",
      "Epoch [10/500], Training Loss: 0.4821, Validation Loss: 0.4381\n",
      "Epoch [11/500], Training Loss: 0.4806, Validation Loss: 0.4406\n",
      "Epoch [12/500], Training Loss: 0.4807, Validation Loss: 0.4418\n",
      "Epoch [13/500], Training Loss: 0.4807, Validation Loss: 0.4420\n",
      "Epoch [14/500], Training Loss: 0.4801, Validation Loss: 0.4418\n",
      "Epoch [15/500], Training Loss: 0.4801, Validation Loss: 0.4413\n",
      "Epoch [16/500], Training Loss: 0.4799, Validation Loss: 0.4415\n",
      "Epoch [17/500], Training Loss: 0.4797, Validation Loss: 0.4413\n",
      "Early stopping at epoch 18\n",
      "Validation loss 0.4413 did not improve over the best loss 0.0361\n",
      "\n",
      "Testing hyperparameter combination 137/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.001, 'batch_size': 64, 'leaky_relu_negative_slope': 0.2, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0}\n",
      "Epoch [1/500], Training Loss: 0.4990, Validation Loss: 0.5200\n",
      "Epoch [2/500], Training Loss: 0.7958, Validation Loss: 0.5378\n",
      "Epoch [3/500], Training Loss: 0.4849, Validation Loss: 0.5187\n",
      "Epoch [4/500], Training Loss: 0.4824, Validation Loss: 0.4826\n",
      "Epoch [5/500], Training Loss: 0.9674, Validation Loss: 2.6593\n",
      "Epoch [6/500], Training Loss: 12.5114, Validation Loss: 3.6250\n",
      "Epoch [7/500], Training Loss: 1.4829, Validation Loss: 6.1011\n",
      "Epoch [8/500], Training Loss: 1.3764, Validation Loss: 0.9671\n",
      "Epoch [9/500], Training Loss: 0.6845, Validation Loss: 0.7249\n",
      "Epoch [10/500], Training Loss: 0.6295, Validation Loss: 0.5534\n",
      "Epoch [11/500], Training Loss: 0.5790, Validation Loss: 0.5370\n",
      "Early stopping at epoch 12\n",
      "Validation loss 0.5370 did not improve over the best loss 0.0361\n",
      "\n",
      "Testing hyperparameter combination 138/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.001, 'batch_size': 64, 'leaky_relu_negative_slope': 0.3, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0}\n",
      "Epoch [1/500], Training Loss: 1.0698, Validation Loss: 3.1991\n",
      "Epoch [2/500], Training Loss: 13.4133, Validation Loss: 0.7472\n",
      "Epoch [3/500], Training Loss: 2.6692, Validation Loss: 0.5540\n",
      "Epoch [4/500], Training Loss: 1.2176, Validation Loss: 0.4759\n",
      "Epoch [5/500], Training Loss: 3.3044, Validation Loss: 0.4604\n",
      "Epoch [6/500], Training Loss: 1.6962, Validation Loss: 0.6827\n",
      "Epoch [7/500], Training Loss: 4.3923, Validation Loss: 3.1538\n",
      "Epoch [8/500], Training Loss: 2.1428, Validation Loss: 0.4965\n",
      "Epoch [9/500], Training Loss: 29.7570, Validation Loss: 0.4413\n",
      "Epoch [10/500], Training Loss: 11.2042, Validation Loss: 0.4402\n",
      "Epoch [11/500], Training Loss: 2.8834, Validation Loss: 0.4421\n",
      "Epoch [12/500], Training Loss: 0.4932, Validation Loss: 0.4404\n",
      "Epoch [13/500], Training Loss: 0.4899, Validation Loss: 0.4410\n",
      "Epoch [14/500], Training Loss: 0.4873, Validation Loss: 0.4413\n",
      "Epoch [15/500], Training Loss: 0.4867, Validation Loss: 0.4424\n",
      "Epoch [16/500], Training Loss: 0.4849, Validation Loss: 0.4418\n",
      "Epoch [17/500], Training Loss: 0.4851, Validation Loss: 0.4410\n",
      "Early stopping at epoch 18\n",
      "Validation loss 0.4410 did not improve over the best loss 0.0361\n",
      "\n",
      "Testing hyperparameter combination 139/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.001, 'batch_size': 64, 'leaky_relu_negative_slope': 0.4, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0}\n",
      "Epoch [1/500], Training Loss: 36.3114, Validation Loss: 6.5434\n",
      "Epoch [2/500], Training Loss: 14.6643, Validation Loss: 6.7413\n",
      "Epoch [3/500], Training Loss: 5.1974, Validation Loss: 1.6148\n",
      "Epoch [4/500], Training Loss: 0.7782, Validation Loss: 0.5152\n",
      "Epoch [5/500], Training Loss: 0.6061, Validation Loss: 1.4294\n",
      "Epoch [6/500], Training Loss: 1.0452, Validation Loss: 3.8792\n",
      "Epoch [7/500], Training Loss: 2.8571, Validation Loss: 25.2621\n",
      "Epoch [8/500], Training Loss: 3.5173, Validation Loss: 0.5754\n",
      "Epoch [9/500], Training Loss: 0.5476, Validation Loss: 0.5490\n",
      "Epoch [10/500], Training Loss: 0.6272, Validation Loss: 0.6370\n",
      "Epoch [11/500], Training Loss: 0.5624, Validation Loss: 0.5343\n",
      "Early stopping at epoch 12\n",
      "Validation loss 0.5343 did not improve over the best loss 0.0361\n",
      "\n",
      "Testing hyperparameter combination 140/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.001, 'batch_size': 64, 'leaky_relu_negative_slope': 0.5, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0}\n",
      "Epoch [1/500], Training Loss: 71.0858, Validation Loss: 51.0762\n",
      "Epoch [2/500], Training Loss: 203.3877, Validation Loss: 5.9324\n",
      "Epoch [3/500], Training Loss: 2.7109, Validation Loss: 1.0332\n",
      "Epoch [4/500], Training Loss: 6.3394, Validation Loss: 4.4486\n",
      "Epoch [5/500], Training Loss: 3.2971, Validation Loss: 7.1644\n",
      "Epoch [6/500], Training Loss: 2.6670, Validation Loss: 0.5584\n",
      "Epoch [7/500], Training Loss: 1.0343, Validation Loss: 0.5309\n",
      "Epoch [8/500], Training Loss: 0.7956, Validation Loss: 1.6863\n",
      "Epoch [9/500], Training Loss: 0.9693, Validation Loss: 0.4554\n",
      "Epoch [10/500], Training Loss: 0.5303, Validation Loss: 0.4249\n",
      "Epoch [11/500], Training Loss: 0.4780, Validation Loss: 0.4184\n",
      "Epoch [12/500], Training Loss: 0.4874, Validation Loss: 0.4274\n",
      "Epoch [13/500], Training Loss: 0.4760, Validation Loss: 0.4202\n",
      "Epoch [14/500], Training Loss: 3.0824, Validation Loss: 2.3231\n",
      "Epoch [15/500], Training Loss: 0.7297, Validation Loss: 0.4509\n",
      "Epoch [16/500], Training Loss: 0.4871, Validation Loss: 0.4429\n",
      "Epoch [17/500], Training Loss: 0.4842, Validation Loss: 0.4455\n",
      "Epoch [18/500], Training Loss: 0.4831, Validation Loss: 0.4425\n",
      "Early stopping at epoch 19\n",
      "Validation loss 0.4425 did not improve over the best loss 0.0361\n",
      "\n",
      "Testing hyperparameter combination 141/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.001, 'batch_size': 128, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0}\n",
      "Epoch [1/500], Training Loss: 0.4860, Validation Loss: 0.4515\n",
      "Epoch [2/500], Training Loss: 0.4815, Validation Loss: 0.4412\n",
      "Epoch [3/500], Training Loss: 0.4809, Validation Loss: 0.4395\n",
      "Epoch [4/500], Training Loss: 0.4807, Validation Loss: 0.4420\n",
      "Epoch [5/500], Training Loss: 0.4808, Validation Loss: 0.4390\n",
      "Epoch [6/500], Training Loss: 0.4807, Validation Loss: 0.4446\n",
      "Epoch [7/500], Training Loss: 5.4156, Validation Loss: 3.0664\n",
      "Epoch [8/500], Training Loss: 0.8797, Validation Loss: 0.4583\n",
      "Epoch [9/500], Training Loss: 0.5580, Validation Loss: 0.4515\n",
      "Epoch [10/500], Training Loss: 0.5593, Validation Loss: 0.4532\n",
      "Epoch [11/500], Training Loss: 0.5354, Validation Loss: 0.4473\n",
      "Epoch [12/500], Training Loss: 0.5215, Validation Loss: 0.4437\n",
      "Early stopping at epoch 13\n",
      "Validation loss 0.4437 did not improve over the best loss 0.0361\n",
      "\n",
      "Testing hyperparameter combination 142/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.001, 'batch_size': 128, 'leaky_relu_negative_slope': 0.2, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0}\n",
      "Epoch [1/500], Training Loss: 0.4889, Validation Loss: 0.4583\n",
      "Epoch [2/500], Training Loss: 0.4821, Validation Loss: 0.4409\n",
      "Epoch [3/500], Training Loss: 0.4819, Validation Loss: 0.4400\n",
      "Epoch [4/500], Training Loss: 0.4826, Validation Loss: 0.4415\n",
      "Epoch [5/500], Training Loss: 0.4813, Validation Loss: 0.4435\n",
      "Epoch [6/500], Training Loss: 0.4811, Validation Loss: 0.4391\n",
      "Epoch [7/500], Training Loss: 0.4813, Validation Loss: 0.4391\n",
      "Epoch [8/500], Training Loss: 0.4819, Validation Loss: 0.4402\n",
      "Epoch [9/500], Training Loss: 0.4816, Validation Loss: 0.4388\n",
      "Epoch [10/500], Training Loss: 0.4836, Validation Loss: 0.4416\n",
      "Epoch [11/500], Training Loss: 0.4817, Validation Loss: 0.4480\n",
      "Epoch [12/500], Training Loss: 0.4821, Validation Loss: 0.4410\n",
      "Epoch [13/500], Training Loss: 0.4795, Validation Loss: 0.4408\n",
      "Epoch [14/500], Training Loss: 0.4794, Validation Loss: 0.4413\n",
      "Epoch [15/500], Training Loss: 0.4794, Validation Loss: 0.4421\n",
      "Epoch [16/500], Training Loss: 0.4790, Validation Loss: 0.4412\n",
      "Early stopping at epoch 17\n",
      "Validation loss 0.4412 did not improve over the best loss 0.0361\n",
      "\n",
      "Testing hyperparameter combination 143/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.001, 'batch_size': 128, 'leaky_relu_negative_slope': 0.3, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0}\n",
      "Epoch [1/500], Training Loss: 1.4557, Validation Loss: 0.4418\n",
      "Epoch [2/500], Training Loss: 0.4914, Validation Loss: 0.4469\n",
      "Epoch [3/500], Training Loss: 0.4838, Validation Loss: 0.4397\n",
      "Epoch [4/500], Training Loss: 0.4828, Validation Loss: 0.4393\n",
      "Epoch [5/500], Training Loss: 0.4897, Validation Loss: 0.4427\n",
      "Epoch [6/500], Training Loss: 0.4872, Validation Loss: 0.4379\n",
      "Epoch [7/500], Training Loss: 0.4814, Validation Loss: 0.4379\n",
      "Epoch [8/500], Training Loss: 0.4817, Validation Loss: 0.4414\n",
      "Epoch [9/500], Training Loss: 0.4813, Validation Loss: 0.4371\n",
      "Epoch [10/500], Training Loss: 0.4815, Validation Loss: 0.4499\n",
      "Epoch [11/500], Training Loss: 0.4815, Validation Loss: 0.4492\n",
      "Epoch [12/500], Training Loss: 0.4809, Validation Loss: 0.4406\n",
      "Epoch [13/500], Training Loss: 0.4796, Validation Loss: 0.4395\n",
      "Epoch [14/500], Training Loss: 0.4794, Validation Loss: 0.4408\n",
      "Epoch [15/500], Training Loss: 0.4792, Validation Loss: 0.4407\n",
      "Epoch [16/500], Training Loss: 0.4790, Validation Loss: 0.4411\n",
      "Early stopping at epoch 17\n",
      "Validation loss 0.4411 did not improve over the best loss 0.0361\n",
      "\n",
      "Testing hyperparameter combination 144/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.001, 'batch_size': 128, 'leaky_relu_negative_slope': 0.4, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0}\n",
      "Epoch [1/500], Training Loss: 4.0098, Validation Loss: 6.1649\n",
      "Epoch [2/500], Training Loss: 1.9144, Validation Loss: 0.4410\n",
      "Epoch [3/500], Training Loss: 3.6221, Validation Loss: 0.4502\n",
      "Epoch [4/500], Training Loss: 2.2109, Validation Loss: 10.7242\n",
      "Epoch [5/500], Training Loss: 111.4346, Validation Loss: 0.4578\n",
      "Epoch [6/500], Training Loss: 6.2185, Validation Loss: 0.4414\n",
      "Epoch [7/500], Training Loss: 3.1100, Validation Loss: 0.4712\n",
      "Epoch [8/500], Training Loss: 1.4003, Validation Loss: 0.4459\n",
      "Epoch [9/500], Training Loss: 0.9960, Validation Loss: 0.4406\n",
      "Epoch [10/500], Training Loss: 0.8815, Validation Loss: 0.4425\n",
      "Epoch [11/500], Training Loss: 1.0467, Validation Loss: 0.4399\n",
      "Epoch [12/500], Training Loss: 0.9087, Validation Loss: 0.4399\n",
      "Epoch [13/500], Training Loss: 0.7965, Validation Loss: 0.4433\n",
      "Epoch [14/500], Training Loss: 0.6995, Validation Loss: 0.4410\n",
      "Epoch [15/500], Training Loss: 0.6699, Validation Loss: 0.4404\n",
      "Epoch [16/500], Training Loss: 0.7116, Validation Loss: 0.4404\n",
      "Epoch [17/500], Training Loss: 0.6339, Validation Loss: 0.4402\n",
      "Epoch [18/500], Training Loss: 0.7077, Validation Loss: 0.4406\n",
      "Early stopping at epoch 19\n",
      "Validation loss 0.4406 did not improve over the best loss 0.0361\n",
      "\n",
      "Testing hyperparameter combination 145/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.001, 'batch_size': 128, 'leaky_relu_negative_slope': 0.5, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0}\n",
      "Epoch [1/500], Training Loss: 0.4964, Validation Loss: 0.4699\n",
      "Epoch [2/500], Training Loss: 0.4819, Validation Loss: 0.4461\n",
      "Epoch [3/500], Training Loss: 0.4812, Validation Loss: 0.4464\n",
      "Epoch [4/500], Training Loss: 0.4815, Validation Loss: 0.4462\n",
      "Epoch [5/500], Training Loss: 0.4813, Validation Loss: 0.4475\n",
      "Epoch [6/500], Training Loss: 0.4793, Validation Loss: 0.4439\n",
      "Epoch [7/500], Training Loss: 0.4794, Validation Loss: 0.4473\n",
      "Epoch [8/500], Training Loss: 0.4793, Validation Loss: 0.4444\n",
      "Epoch [9/500], Training Loss: 0.4794, Validation Loss: 0.4449\n",
      "Epoch [10/500], Training Loss: 0.4790, Validation Loss: 0.4466\n",
      "Epoch [11/500], Training Loss: 0.4790, Validation Loss: 0.4462\n",
      "Epoch [12/500], Training Loss: 0.4791, Validation Loss: 0.4455\n",
      "Epoch [13/500], Training Loss: 0.4790, Validation Loss: 0.4459\n",
      "Early stopping at epoch 14\n",
      "Validation loss 0.4459 did not improve over the best loss 0.0361\n",
      "\n",
      "Testing hyperparameter combination 146/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.001, 'batch_size': 256, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0}\n",
      "Epoch [1/500], Training Loss: 0.4903, Validation Loss: 0.4405\n",
      "Epoch [2/500], Training Loss: 0.4804, Validation Loss: 0.4378\n",
      "Epoch [3/500], Training Loss: 0.4808, Validation Loss: 0.4378\n",
      "Epoch [4/500], Training Loss: 0.4802, Validation Loss: 0.4461\n",
      "Epoch [5/500], Training Loss: 0.4803, Validation Loss: 0.4424\n",
      "Epoch [6/500], Training Loss: 0.4792, Validation Loss: 0.4417\n",
      "Epoch [7/500], Training Loss: 0.4791, Validation Loss: 0.4419\n",
      "Epoch [8/500], Training Loss: 0.4791, Validation Loss: 0.4426\n",
      "Epoch [9/500], Training Loss: 0.4789, Validation Loss: 0.4400\n",
      "Early stopping at epoch 10\n",
      "Validation loss 0.4400 did not improve over the best loss 0.0361\n",
      "\n",
      "Testing hyperparameter combination 147/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.001, 'batch_size': 256, 'leaky_relu_negative_slope': 0.2, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0}\n",
      "Epoch [1/500], Training Loss: 0.4942, Validation Loss: 0.4443\n",
      "Epoch [2/500], Training Loss: 0.4824, Validation Loss: 0.4385\n",
      "Epoch [3/500], Training Loss: 0.4814, Validation Loss: 0.4394\n",
      "Epoch [4/500], Training Loss: 0.4804, Validation Loss: 0.4417\n",
      "Epoch [5/500], Training Loss: 0.4817, Validation Loss: 0.4417\n",
      "Epoch [6/500], Training Loss: 0.4795, Validation Loss: 0.4411\n",
      "Epoch [7/500], Training Loss: 0.4750, Validation Loss: 0.4126\n",
      "Epoch [8/500], Training Loss: 0.4453, Validation Loss: 0.3943\n",
      "Epoch [9/500], Training Loss: 0.4423, Validation Loss: 0.3972\n",
      "Epoch [10/500], Training Loss: 0.4412, Validation Loss: 0.3909\n",
      "Epoch [11/500], Training Loss: 0.4382, Validation Loss: 0.3908\n",
      "Epoch [12/500], Training Loss: 0.4360, Validation Loss: 0.3906\n",
      "Epoch [13/500], Training Loss: 0.4344, Validation Loss: 0.3863\n",
      "Epoch [14/500], Training Loss: 0.4329, Validation Loss: 0.3888\n",
      "Epoch [15/500], Training Loss: 0.4317, Validation Loss: 0.3833\n",
      "Epoch [16/500], Training Loss: 0.4276, Validation Loss: 0.3766\n",
      "Epoch [17/500], Training Loss: 0.4230, Validation Loss: 0.3741\n",
      "Epoch [18/500], Training Loss: 0.4192, Validation Loss: 0.3699\n",
      "Epoch [19/500], Training Loss: 0.4178, Validation Loss: 0.3683\n",
      "Epoch [20/500], Training Loss: 0.4157, Validation Loss: 0.3639\n",
      "Epoch [21/500], Training Loss: 0.4137, Validation Loss: 0.3672\n",
      "Epoch [22/500], Training Loss: 0.4154, Validation Loss: 0.3638\n",
      "Epoch [23/500], Training Loss: 0.4154, Validation Loss: 0.3671\n",
      "Epoch [24/500], Training Loss: 0.4143, Validation Loss: 0.3624\n",
      "Epoch [25/500], Training Loss: 0.4124, Validation Loss: 0.3611\n",
      "Epoch [26/500], Training Loss: 0.4088, Validation Loss: 0.3567\n",
      "Epoch [27/500], Training Loss: 0.4075, Validation Loss: 0.3570\n",
      "Epoch [28/500], Training Loss: 0.4073, Validation Loss: 0.3572\n",
      "Epoch [29/500], Training Loss: 0.4054, Validation Loss: 0.3548\n",
      "Epoch [30/500], Training Loss: 0.4036, Validation Loss: 0.3505\n",
      "Epoch [31/500], Training Loss: 0.3994, Validation Loss: 0.3436\n",
      "Epoch [32/500], Training Loss: 0.3934, Validation Loss: 0.3398\n",
      "Epoch [33/500], Training Loss: 0.3919, Validation Loss: 0.3395\n",
      "Epoch [34/500], Training Loss: 0.3896, Validation Loss: 0.3345\n",
      "Epoch [35/500], Training Loss: 0.3867, Validation Loss: 0.3328\n",
      "Epoch [36/500], Training Loss: 0.3861, Validation Loss: 0.3289\n",
      "Epoch [37/500], Training Loss: 0.3831, Validation Loss: 0.3287\n",
      "Epoch [38/500], Training Loss: 0.3822, Validation Loss: 0.3288\n",
      "Epoch [39/500], Training Loss: 0.3806, Validation Loss: 0.3236\n",
      "Epoch [40/500], Training Loss: 0.3798, Validation Loss: 0.3252\n",
      "Epoch [41/500], Training Loss: 0.3792, Validation Loss: 0.3277\n",
      "Epoch [42/500], Training Loss: 0.3783, Validation Loss: 0.3259\n",
      "Epoch [43/500], Training Loss: 0.3746, Validation Loss: 0.3195\n",
      "Epoch [44/500], Training Loss: 0.3721, Validation Loss: 0.3176\n",
      "Epoch [45/500], Training Loss: 0.3710, Validation Loss: 0.3163\n",
      "Epoch [46/500], Training Loss: 0.3704, Validation Loss: 0.3159\n",
      "Epoch [47/500], Training Loss: 0.3700, Validation Loss: 0.3161\n",
      "Epoch [48/500], Training Loss: 0.3695, Validation Loss: 0.3150\n",
      "Epoch [49/500], Training Loss: 0.3689, Validation Loss: 0.3142\n",
      "Epoch [50/500], Training Loss: 0.3688, Validation Loss: 0.3145\n",
      "Epoch [51/500], Training Loss: 0.3682, Validation Loss: 0.3140\n",
      "Epoch [52/500], Training Loss: 0.3680, Validation Loss: 0.3133\n",
      "Epoch [53/500], Training Loss: 0.3677, Validation Loss: 0.3128\n",
      "Epoch [54/500], Training Loss: 0.3674, Validation Loss: 0.3129\n",
      "Epoch [55/500], Training Loss: 0.3669, Validation Loss: 0.3133\n",
      "Epoch [56/500], Training Loss: 0.3664, Validation Loss: 0.3117\n",
      "Epoch [57/500], Training Loss: 0.3663, Validation Loss: 0.3115\n",
      "Epoch [58/500], Training Loss: 0.3662, Validation Loss: 0.3113\n",
      "Epoch [59/500], Training Loss: 0.3658, Validation Loss: 0.3106\n",
      "Epoch [60/500], Training Loss: 0.3653, Validation Loss: 0.3104\n",
      "Epoch [61/500], Training Loss: 0.3650, Validation Loss: 0.3103\n",
      "Epoch [62/500], Training Loss: 0.3648, Validation Loss: 0.3103\n",
      "Epoch [63/500], Training Loss: 0.3649, Validation Loss: 0.3100\n",
      "Epoch [64/500], Training Loss: 0.3647, Validation Loss: 0.3110\n",
      "Epoch [65/500], Training Loss: 0.3645, Validation Loss: 0.3099\n",
      "Epoch [66/500], Training Loss: 0.3641, Validation Loss: 0.3093\n",
      "Epoch [67/500], Training Loss: 0.3637, Validation Loss: 0.3089\n",
      "Epoch [68/500], Training Loss: 0.3636, Validation Loss: 0.3091\n",
      "Epoch [69/500], Training Loss: 0.3635, Validation Loss: 0.3089\n",
      "Epoch [70/500], Training Loss: 0.3634, Validation Loss: 0.3087\n",
      "Epoch [71/500], Training Loss: 0.3630, Validation Loss: 0.3086\n",
      "Epoch [72/500], Training Loss: 0.3628, Validation Loss: 0.3078\n",
      "Epoch [73/500], Training Loss: 0.3626, Validation Loss: 0.3076\n",
      "Epoch [74/500], Training Loss: 0.3624, Validation Loss: 0.3064\n",
      "Epoch [75/500], Training Loss: 0.3621, Validation Loss: 0.3078\n",
      "Epoch [76/500], Training Loss: 0.3622, Validation Loss: 0.3077\n",
      "Epoch [77/500], Training Loss: 0.3622, Validation Loss: 0.3079\n",
      "Epoch [78/500], Training Loss: 0.3615, Validation Loss: 0.3065\n",
      "Epoch [79/500], Training Loss: 0.3613, Validation Loss: 0.3064\n",
      "Epoch [80/500], Training Loss: 0.3610, Validation Loss: 0.3060\n",
      "Epoch [81/500], Training Loss: 0.3611, Validation Loss: 0.3059\n",
      "Epoch [82/500], Training Loss: 0.3610, Validation Loss: 0.3060\n",
      "Epoch [83/500], Training Loss: 0.3609, Validation Loss: 0.3061\n",
      "Epoch [84/500], Training Loss: 0.3608, Validation Loss: 0.3059\n",
      "Epoch [85/500], Training Loss: 0.3607, Validation Loss: 0.3059\n",
      "Epoch [86/500], Training Loss: 0.3609, Validation Loss: 0.3058\n",
      "Epoch [87/500], Training Loss: 0.3608, Validation Loss: 0.3058\n",
      "Epoch [88/500], Training Loss: 0.3606, Validation Loss: 0.3058\n",
      "Epoch [89/500], Training Loss: 0.3607, Validation Loss: 0.3058\n",
      "Epoch [90/500], Training Loss: 0.3607, Validation Loss: 0.3059\n",
      "Epoch [91/500], Training Loss: 0.3606, Validation Loss: 0.3059\n",
      "Epoch [92/500], Training Loss: 0.3609, Validation Loss: 0.3058\n",
      "Epoch [93/500], Training Loss: 0.3607, Validation Loss: 0.3058\n",
      "Early stopping at epoch 94\n",
      "Validation loss 0.3058 did not improve over the best loss 0.0361\n",
      "\n",
      "Testing hyperparameter combination 148/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.001, 'batch_size': 256, 'leaky_relu_negative_slope': 0.3, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0}\n",
      "Epoch [1/500], Training Loss: 0.5067, Validation Loss: 0.4453\n",
      "Epoch [2/500], Training Loss: 0.4809, Validation Loss: 0.4402\n",
      "Epoch [3/500], Training Loss: 0.4815, Validation Loss: 0.4399\n",
      "Epoch [4/500], Training Loss: 0.4804, Validation Loss: 0.4407\n",
      "Epoch [5/500], Training Loss: 0.4803, Validation Loss: 0.4395\n",
      "Epoch [6/500], Training Loss: 0.4809, Validation Loss: 0.4412\n",
      "Epoch [7/500], Training Loss: 0.4804, Validation Loss: 0.4400\n",
      "Epoch [8/500], Training Loss: 0.4803, Validation Loss: 0.4461\n",
      "Epoch [9/500], Training Loss: 0.4792, Validation Loss: 0.4414\n",
      "Epoch [10/500], Training Loss: 0.4792, Validation Loss: 0.4419\n",
      "Epoch [11/500], Training Loss: 0.4792, Validation Loss: 0.4429\n",
      "Epoch [12/500], Training Loss: 0.4790, Validation Loss: 0.4409\n",
      "Early stopping at epoch 13\n",
      "Validation loss 0.4409 did not improve over the best loss 0.0361\n",
      "\n",
      "Testing hyperparameter combination 149/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.001, 'batch_size': 256, 'leaky_relu_negative_slope': 0.4, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0}\n",
      "Epoch [1/500], Training Loss: 0.5086, Validation Loss: 0.4482\n",
      "Epoch [2/500], Training Loss: 0.4808, Validation Loss: 0.4446\n",
      "Epoch [3/500], Training Loss: 0.4814, Validation Loss: 0.4445\n",
      "Epoch [4/500], Training Loss: 0.4804, Validation Loss: 0.4470\n",
      "Epoch [5/500], Training Loss: 0.4802, Validation Loss: 0.4434\n",
      "Epoch [6/500], Training Loss: 0.4808, Validation Loss: 0.4456\n",
      "Epoch [7/500], Training Loss: 0.4804, Validation Loss: 0.4434\n",
      "Epoch [8/500], Training Loss: 0.4802, Validation Loss: 0.4506\n",
      "Epoch [9/500], Training Loss: 0.4792, Validation Loss: 0.4463\n",
      "Epoch [10/500], Training Loss: 0.4792, Validation Loss: 0.4467\n",
      "Epoch [11/500], Training Loss: 0.4792, Validation Loss: 0.4485\n",
      "Epoch [12/500], Training Loss: 0.4790, Validation Loss: 0.4469\n",
      "Early stopping at epoch 13\n",
      "Validation loss 0.4469 did not improve over the best loss 0.0361\n",
      "\n",
      "Testing hyperparameter combination 150/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.001, 'batch_size': 256, 'leaky_relu_negative_slope': 0.5, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0}\n",
      "Epoch [1/500], Training Loss: 0.5213, Validation Loss: 0.6541\n",
      "Epoch [2/500], Training Loss: 0.4802, Validation Loss: 0.6381\n",
      "Epoch [3/500], Training Loss: 0.4812, Validation Loss: 0.6205\n",
      "Epoch [4/500], Training Loss: 0.4804, Validation Loss: 0.5999\n",
      "Epoch [5/500], Training Loss: 0.4802, Validation Loss: 0.5865\n",
      "Epoch [6/500], Training Loss: 1.2440, Validation Loss: 625.0537\n",
      "Epoch [7/500], Training Loss: 65.8424, Validation Loss: 3.1778\n",
      "Epoch [8/500], Training Loss: 6.1042, Validation Loss: 0.4752\n",
      "Epoch [9/500], Training Loss: 1.0689, Validation Loss: 0.4937\n",
      "Epoch [10/500], Training Loss: 0.7821, Validation Loss: 0.4523\n",
      "Epoch [11/500], Training Loss: 0.5910, Validation Loss: 0.4454\n",
      "Epoch [12/500], Training Loss: 0.6146, Validation Loss: 0.4860\n",
      "Epoch [13/500], Training Loss: 0.9742, Validation Loss: 0.4790\n",
      "Epoch [14/500], Training Loss: 0.7396, Validation Loss: 0.5934\n",
      "Epoch [15/500], Training Loss: 1.0214, Validation Loss: 0.5178\n",
      "Epoch [16/500], Training Loss: 0.8460, Validation Loss: 0.4830\n",
      "Epoch [17/500], Training Loss: 0.5950, Validation Loss: 0.4700\n",
      "Epoch [18/500], Training Loss: 0.5577, Validation Loss: 0.4701\n",
      "Early stopping at epoch 19\n",
      "Validation loss 0.4701 did not improve over the best loss 0.0361\n",
      "\n",
      "Testing hyperparameter combination 151/360: {'learning_rate': 0.01, 'layer_sizes': [800, 200, 120], 'dropout_rates': [0.1, 0.1, 0.1], 'weight_decay': 0.1, 'batch_size': 64, 'leaky_relu_negative_slope': 0.1, 'num_epochs': 500, 'early_stopping_patience': 7, 'early_stopping_min_delta': 0.0001, 'scheduler_mode': 'min', 'scheduler_factor': 0.15, 'scheduler_patience': 2, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0}\n",
      "Epoch [1/500], Training Loss: 0.4856, Validation Loss: 0.4603\n",
      "Epoch [2/500], Training Loss: 1.2089, Validation Loss: 0.4782\n",
      "Epoch [3/500], Training Loss: 0.6752, Validation Loss: 0.4410\n",
      "Epoch [4/500], Training Loss: 0.4798, Validation Loss: 0.4414\n",
      "Epoch [5/500], Training Loss: 0.4799, Validation Loss: 0.4436\n",
      "Epoch [6/500], Training Loss: 0.4799, Validation Loss: 0.4410\n",
      "Epoch [7/500], Training Loss: 0.4791, Validation Loss: 0.4414\n",
      "Epoch [8/500], Training Loss: 0.4547, Validation Loss: 0.3970\n",
      "Epoch [9/500], Training Loss: 0.4309, Validation Loss: 0.3734\n",
      "Epoch [10/500], Training Loss: 0.4153, Validation Loss: 0.3587\n",
      "Epoch [11/500], Training Loss: 0.3954, Validation Loss: 0.3261\n",
      "Epoch [12/500], Training Loss: 0.3707, Validation Loss: 0.3087\n",
      "Epoch [13/500], Training Loss: 0.3568, Validation Loss: 0.2926\n",
      "Epoch [14/500], Training Loss: 0.3437, Validation Loss: 0.2805\n",
      "Epoch [15/500], Training Loss: 0.3320, Validation Loss: 0.2657\n",
      "Epoch [16/500], Training Loss: 0.3194, Validation Loss: 0.2500\n",
      "Epoch [17/500], Training Loss: 0.3077, Validation Loss: 0.2384\n",
      "Epoch [18/500], Training Loss: 0.2984, Validation Loss: 0.2286\n",
      "Epoch [19/500], Training Loss: 0.2902, Validation Loss: 0.2176\n",
      "Epoch [20/500], Training Loss: 0.2830, Validation Loss: 0.2089\n",
      "Epoch [21/500], Training Loss: 0.2766, Validation Loss: 0.2022\n",
      "Epoch [22/500], Training Loss: 0.2710, Validation Loss: 0.1947\n",
      "Epoch [23/500], Training Loss: 0.2657, Validation Loss: 0.1892\n",
      "Epoch [24/500], Training Loss: 0.2608, Validation Loss: 0.1838\n",
      "Epoch [25/500], Training Loss: 0.2566, Validation Loss: 0.1783\n",
      "Epoch [26/500], Training Loss: 0.2527, Validation Loss: 0.1740\n",
      "Epoch [27/500], Training Loss: 0.2487, Validation Loss: 0.1697\n",
      "Epoch [28/500], Training Loss: 0.2455, Validation Loss: 0.1674\n",
      "Epoch [29/500], Training Loss: 0.2422, Validation Loss: 0.1624\n",
      "Epoch [30/500], Training Loss: 0.2391, Validation Loss: 0.1587\n",
      "Epoch [31/500], Training Loss: 0.2364, Validation Loss: 0.1556\n",
      "Epoch [32/500], Training Loss: 0.2339, Validation Loss: 0.1512\n",
      "Epoch [33/500], Training Loss: 0.2317, Validation Loss: 0.1498\n",
      "Epoch [34/500], Training Loss: 0.2296, Validation Loss: 0.1469\n",
      "Epoch [35/500], Training Loss: 0.2276, Validation Loss: 0.1430\n",
      "Epoch [36/500], Training Loss: 0.2259, Validation Loss: 0.1430\n",
      "Epoch [37/500], Training Loss: 0.2243, Validation Loss: 0.1406\n",
      "Epoch [38/500], Training Loss: 0.2229, Validation Loss: 0.1396\n",
      "Epoch [39/500], Training Loss: 0.2216, Validation Loss: 0.1379\n",
      "Epoch [40/500], Training Loss: 0.2203, Validation Loss: 0.1366\n",
      "Epoch [41/500], Training Loss: 0.2191, Validation Loss: 0.1365\n",
      "Epoch [42/500], Training Loss: 0.2181, Validation Loss: 0.1334\n",
      "Epoch [43/500], Training Loss: 0.2169, Validation Loss: 0.1330\n",
      "Epoch [44/500], Training Loss: 0.2160, Validation Loss: 0.1293\n",
      "Epoch [45/500], Training Loss: 0.2152, Validation Loss: 0.1303\n",
      "Epoch [46/500], Training Loss: 0.2143, Validation Loss: 0.1291\n",
      "Epoch [47/500], Training Loss: 0.2136, Validation Loss: 0.1289\n",
      "Epoch [48/500], Training Loss: 0.2127, Validation Loss: 0.1298\n",
      "Epoch [49/500], Training Loss: 0.2120, Validation Loss: 0.1266\n",
      "Epoch [50/500], Training Loss: 0.2112, Validation Loss: 0.1265\n",
      "Epoch [51/500], Training Loss: 0.2106, Validation Loss: 0.1243\n",
      "Epoch [52/500], Training Loss: 0.2100, Validation Loss: 0.1234\n",
      "Epoch [53/500], Training Loss: 0.2094, Validation Loss: 0.1233\n",
      "Epoch [54/500], Training Loss: 0.2089, Validation Loss: 0.1223\n",
      "Epoch [55/500], Training Loss: 0.2084, Validation Loss: 0.1236\n",
      "Epoch [56/500], Training Loss: 0.2079, Validation Loss: 0.1218\n",
      "Epoch [57/500], Training Loss: 0.2074, Validation Loss: 0.1220\n",
      "Epoch [58/500], Training Loss: 0.2070, Validation Loss: 0.1209\n",
      "Epoch [59/500], Training Loss: 0.2065, Validation Loss: 0.1194\n",
      "Epoch [60/500], Training Loss: 0.2061, Validation Loss: 0.1207\n",
      "Epoch [61/500], Training Loss: 0.2056, Validation Loss: 0.1183\n",
      "Epoch [62/500], Training Loss: 0.2053, Validation Loss: 0.1215\n",
      "Epoch [63/500], Training Loss: 0.2049, Validation Loss: 0.1178\n",
      "Epoch [64/500], Training Loss: 0.2047, Validation Loss: 0.1183\n",
      "Epoch [65/500], Training Loss: 0.2044, Validation Loss: 0.1184\n",
      "Epoch [66/500], Training Loss: 0.2040, Validation Loss: 0.1167\n",
      "Epoch [67/500], Training Loss: 0.2036, Validation Loss: 0.1163\n",
      "Epoch [68/500], Training Loss: 0.2032, Validation Loss: 0.1152\n",
      "Epoch [69/500], Training Loss: 0.2029, Validation Loss: 0.1163\n",
      "Epoch [70/500], Training Loss: 0.2028, Validation Loss: 0.1159\n",
      "Epoch [71/500], Training Loss: 0.2025, Validation Loss: 0.1173\n",
      "Epoch [72/500], Training Loss: 0.1908, Validation Loss: 0.1018\n",
      "Epoch [73/500], Training Loss: 0.1883, Validation Loss: 0.1006\n",
      "Epoch [74/500], Training Loss: 0.1876, Validation Loss: 0.1002\n",
      "Epoch [75/500], Training Loss: 0.1871, Validation Loss: 0.0997\n",
      "Epoch [76/500], Training Loss: 0.1866, Validation Loss: 0.0998\n",
      "Epoch [77/500], Training Loss: 0.1862, Validation Loss: 0.0988\n",
      "Epoch [78/500], Training Loss: 0.1860, Validation Loss: 0.0987\n",
      "Epoch [79/500], Training Loss: 0.1856, Validation Loss: 0.0984\n",
      "Epoch [80/500], Training Loss: 0.1855, Validation Loss: 0.0982\n",
      "Epoch [81/500], Training Loss: 0.1853, Validation Loss: 0.0983\n",
      "Epoch [82/500], Training Loss: 0.1852, Validation Loss: 0.0979\n",
      "Epoch [83/500], Training Loss: 0.1850, Validation Loss: 0.0973\n",
      "Epoch [84/500], Training Loss: 0.1847, Validation Loss: 0.0973\n",
      "Epoch [85/500], Training Loss: 0.1846, Validation Loss: 0.0973\n",
      "Epoch [86/500], Training Loss: 0.1845, Validation Loss: 0.0967\n",
      "Epoch [87/500], Training Loss: 0.1844, Validation Loss: 0.0967\n",
      "Epoch [88/500], Training Loss: 0.1842, Validation Loss: 0.0968\n",
      "Epoch [89/500], Training Loss: 0.1841, Validation Loss: 0.0962\n",
      "Epoch [90/500], Training Loss: 0.1840, Validation Loss: 0.0960\n",
      "Epoch [91/500], Training Loss: 0.1839, Validation Loss: 0.0962\n",
      "Epoch [92/500], Training Loss: 0.1837, Validation Loss: 0.0957\n",
      "Epoch [93/500], Training Loss: 0.1836, Validation Loss: 0.0960\n",
      "Epoch [94/500], Training Loss: 0.1835, Validation Loss: 0.0954\n",
      "Epoch [95/500], Training Loss: 0.1834, Validation Loss: 0.0962\n",
      "Epoch [96/500], Training Loss: 0.1832, Validation Loss: 0.0954\n",
      "Epoch [97/500], Training Loss: 0.1831, Validation Loss: 0.0954\n",
      "Epoch [98/500], Training Loss: 0.1830, Validation Loss: 0.0950\n",
      "Epoch [99/500], Training Loss: 0.1830, Validation Loss: 0.0954\n",
      "Epoch [100/500], Training Loss: 0.1829, Validation Loss: 0.0948\n",
      "Epoch [101/500], Training Loss: 0.1827, Validation Loss: 0.0947\n",
      "Epoch [102/500], Training Loss: 0.1828, Validation Loss: 0.0953\n",
      "Epoch [103/500], Training Loss: 0.1825, Validation Loss: 0.0946\n",
      "Epoch [104/500], Training Loss: 0.1825, Validation Loss: 0.0943\n",
      "Epoch [105/500], Training Loss: 0.1823, Validation Loss: 0.0943\n",
      "Epoch [106/500], Training Loss: 0.1821, Validation Loss: 0.0944\n",
      "Epoch [107/500], Training Loss: 0.1822, Validation Loss: 0.0942\n",
      "Epoch [108/500], Training Loss: 0.1821, Validation Loss: 0.0943\n",
      "Epoch [109/500], Training Loss: 0.1820, Validation Loss: 0.0939\n",
      "Epoch [110/500], Training Loss: 0.1819, Validation Loss: 0.0937\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "from torchvision import datasets, transforms\n",
    "import csv\n",
    "import time\n",
    "import json\n",
    "import hashlib\n",
    "import random\n",
    "from SAE_model import StackedAutoencoder  # Import the SAE class\n",
    "from itertools import product\n",
    "\n",
    "# Function to set all random seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # If you are using multi-GPU.\n",
    "        \n",
    "# Set the base seed\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(m.weight, nonlinearity='leaky_relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),  # Normalize to [-1, 1]\n",
    "    transforms.Lambda(lambda x: x.view(-1))  # Flatten the image to a vector\n",
    "])\n",
    "\n",
    "# Load the KMNIST dataset\n",
    "# Load the full training dataset\n",
    "full_train_dataset = datasets.KMNIST(root='../data', train=True, transform=transform, download=True)\n",
    "\n",
    "# Define the sizes for the training and validation datasets\n",
    "val_size = 10000\n",
    "train_size = len(full_train_dataset) - val_size\n",
    "\n",
    "# Split the training data into train and validation sets\n",
    "# When using random_split, create a generator with the seed\n",
    "generator = torch.Generator().manual_seed(seed)\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "\n",
    "# Hyperparameter grid for tuning\n",
    "hyperparameter_grid = {\n",
    "    'learning_rate': [0.1, 1e-2, 1e-3, 1e-4], # [1e-3]\n",
    "    'layer_sizes': [\n",
    "        [800, 200, 120],\n",
    "    ],\n",
    "    'dropout_rates': [[0.0, 0.0, 0.0], [0.1, 0.1, 0.1]], # [0.0, 0.0, 0.0]\n",
    "    'weight_decay': [0.001, 0.1, 0.2], # 0.1\n",
    "    'batch_size': [64, 128, 256], # 128\n",
    "    'leaky_relu_negative_slope': [0.1, 0.2, 0.3, 0.4, 0.5], # 0.3\n",
    "    'num_epochs': [500],  # Max number of training epochs\n",
    "    # Early Stopping hyperparameters\n",
    "    'early_stopping_patience': [7], # 7\n",
    "    'early_stopping_min_delta': [1e-4], # 1e-4\n",
    "    # ReduceLROnPlateau hyperparameters\n",
    "    'scheduler_mode': ['min'], \n",
    "    'scheduler_factor': [0.15], # 0.15\n",
    "    'scheduler_patience': [2], # 2\n",
    "    'scheduler_threshold': [1e-4], # 1e-4\n",
    "    'scheduler_cooldown': [0]\n",
    "}\n",
    "\n",
    "# Generate all combinations of hyperparameters\n",
    "keys, values = zip(*hyperparameter_grid.items())\n",
    "hyperparameter_combinations = [dict(zip(keys, v)) for v in product(*values)]\n",
    "\n",
    "csv_filename = 'SAE_hyperparameter_tuning_results.csv'\n",
    "\n",
    "# Initialize an empty set to store hashes of existing results\n",
    "existing_hashes = set()\n",
    "\n",
    "# Initialize global_best_val_loss by reading existing CSV\n",
    "global_best_val_loss = float('inf')  # Initialize to infinity\n",
    "\n",
    "if os.path.exists(csv_filename):\n",
    "    with open(csv_filename, 'r', newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            hparams_hash = row['hparams_hash']\n",
    "            existing_hashes.add(hparams_hash)\n",
    "            try:\n",
    "                val_loss = float(row['val_loss'])\n",
    "                if val_loss < global_best_val_loss:\n",
    "                    global_best_val_loss = val_loss\n",
    "            except ValueError:\n",
    "                # If val_loss is not a float, skip\n",
    "                continue\n",
    "    print(f\"Current global best validation loss from CSV: {global_best_val_loss:.4f}\")\n",
    "else:\n",
    "    print(\"CSV file does not exist. Starting fresh.\")\n",
    "    # If the file does not exist, we'll create it later\n",
    "\n",
    "def train_and_evaluate(hparams):\n",
    "    # Unpack hyperparameters\n",
    "    learning_rate = hparams['learning_rate']\n",
    "    layer_sizes = hparams['layer_sizes']\n",
    "    dropout_rates = hparams['dropout_rates']\n",
    "    weight_decay = hparams['weight_decay']\n",
    "    batch_size = hparams['batch_size']\n",
    "    leaky_relu_negative_slope = hparams['leaky_relu_negative_slope']\n",
    "    num_epochs = hparams.get('num_epochs', 50)\n",
    "    \n",
    "    # Early Stopping hyperparameters\n",
    "    early_stopping_patience = hparams['early_stopping_patience']\n",
    "    early_stopping_min_delta = hparams['early_stopping_min_delta']\n",
    "    \n",
    "    # Scheduler hyperparameters\n",
    "    scheduler_mode = hparams['scheduler_mode']\n",
    "    scheduler_factor = hparams['scheduler_factor']\n",
    "    scheduler_patience = hparams['scheduler_patience']\n",
    "    scheduler_threshold = hparams['scheduler_threshold']\n",
    "    scheduler_cooldown = hparams['scheduler_cooldown']\n",
    "    \n",
    "    # Define activation functions\n",
    "    activation_functions = [nn.LeakyReLU(negative_slope=leaky_relu_negative_slope) for _ in layer_sizes]\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = StackedAutoencoder(\n",
    "        input_size=784,  # For 28x28 images\n",
    "        layer_sizes=layer_sizes,\n",
    "        activation_functions=activation_functions,\n",
    "        dropout_rates=dropout_rates,\n",
    "        weight_init=init_weights\n",
    "    ).to(device)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    # Scheduler: ReduceLROnPlateau\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode=scheduler_mode,\n",
    "        factor=scheduler_factor,\n",
    "        patience=scheduler_patience,\n",
    "        threshold=scheduler_threshold,\n",
    "        threshold_mode='rel',\n",
    "        cooldown=scheduler_cooldown,\n",
    "    )\n",
    "    \n",
    "    # Data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Loss function\n",
    "    loss_function = nn.MSELoss()\n",
    "    \n",
    "    # Early Stopping variables\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "    \n",
    "    # Initialize variables for logging\n",
    "    initial_lr = optimizer.param_groups[0]['lr']\n",
    "    current_lr = initial_lr\n",
    "    lr_reduction_epochs = []\n",
    "    epoch_logs = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        if early_stop:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for data in train_loader:\n",
    "            inputs, _ = data\n",
    "            inputs = inputs.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, inputs)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                inputs, _ = data\n",
    "                inputs = inputs.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_function(outputs, inputs)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        # Scheduler step\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Check if learning rate has been reduced\n",
    "        new_lr = optimizer.param_groups[0]['lr']\n",
    "        if new_lr < current_lr:\n",
    "            lr_reduction_epochs.append(epoch+1)  # Epochs are 1-indexed\n",
    "            current_lr = new_lr\n",
    "        \n",
    "        # Record per-epoch logs\n",
    "        epoch_logs.append({\n",
    "            'epoch': epoch+1,\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'learning_rate': current_lr\n",
    "        })\n",
    "        \n",
    "        # Early Stopping check\n",
    "        if val_loss < best_val_loss - early_stopping_min_delta:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        \n",
    "        if epochs_no_improve >= early_stopping_patience:\n",
    "            early_stop = True\n",
    "        \n",
    "        # Optionally, print progress\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "              f\"Training Loss: {train_loss:.4f}, \"\n",
    "              f\"Validation Loss: {val_loss:.4f}\")\n",
    "        \n",
    "    # Prepare training log\n",
    "    training_log = {\n",
    "        'final_epoch': epoch+1,\n",
    "        'lr_reduction_epochs': lr_reduction_epochs,\n",
    "        'epoch_logs': epoch_logs\n",
    "    }\n",
    "    \n",
    "    # Return final validation loss, training log, and the trained model\n",
    "    return val_loss, training_log, model, activation_functions, optimizer, scheduler\n",
    "\n",
    "# Main loop for hyperparameter tuning\n",
    "results = []\n",
    "\n",
    "for idx, hparams in enumerate(hyperparameter_combinations):\n",
    "    # Set the base seed\n",
    "    set_seed(42)\n",
    "    # Generate a unique id for the hyperparameters\n",
    "    hparams_str = json.dumps(hparams, sort_keys=True)\n",
    "    hparams_hash = hashlib.md5(hparams_str.encode('utf-8')).hexdigest()\n",
    "    \n",
    "    # Check if this combination exists in existing_results\n",
    "    if hparams_hash in existing_hashes:\n",
    "        print(f\"Skipping hyperparameter combination {idx+1}/{len(hyperparameter_combinations)}: {hparams} (already tested)\")\n",
    "        continue\n",
    "    else:\n",
    "        print(f\"Testing hyperparameter combination {idx+1}/{len(hyperparameter_combinations)}: {hparams}\")\n",
    "        try:\n",
    "            # Call train_and_evaluate and record the computation time\n",
    "            start_time = time.time()\n",
    "            val_loss, training_log, model, activation_functions, optimizer, scheduler = train_and_evaluate(hparams)\n",
    "            end_time = time.time()\n",
    "            computation_time = end_time - start_time\n",
    "\n",
    "            # Save the per-epoch logs to a file\n",
    "            log_filename = f\"misc/training_log_{hparams_hash}.json\"\n",
    "            with open(log_filename, 'w') as f:\n",
    "                json.dump(training_log, f)\n",
    "            \n",
    "            # Append the result to the CSV file\n",
    "            with open(csv_filename, 'a', newline='') as csvfile:\n",
    "                fieldnames = ['hparams_hash'] + list(hparams.keys()) + ['val_loss', 'computation_time', 'final_num_epochs', 'lr_reduction_epochs', 'log_filename']\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                # If the file is new, write the header\n",
    "                if csvfile.tell() == 0:\n",
    "                    writer.writeheader()\n",
    "                result_row = {'hparams_hash': hparams_hash}\n",
    "                for key, value in hparams.items():\n",
    "                    # Convert value to string using json.dumps\n",
    "                    result_row[key] = json.dumps(value)\n",
    "                result_row.update({\n",
    "                    'val_loss': val_loss,\n",
    "                    'computation_time': computation_time,\n",
    "                    'final_num_epochs': training_log['final_epoch'],\n",
    "                    'lr_reduction_epochs': json.dumps(training_log['lr_reduction_epochs']),\n",
    "                    'log_filename': log_filename\n",
    "                })\n",
    "                writer.writerow(result_row)\n",
    "            \n",
    "            # Update the set of existing hashes\n",
    "            existing_hashes.add(hparams_hash)\n",
    "\n",
    "            # Check and save the best model\n",
    "            if val_loss < global_best_val_loss: \n",
    "                torch.save({\n",
    "                    'state_dict': model.state_dict(),         # Model weights\n",
    "                    'config': {                               # Model configuration\n",
    "                        'input_size': 784,\n",
    "                        'layer_sizes': hparams['layer_sizes'],\n",
    "                        'activation_functions': [str(type(act)) for act in activation_functions],\n",
    "                        'dropout_rates': hparams['dropout_rates'],\n",
    "                        'weight_init': 'kaiming_uniform',\n",
    "                    },\n",
    "                    'hyperparameters': hparams,              # Hyperparameters\n",
    "                    'training_log': training_log,            # Training logs\n",
    "                    'best_val_loss': val_loss,               # Best validation loss\n",
    "                    'optimizer_state': optimizer.state_dict(), # Optimizer state\n",
    "                    'scheduler_state': scheduler.state_dict()  # Scheduler state\n",
    "                }, 'SAE_best_model.pth')\n",
    "                global_best_val_loss = val_loss\n",
    "                print(f\"New best model saved with validation loss: {val_loss:.4f}\\n\")\n",
    "            else:\n",
    "                print(f\"Validation loss {val_loss:.4f} did not improve over the best loss {global_best_val_loss:.4f}\\n\")\n",
    "             \n",
    "            results.append({'hparams': hparams, 'val_loss': val_loss})\n",
    "        except Exception as e:\n",
    "            print(f\"Error with hyperparameters {hparams}: {e}\\n\")\n",
    "            continue\n",
    "\n",
    "# Find the best hyperparameters based on validation loss\n",
    "if results:\n",
    "    best_result = min(results, key=lambda x: x['val_loss'])\n",
    "    print(\"Best hyperparameters based on validation loss:\")\n",
    "    print(best_result['hparams'])\n",
    "    print(f\"Validation Loss: {best_result['val_loss']:.4f}\")\n",
    "    print(\"The best model has been saved as 'best_model.pth'\")\n",
    "else:\n",
    "    print(\"No successful runs to report.\")\n"
   ]
  }
 ],
 "metadata": {
  "deepnote_notebook_id": "16d034cc308c411b9c5ec5cfddcc77a3",
  "kernelspec": {
   "display_name": "EEL",
   "language": "EEL",
   "name": "eel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

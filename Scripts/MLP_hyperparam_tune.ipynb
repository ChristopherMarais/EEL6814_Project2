{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f4c8bd-8652-406a-8e27-b6320d40de2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing hyperparameter combination 1/19200: {'learning_rate': 1e-05, 'layer_sizes': [256, 128, 64], 'dropout_rates': [0.1, 0.1, 0.1], 'batch_norm': [True, True, True], 'weight_decay': 0.0, 'batch_size': 16, 'leaky_relu_negative_slope': 0.01, 'num_epochs': 200, 'scheduler_mode': 'min', 'scheduler_factor': 0.1, 'scheduler_patience': 3, 'scheduler_threshold': 0.0001, 'scheduler_cooldown': 0, 'early_stopping_mode': 'min', 'early_stopping_patience': 5, 'early_stopping_delta': 0.0001}\n",
      "Epoch [1/200], Training Loss: 1.9025, Training Accuracy: 35.75%, Validation Loss: 1.5777, Validation Accuracy: 50.95%\n",
      "Epoch [2/200], Training Loss: 1.2407, Training Accuracy: 62.38%, Validation Loss: 1.2519, Validation Accuracy: 62.65%\n",
      "Epoch [3/200], Training Loss: 1.0007, Training Accuracy: 70.44%, Validation Loss: 1.0987, Validation Accuracy: 67.13%\n",
      "Epoch [4/200], Training Loss: 0.8695, Training Accuracy: 74.26%, Validation Loss: 0.9853, Validation Accuracy: 70.53%\n",
      "Epoch [5/200], Training Loss: 0.7857, Training Accuracy: 76.90%, Validation Loss: 0.9196, Validation Accuracy: 72.01%\n",
      "Epoch [6/200], Training Loss: 0.7259, Training Accuracy: 78.67%, Validation Loss: 0.8542, Validation Accuracy: 73.60%\n",
      "Epoch [7/200], Training Loss: 0.6761, Training Accuracy: 79.90%, Validation Loss: 0.8242, Validation Accuracy: 74.28%\n",
      "Epoch [8/200], Training Loss: 0.6391, Training Accuracy: 81.07%, Validation Loss: 0.7877, Validation Accuracy: 75.58%\n",
      "Epoch [9/200], Training Loss: 0.6108, Training Accuracy: 81.72%, Validation Loss: 0.7473, Validation Accuracy: 76.76%\n",
      "Epoch [10/200], Training Loss: 0.5773, Training Accuracy: 82.69%, Validation Loss: 0.7250, Validation Accuracy: 76.73%\n",
      "Epoch [11/200], Training Loss: 0.5587, Training Accuracy: 83.22%, Validation Loss: 0.6946, Validation Accuracy: 77.88%\n",
      "Epoch [12/200], Training Loss: 0.5456, Training Accuracy: 83.57%, Validation Loss: 0.6588, Validation Accuracy: 79.51%\n",
      "Epoch [13/200], Training Loss: 0.5246, Training Accuracy: 84.20%, Validation Loss: 0.6563, Validation Accuracy: 79.57%\n",
      "Epoch [14/200], Training Loss: 0.5024, Training Accuracy: 84.84%, Validation Loss: 0.6261, Validation Accuracy: 80.26%\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import csv\n",
    "import time\n",
    "import json\n",
    "import hashlib\n",
    "import random\n",
    "from MLP_model import MLPClassifier  # Import the MLPClassifier class\n",
    "from itertools import product\n",
    "\n",
    "# Function to set all random seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # If you are using multi-GPU.\n",
    "    \n",
    "# Set the base seed\n",
    "set_seed(42)\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(m.weight, nonlinearity='leaky_relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),  # Adjust if necessary\n",
    "    transforms.Lambda(lambda x: x.view(-1))  # Flatten the image\n",
    "])\n",
    "\n",
    "# Load the KMNIST dataset\n",
    "train_dataset = datasets.KMNIST(root='../data', train=True, transform=transform, download=True)\n",
    "val_dataset = datasets.KMNIST(root='../data', train=False, transform=transform, download=True)\n",
    "\n",
    "# Input and output sizes\n",
    "input_size = 784  # For 28x28 images\n",
    "output_size = 10  # Number of classes\n",
    "\n",
    "# Hyperparameter grid for tuning\n",
    "hyperparameter_grid = {\n",
    "    'learning_rate': [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3],\n",
    "    'layer_sizes': [[256, 128, 64], [512, 256, 128], [1024, 512, 256], [2048, 1024, 512]],\n",
    "    'dropout_rates': [[0.1, 0.1, 0.1], [0.2, 0.2, 0.2], [0.3, 0.3, 0.3], [0.4, 0.4, 0.4], [0.5, 0.5, 0.5]],\n",
    "    'batch_norm': [[True, True, True], [False, False, False]],\n",
    "    'weight_decay': [0.0, 1e-5, 1e-4, 1e-3],\n",
    "    'batch_size': [16, 32, 64, 128, 256],\n",
    "    'leaky_relu_negative_slope': [0.01, 0.05, 0.1, 0.2],  # Negative slope for LeakyReLU\n",
    "    'num_epochs': [200],  # Max number of training epochs\n",
    "    # ReduceLROnPlateau hyperparameters\n",
    "    'scheduler_mode': ['min'],  # Mode for the scheduler\n",
    "    'scheduler_factor': [0.1],  # Factor by which the learning rate will be reduced\n",
    "    'scheduler_patience': [3],  # Number of epochs with no improvement after which learning rate will be reduced\n",
    "    'scheduler_threshold': [1e-4],  # Threshold for measuring the new optimum\n",
    "    'scheduler_cooldown': [0],  # Number of epochs to wait before resuming normal operation after lr has been reduced\n",
    "    # Early Stopping hyperparameters\n",
    "    'early_stopping_mode': ['min'],  # Mode for early stopping\n",
    "    'early_stopping_patience': [5],  # Patience for early stopping\n",
    "    'early_stopping_delta': [1e-4],  # Minimum change to qualify as improvement\n",
    "}\n",
    "\n",
    "# Generate all combinations of hyperparameters\n",
    "keys, values = zip(*hyperparameter_grid.items())\n",
    "hyperparameter_combinations = [dict(zip(keys, v)) for v in product(*values)]\n",
    "\n",
    "csv_filename = 'MLP_hyperparameter_tuning_results.csv'\n",
    "\n",
    "# Initialize an empty set to store hashes of existing results\n",
    "existing_hashes = set()\n",
    "\n",
    "if os.path.exists(csv_filename):\n",
    "    with open(csv_filename, 'r', newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            hparams_hash = row['hparams_hash']\n",
    "            existing_hashes.add(hparams_hash)\n",
    "else:\n",
    "    # If the file does not exist, we'll create it later\n",
    "    pass\n",
    "\n",
    "def train_and_evaluate(hparams):\n",
    "    # Unpack hyperparameters\n",
    "    learning_rate = hparams['learning_rate']\n",
    "    layer_sizes = hparams['layer_sizes']\n",
    "    dropout_rates = hparams['dropout_rates']\n",
    "    batch_norm = hparams['batch_norm']\n",
    "    weight_decay = hparams['weight_decay']\n",
    "    batch_size = hparams['batch_size']\n",
    "    leaky_relu_negative_slope = hparams['leaky_relu_negative_slope']\n",
    "    num_epochs = hparams.get('num_epochs', 50)\n",
    "    \n",
    "    # Scheduler hyperparameters\n",
    "    scheduler_mode = hparams['scheduler_mode']\n",
    "    scheduler_factor = hparams['scheduler_factor']\n",
    "    scheduler_patience = hparams['scheduler_patience']\n",
    "    scheduler_threshold = hparams['scheduler_threshold']\n",
    "    scheduler_cooldown = hparams['scheduler_cooldown']\n",
    "    \n",
    "    # Early Stopping hyperparameters\n",
    "    early_stopping_mode = hparams['early_stopping_mode']\n",
    "    early_stopping_patience = hparams['early_stopping_patience']\n",
    "    early_stopping_delta = hparams['early_stopping_delta']\n",
    "    \n",
    "    # Define activation functions: Always LeakyReLU with specified negative slope\n",
    "    activation_functions = [nn.LeakyReLU(negative_slope=leaky_relu_negative_slope) for _ in layer_sizes]\n",
    "\n",
    "    # Initialize the model\n",
    "    model = MLPClassifier(\n",
    "        input_size=input_size,\n",
    "        layer_sizes=layer_sizes,\n",
    "        output_size=output_size,\n",
    "        activation_functions=activation_functions,\n",
    "        dropout_rates=dropout_rates,\n",
    "        batch_norm=batch_norm,\n",
    "        weight_init=init_weights\n",
    "    ).to(device)\n",
    "\n",
    "    # Optimizer (always AdamW)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Scheduler: ReduceLROnPlateau\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode=scheduler_mode,\n",
    "        factor=scheduler_factor,\n",
    "        patience=scheduler_patience,\n",
    "        threshold=scheduler_threshold,\n",
    "        threshold_mode='rel',\n",
    "        cooldown=scheduler_cooldown,\n",
    "    )\n",
    "\n",
    "    # Data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Loss function\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Early Stopping variables\n",
    "    best_metric = None\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "\n",
    "    # Determine the comparison operator based on mode\n",
    "    if early_stopping_mode == 'min':\n",
    "        def is_improvement(current, best):\n",
    "            return current < best - early_stopping_delta\n",
    "        best_metric = float('inf')\n",
    "    elif early_stopping_mode == 'max':\n",
    "        def is_improvement(current, best):\n",
    "            return current > best + early_stopping_delta\n",
    "        best_metric = float('-inf')\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported early_stopping_mode: {early_stopping_mode}\")\n",
    "\n",
    "    # Initialize variables for logging\n",
    "    initial_lr = optimizer.param_groups[0]['lr']\n",
    "    current_lr = initial_lr\n",
    "    lr_reduction_epochs = []\n",
    "    epoch_logs = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        if early_stop:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for data in train_loader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Calculate training accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_accuracy = 100 * correct_train / total_train\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_function(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Calculate validation accuracy\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = 100 * correct_val / total_val\n",
    "\n",
    "        # Scheduler step: ReduceLROnPlateau expects a metric to monitor\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Check if learning rate has been reduced\n",
    "        new_lr = optimizer.param_groups[0]['lr']\n",
    "        if new_lr < current_lr:\n",
    "            lr_reduction_epochs.append(epoch+1)  # Epochs are 1-indexed\n",
    "            current_lr = new_lr\n",
    "\n",
    "        # Record per-epoch logs\n",
    "        epoch_logs.append({\n",
    "            'epoch': epoch+1,\n",
    "            'train_loss': train_loss,\n",
    "            'train_accuracy': train_accuracy,\n",
    "            'val_loss': val_loss,\n",
    "            'val_accuracy': val_accuracy,\n",
    "            'learning_rate': current_lr\n",
    "        })\n",
    "\n",
    "        # Early Stopping check\n",
    "        if is_improvement(val_loss, best_metric):\n",
    "            best_metric = val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= early_stopping_patience:\n",
    "            early_stop = True\n",
    "\n",
    "        # Optionally, print progress\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "              f\"Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%, \"\n",
    "              f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "    # Prepare training log\n",
    "    training_log = {\n",
    "        'final_epoch': epoch+1,\n",
    "        'lr_reduction_epochs': lr_reduction_epochs,\n",
    "        'epoch_logs': epoch_logs\n",
    "    }\n",
    "\n",
    "    # Return final validation loss and accuracy, and training log\n",
    "    return val_loss, val_accuracy, training_log\n",
    "\n",
    "# Main loop for hyperparameter tuning\n",
    "results = []\n",
    "\n",
    "for idx, hparams in enumerate(hyperparameter_combinations):\n",
    "    # Generate a unique id for the hyperparameters\n",
    "    hparams_str = json.dumps(hparams, sort_keys=True)\n",
    "    hparams_hash = hashlib.md5(hparams_str.encode('utf-8')).hexdigest()\n",
    "    \n",
    "    # Check if this combination exists in existing_results\n",
    "    if hparams_hash in existing_hashes:\n",
    "        print(f\"Skipping hyperparameter combination {idx+1}/{len(hyperparameter_combinations)}: {hparams} (already tested)\")\n",
    "        continue\n",
    "    else:\n",
    "        print(f\"Testing hyperparameter combination {idx+1}/{len(hyperparameter_combinations)}: {hparams}\")\n",
    "        try:\n",
    "            # Call train_and_evaluate and record the computation time\n",
    "            start_time = time.time()\n",
    "            val_loss, val_accuracy, training_log = train_and_evaluate(hparams)\n",
    "            end_time = time.time()\n",
    "            computation_time = end_time - start_time\n",
    "\n",
    "            # Save the per-epoch logs to a file\n",
    "            log_filename = f\"training_log_{hparams_hash}.json\"\n",
    "            with open(log_filename, 'w') as f:\n",
    "                json.dump(training_log, f)\n",
    "            \n",
    "            # Append the result to the CSV file\n",
    "            with open(csv_filename, 'a', newline='') as csvfile:\n",
    "                fieldnames = ['hparams_hash'] + list(hparams.keys()) + ['val_loss', 'val_accuracy', 'computation_time', 'final_num_epochs', 'lr_reduction_epochs', 'log_filename']\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                # If the file is new, write the header\n",
    "                if csvfile.tell() == 0:\n",
    "                    writer.writeheader()\n",
    "                result_row = {'hparams_hash': hparams_hash}\n",
    "                for key, value in hparams.items():\n",
    "                    # Convert value to string using json.dumps\n",
    "                    result_row[key] = json.dumps(value)\n",
    "                result_row.update({\n",
    "                    'val_loss': val_loss,\n",
    "                    'val_accuracy': val_accuracy,\n",
    "                    'computation_time': computation_time,\n",
    "                    'final_num_epochs': training_log['final_epoch'],\n",
    "                    'lr_reduction_epochs': json.dumps(training_log['lr_reduction_epochs']),\n",
    "                    'log_filename': log_filename\n",
    "                })\n",
    "                writer.writerow(result_row)\n",
    "\n",
    "            # Update the set of existing hashes\n",
    "            existing_hashes.add(hparams_hash)\n",
    "\n",
    "            results.append({'hparams': hparams, 'val_loss': val_loss, 'val_accuracy': val_accuracy})\n",
    "            print(f\"Result - Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error with hyperparameters {hparams}: {e}\\n\")\n",
    "            continue\n",
    "\n",
    "# Find the best hyperparameters based on validation accuracy\n",
    "best_result = max(results, key=lambda x: x['val_accuracy'])\n",
    "print(\"Best hyperparameters based on validation accuracy:\")\n",
    "print(best_result['hparams'])\n",
    "print(f\"Validation Loss: {best_result['val_loss']:.4f}, Validation Accuracy: {best_result['val_accuracy']:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EEL",
   "language": "EEL",
   "name": "eel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
